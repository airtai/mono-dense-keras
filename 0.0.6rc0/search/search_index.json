{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Guide","text":"<p>This Python library implements Monotonic Dense Layer as described in Davor Runje, Sharath M. Shankaranarayana, \u201cConstrained Monotonic Neural Networks\u201d, https://https://arxiv.org/abs/2205.11775.</p> <p>If you use this library, please cite:</p> <pre><code>@misc{https://doi.org/10.48550/arxiv.2205.11775,\n  doi = {10.48550/ARXIV.2205.11775},\n  url = {https://arxiv.org/abs/2205.11775},\n  author = {Davor Runje and Sharath M. Shankaranarayana},\n  title = {Constrained Monotonic Neural Networks},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}\n</code></pre>"},{"location":"#install","title":"Install","text":"<pre><code>pip install mono-dense-keras\n</code></pre>"},{"location":"#how-to-use","title":"How to use","text":"<p>First, we\u2019ll create a simple dataset for testing using numpy. Inputs values \\(x_1\\), \\(x_2\\) and \\(x_3\\) will be sampled from the normal distribution, while the output value \\(y\\) will be calculated according to the following formula before adding noise to it:</p> <p>\\(y = x_1^3 + \\sin\\left(\\frac{x_2}{2 \\pi}\\right) + e^{-x_3}\\)</p> <pre><code>import numpy as np\n\nrng = np.random.default_rng(42)\n\ndef generate_data(no_samples: int, noise: float):\n    x = rng.normal(size=(no_samples, 3))\n    y = x[:, 0] ** 3\n    y += np.sin(x[:, 1] / (2*np.pi))\n    y += np.exp(-x[:, 2])\n    y += noise * rng.normal(size=no_samples)\n    return x, y\n\nx_train, y_train = generate_data(10_000, noise=0.1)\nx_val, y_val = generate_data(10_000, noise=0.)\n</code></pre> <p>Now, we\u2019ll use the <code>MonoDense</code> layer instead of <code>Dense</code> layer. By default, the <code>MonoDense</code> layer assumes the output of the layer is monotonically increasing with all inputs. This assumtion is always true for all layers except possibly the first one. For the first layer, we use <code>monotonicity_indicator</code> to specify which input parameters are monotonic and to specify are they increasingly or decreasingly monotonic: - set 1 for increasingly monotonic parameter,</p> <ul> <li> <p>set -1 for decreasingly monotonic parameter, and</p> </li> <li> <p>set 0 otherwise.</p> </li> </ul> <p>In our case, the <code>monotonicity_indicator</code> is <code>[1, 0, -1]</code> because \\(y\\) is: - monotonically increasing w.r.t. \\(x_1\\) \\(\\left(\\frac{\\partial y}{x_1} = 3 {x_1}^2 \\geq 0\\right)\\), and</p> <ul> <li>monotonically decreasing w.r.t. \\(x_3\\) \\(\\left(\\frac{\\partial y}{x_3} = - e^{-x_2} \\leq 0\\right)\\).</li> </ul> <pre><code>from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, Dense\nfrom mono_dense_keras import MonoDense\n\n# build a simple model with 3 hidden layer, but this using MonotonicDense layer\nmodel = Sequential()\n\nmodel.add(Input(shape=(3,)))\nmonotonicity_indicator = [1, 0, -1]\nmodel.add(MonoDense(128, activation=\"elu\", monotonicity_indicator=monotonicity_indicator))\nmodel.add(MonoDense(128, activation=\"elu\"))\nmodel.add(MonoDense(1))\n\nmodel.summary()\n</code></pre> <pre><code>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n mono_dense_2 (MonoDense)    (None, 128)               512\n\n mono_dense_3 (MonoDense)    (None, 128)               16512\n\n mono_dense_4 (MonoDense)    (None, 1)                 129\n\n=================================================================\nTotal params: 17,153\nTrainable params: 17,153\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <pre><code>from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ndef train_model(model, initial_learning_rate):\n    # train the model\n    lr_schedule = ExponentialDecay(\n        initial_learning_rate=initial_learning_rate,\n        decay_steps=10_000 // 32,\n        decay_rate=0.9,\n    )\n    optimizer = Adam(learning_rate=lr_schedule)\n    model.compile(optimizer=\"adam\", loss=\"mse\")\n\n    model.fit(x=x_train, y=y_train, batch_size=32, validation_data=(x_val, y_val), epochs=10)\n\ntrain_model(model, initial_learning_rate=1.)\n</code></pre> <pre><code>Epoch 1/10\n313/313 [==============================] - 2s 5ms/step - loss: 0.2590 - val_loss: 0.4990\nEpoch 2/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2875 - val_loss: 0.1390\nEpoch 3/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2241 - val_loss: 0.0790\nEpoch 4/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2297 - val_loss: 0.1043\nEpoch 5/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2502 - val_loss: 0.1089\nEpoch 6/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2231 - val_loss: 0.0590\nEpoch 7/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1715 - val_loss: 0.5466\nEpoch 8/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1890 - val_loss: 0.0863\nEpoch 9/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1655 - val_loss: 0.1200\nEpoch 10/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2332 - val_loss: 0.1196\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The full text of the license is available at:</p> <p>https://github.com/airtai/mono-dense-keras/blob/main/LICENSE</p> <p>You are free to: - Share \u2014 copy and redistribute the material in any medium or format</p> <ul> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p> <p>Under the following terms: - Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> <ul> <li> <p>NonCommercial \u2014 You may not use the material for commercial purposes.</p> </li> <li> <p>ShareAlike \u2014 If you remix, transform, or build upon the material, you   must distribute your contributions under the same license as the   original.</p> </li> <li> <p>No additional restrictions \u2014 You may not apply legal terms or   technological measures that legally restrict others from doing   anything the license permits.</p> </li> </ul>"},{"location":"MonoDenseLayer/","title":"Constrained Monotonic Neural Networks","text":""},{"location":"MonoDenseLayer/#imports","title":"Imports","text":"<pre><code>from pathlib import Path\n\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport pytest\nimport seaborn as sns\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras import Model\n</code></pre> <pre><code>from os import environ\n\nenviron[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n</code></pre>"},{"location":"MonoDenseLayer/#monotonic-dense-layer","title":"Monotonic Dense Layer","text":""},{"location":"MonoDenseLayer/#actvation-functions","title":"Actvation Functions","text":"<p>source</p>"},{"location":"MonoDenseLayer/#get_activation_functions","title":"get_activation_functions","text":"<pre><code> get_activation_functions (activation:Union[str,Callable[[Union[tensorflow\n                           .python.types.core.Tensor,tensorflow.python.typ\n                           es.core.TensorProtocol,int,float,bool,str,bytes\n                           ,complex,tuple,list,numpy.ndarray,numpy.generic\n                           ]],Union[tensorflow.python.types.core.Tensor,te\n                           nsorflow.python.types.core.TensorProtocol,int,f\n                           loat,bool,str,bytes,complex,tuple,list,numpy.nd\n                           array,numpy.generic]],NoneType]=None)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#get_saturated_activation","title":"get_saturated_activation","text":"<pre><code> get_saturated_activation (convex_activation:Callable[[Union[tensorflow.py\n                           thon.types.core.Tensor,tensorflow.python.types.\n                           core.TensorProtocol,int,float,bool,str,bytes,co\n                           mplex,tuple,list,numpy.ndarray,numpy.generic]],\n                           Union[tensorflow.python.types.core.Tensor,tenso\n                           rflow.python.types.core.TensorProtocol,int,floa\n                           t,bool,str,bytes,complex,tuple,list,numpy.ndarr\n                           ay,numpy.generic]], concave_activation:Callable\n                           [[Union[tensorflow.python.types.core.Tensor,ten\n                           sorflow.python.types.core.TensorProtocol,int,fl\n                           oat,bool,str,bytes,complex,tuple,list,numpy.nda\n                           rray,numpy.generic]],Union[tensorflow.python.ty\n                           pes.core.Tensor,tensorflow.python.types.core.Te\n                           nsorProtocol,int,float,bool,str,bytes,complex,t\n                           uple,list,numpy.ndarray,numpy.generic]],\n                           a:float=1.0, c:float=1.0)\n</code></pre> <pre><code>for activation in [None, \"relu\", tf.keras.activations.elu]:\n    f, g, h = get_activation_functions(activation)\n    hasattr(f, \"__call__\")\n    hasattr(g, \"__call__\")\n    hasattr(h, \"__call__\")\n</code></pre> <pre><code>def plot_activation_functions(\n    activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    *,\n    font_size: int = 20,\n    save_pdf: bool = False,\n    save_path: Union[Path, str] = \"plots\",\n    linestyle=\"--\", \n    alpha=0.7,\n    linewidth=4.0\n) -&gt; None:\n    font = {\"size\": font_size}\n    matplotlib.rc(\"font\", **font)\n    (\n        convex_activation,\n        concave_activation,\n        saturated_activation,\n    ) = get_activation_functions(activation)\n    plt.rcParams[\"figure.figsize\"] = (6, 4)\n\n    x = np.arange(-3.5, 3.5, 0.1)\n    plot_kwargs = dict(linestyle=linestyle, alpha=alpha, linewidth=linewidth)\n    plt.plot(x, convex_activation(x), label=r\"$\\breve{\\rho}(x)$\", **plot_kwargs)\n    plt.plot(x, concave_activation(x), label=r\"$\\hat{\\rho}(x)$\", **plot_kwargs)\n    plt.plot(x, saturated_activation(x), label=r\"$\\tilde{\\rho}(x)$\", **plot_kwargs)\n    plt.legend()\n\n    title = f\"{activation.__name__ if hasattr(activation, '__name__') else activation}-based activations\"\n    plt.title(title)\n    if save_pdf:\n        path = Path(save_path) / (title.replace(\" \", \"_\") + \".pdf\")\n        path.parent.mkdir(exist_ok=True, parents=True)\n        plt.savefig(path, format=\"pdf\")\n        print(f\"Saved figure to: {path}\")\n\n    plt.show()\n</code></pre> <pre><code>for activation in [\"linear\", \"ReLU\", \"ELU\", \"SELU\"]:\n    plot_activation_functions(activation, save_pdf=True)\n</code></pre> <pre><code>Saved figure to: plots/linear-based_activations.pdf\nSaved figure to: plots/ReLU-based_activations.pdf\nSaved figure to: plots/ELU-based_activations.pdf\nSaved figure to: plots/SELU-based_activations.pdf\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_activations","title":"apply_activations","text":"<pre><code> apply_activations (x:Union[tensorflow.python.types.core.Tensor,tensorflow\n                    .python.types.core.TensorProtocol,int,float,bool,str,b\n                    ytes,complex,tuple,list,numpy.ndarray,numpy.generic],\n                    units:int, convex_activation:Callable[[Union[tensorflo\n                    w.python.types.core.Tensor,tensorflow.python.types.cor\n                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl\n                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.\n                    python.types.core.Tensor,tensorflow.python.types.core.\n                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                    list,numpy.ndarray,numpy.generic]], concave_activation\n                    :Callable[[Union[tensorflow.python.types.core.Tensor,t\n                    ensorflow.python.types.core.TensorProtocol,int,float,b\n                    ool,str,bytes,complex,tuple,list,numpy.ndarray,numpy.g\n                    eneric]],Union[tensorflow.python.types.core.Tensor,ten\n                    sorflow.python.types.core.TensorProtocol,int,float,boo\n                    l,str,bytes,complex,tuple,list,numpy.ndarray,numpy.gen\n                    eric]], saturated_activation:Callable[[Union[tensorflo\n                    w.python.types.core.Tensor,tensorflow.python.types.cor\n                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl\n                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.\n                    python.types.core.Tensor,tensorflow.python.types.core.\n                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                    list,numpy.ndarray,numpy.generic]],\n                    is_convex:bool=False, is_concave:bool=False,\n                    activation_weights:Tuple[float,float,float]=(7.0, 7.0,\n                    2.0))\n</code></pre> <pre><code>def plot_applied_activation(\n    activation: str = \"relu\",\n    *,\n    save_pdf: bool = False,\n    save_path: Union[Path, str] = \"plots\",\n    font_size: int = 20,\n    linestyle=\"--\",\n    alpha=0.7,\n    linewidth=2.0,\n):\n    font = {\"size\": font_size}\n    matplotlib.rc(\"font\", **font)\n    plt.rcParams[\"figure.figsize\"] = (18, 3)\n\n    x = np.arange(-1.5, 1.5, step=3 / 256)\n    h = 3 * np.sin(2 * np.pi * x)\n\n    (\n        convex_activation,\n        concave_activation,\n        saturated_activation,\n    ) = get_activation_functions(activation)\n\n    y = apply_activations(\n        h,\n        convex_activation=convex_activation,\n        concave_activation=concave_activation,\n        saturated_activation=saturated_activation,\n        units=x.shape[0],\n        activation_weights=(1.0, 1.0, 1.0),\n    )\n\n    plot_kwargs = dict(linestyle=linestyle, alpha=alpha, linewidth=linewidth)\n\n    plt.plot(np.arange(x.shape[0]), h, label=\"$h$\", **plot_kwargs)\n    plt.plot(np.arange(x.shape[0]), y, label=r\"${\\rho}(h)$\", **plot_kwargs)\n    title = (\n        \"Applying \"\n        + (activation.__name__ if hasattr(activation, \"__name__\") else activation)\n        + f\"-based activations to {x.shape[0]}-dimensional vector\"\n        + r\" $h$\"\n    )\n    plt.title(title)\n\n    plt.legend()\n\n    if save_pdf:\n        path = Path(save_path) / (title.replace(\" \", \"_\") + \".pdf\")\n        path.parent.mkdir(exist_ok=True, parents=True)\n        plt.savefig(path, format=\"pdf\")\n    #         print(f\"Saved figure to: {path}\")\n\n    plt.show()\n</code></pre> <pre><code>for activation in [\"linear\", \"ReLU\", \"ELU\", \"SELU\"]:\n    plot_applied_activation(activation, save_pdf=True)\n</code></pre>"},{"location":"MonoDenseLayer/#monotonicity-indicator","title":"Monotonicity indicator","text":"<p>source</p>"},{"location":"MonoDenseLayer/#get_monotonicity_indicator","title":"get_monotonicity_indicator","text":"<pre><code> get_monotonicity_indicator (monotonicity_indicator:Union[numpy.__array_li\n                             ke._SupportsArray[numpy.dtype],numpy.__nested\n                             _sequence._NestedSequence[numpy.__array_like.\n                             _SupportsArray[numpy.dtype]],bool,int,float,c\n                             omplex,str,bytes,numpy.__nested_sequence._Nes\n                             tedSequence[Union[bool,int,float,complex,str,\n                             bytes]]], input_shape:Tuple[int,...],\n                             units:int)\n</code></pre> <pre><code>input_shape = (13, 2)\nunits = 3\n\nlayer = Dense(units=units)\nlayer.build(input_shape=input_shape)\n\nfor monotonicity_indicator in [\n    1,\n    [1],\n    [1, 1],\n    np.ones((2,)),\n    np.ones((2, 1)),\n    np.ones((2, 3)),\n]:\n    expected = np.ones((2, 3))\n    actual = get_monotonicity_indicator(\n        monotonicity_indicator, input_shape=(13, 2), units=3\n    )\n\n    # rank is 2\n    assert len(actual.shape) == 2\n    # it is broadcastable to the kernel shape of (input_shape[-1], units)\n    np.testing.assert_array_equal(np.broadcast_to(actual, (2, 3)), expected)\n</code></pre> <pre><code>expected = [[1], [0], [-1]]\nactual = get_monotonicity_indicator([1, 0, -1], input_shape=(13, 3), units=4)\nnp.testing.assert_array_equal(actual, expected)\n</code></pre> <pre><code>with pytest.raises(ValueError) as e:\n    get_monotonicity_indicator([0, 1, -1], input_shape=(13, 2), units=3)\nassert e.value.args == (\n    \"operands could not be broadcast together with remapped shapes [original-&gt;remapped]: (3,1)  and requested shape (2,3)\",\n)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#replace_kernel_using_monotonicity_indicator","title":"replace_kernel_using_monotonicity_indicator","text":"<pre><code> replace_kernel_using_monotonicity_indicator\n                                              (layer:keras.layers.core.den\n                                              se.Dense, monotonicity_indic\n                                              ator:Union[tensorflow.python\n                                              .types.core.Tensor,tensorflo\n                                              w.python.types.core.TensorPr\n                                              otocol,int,float,bool,str,by\n                                              tes,complex,tuple,list,numpy\n                                              .ndarray,numpy.generic])\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_monotonicity_indicator_to_kernel","title":"apply_monotonicity_indicator_to_kernel","text":"<pre><code> apply_monotonicity_indicator_to_kernel\n                                         (kernel:tensorflow.python.ops.var\n                                         iables.Variable, monotonicity_ind\n                                         icator:Union[numpy.__array_like._\n                                         SupportsArray[numpy.dtype],numpy.\n                                         __nested_sequence._NestedSequence\n                                         [numpy.__array_like._SupportsArra\n                                         y[numpy.dtype]],bool,int,float,co\n                                         mplex,str,bytes,numpy.__nested_se\n                                         quence._NestedSequence[Union[bool\n                                         ,int,float,complex,str,bytes]]])\n</code></pre> <pre><code>def display_kernel(kernel: Union[tf.Variable, np.typing.NDArray[float]]) -&gt; None:\n    cm = sns.color_palette(\"coolwarm_r\", as_cmap=True)\n\n    df = pd.DataFrame(kernel)\n\n    display(df.style.format(\"{:.2f}\").background_gradient(cmap=cm, vmin=-1e-8, vmax=1e-8))\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\n\nunits = 18\ninput_len = 7\n\nlayer = tf.keras.layers.Dense(units=units)\n\ninput_shape = (input_len,)\nlayer.build(input_shape=input_shape)\n\nprint(\"Original kernel:\")\ndisplay_kernel(layer.kernel)\n\nprint(\"Kernel after applying monotocity indicator 1 for all values:\")\nmonotonicity_indicator = get_monotonicity_indicator(\n    1, input_shape=input_shape, units=units\n)\nwith replace_kernel_using_monotonicity_indicator(layer, monotonicity_indicator):\n    display_kernel(layer.kernel)\n</code></pre> <pre><code>Original kernel:\nKernel after applying monotocity indicator 1 for all values:\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 -0.14 0.44 -0.41 0.15 0.46 -0.33 0.02 0.13 -0.41 -0.05 0.46 -0.03 0.00 0.26 -0.47 -0.30 1 0.01 -0.42 -0.45 0.34 0.41 -0.23 0.35 -0.36 -0.04 0.06 0.07 -0.29 -0.28 0.48 -0.38 -0.06 -0.23 -0.37 2 0.23 -0.31 0.18 0.15 -0.45 0.06 -0.16 -0.11 0.45 -0.09 0.03 -0.24 -0.37 0.21 0.11 0.01 -0.46 -0.37 3 0.29 0.36 -0.07 -0.18 -0.46 -0.45 0.25 0.32 -0.12 0.22 -0.18 0.27 -0.18 -0.07 0.35 0.32 0.18 0.39 4 0.35 -0.27 0.13 -0.40 0.44 0.21 0.06 -0.31 -0.30 0.46 -0.44 -0.18 -0.26 -0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 -0.02 -0.36 0.39 -0.13 0.30 0.35 -0.12 -0.43 0.44 0.32 0.06 -0.30 -0.29 0.24 -0.44 -0.13 6 0.38 -0.04 -0.30 0.17 -0.03 0.37 -0.03 -0.18 0.42 -0.39 -0.33 -0.19 0.02 -0.41 -0.44 0.42 0.38 -0.21 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 0.14 0.44 0.41 0.15 0.46 0.33 0.02 0.13 0.41 0.05 0.46 0.03 0.00 0.26 0.47 0.30 1 0.01 0.42 0.45 0.34 0.41 0.23 0.35 0.36 0.04 0.06 0.07 0.29 0.28 0.48 0.38 0.06 0.23 0.37 2 0.23 0.31 0.18 0.15 0.45 0.06 0.16 0.11 0.45 0.09 0.03 0.24 0.37 0.21 0.11 0.01 0.46 0.37 3 0.29 0.36 0.07 0.18 0.46 0.45 0.25 0.32 0.12 0.22 0.18 0.27 0.18 0.07 0.35 0.32 0.18 0.39 4 0.35 0.27 0.13 0.40 0.44 0.21 0.06 0.31 0.30 0.46 0.44 0.18 0.26 0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 0.02 0.36 0.39 0.13 0.30 0.35 0.12 0.43 0.44 0.32 0.06 0.30 0.29 0.24 0.44 0.13 6 0.38 0.04 0.30 0.17 0.03 0.37 0.03 0.18 0.42 0.39 0.33 0.19 0.02 0.41 0.44 0.42 0.38 0.21 <pre><code>monotonicity_indicator = [1] * 2 + [-1] * 2 + [0] * (input_shape[0] - 4)\nmonotonicity_indicator = get_monotonicity_indicator(\n    monotonicity_indicator, input_shape=input_shape, units=units\n)\n\nprint(\"Monotocity indicator:\")\ndisplay_kernel(monotonicity_indicator)\n\nprint(\"Kernel after applying the monotocity indicator:\")\nwith replace_kernel_using_monotonicity_indicator(\n    layer, monotonicity_indicator\n):\n    display_kernel(layer.kernel)\n</code></pre> <pre><code>Monotocity indicator:\nKernel after applying the monotocity indicator:\n</code></pre> 0 0 1.00 1 1.00 2 -1.00 3 -1.00 4 0.00 5 0.00 6 0.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 0.14 0.44 0.41 0.15 0.46 0.33 0.02 0.13 0.41 0.05 0.46 0.03 0.00 0.26 0.47 0.30 1 0.01 0.42 0.45 0.34 0.41 0.23 0.35 0.36 0.04 0.06 0.07 0.29 0.28 0.48 0.38 0.06 0.23 0.37 2 -0.23 -0.31 -0.18 -0.15 -0.45 -0.06 -0.16 -0.11 -0.45 -0.09 -0.03 -0.24 -0.37 -0.21 -0.11 -0.01 -0.46 -0.37 3 -0.29 -0.36 -0.07 -0.18 -0.46 -0.45 -0.25 -0.32 -0.12 -0.22 -0.18 -0.27 -0.18 -0.07 -0.35 -0.32 -0.18 -0.39 4 0.35 -0.27 0.13 -0.40 0.44 0.21 0.06 -0.31 -0.30 0.46 -0.44 -0.18 -0.26 -0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 -0.02 -0.36 0.39 -0.13 0.30 0.35 -0.12 -0.43 0.44 0.32 0.06 -0.30 -0.29 0.24 -0.44 -0.13 6 0.38 -0.04 -0.30 0.17 -0.03 0.37 -0.03 -0.18 0.42 -0.39 -0.33 -0.19 0.02 -0.41 -0.44 0.42 0.38 -0.21"},{"location":"MonoDenseLayer/#monotonic-dense-layer_1","title":"Monotonic Dense Layer","text":"<p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p>"},{"location":"MonoDenseLayer/#monodense","title":"MonoDense","text":"<pre><code> MonoDense (units:int, activation:Union[str,Callable[[Union[tensorflow.pyt\n            hon.types.core.Tensor,tensorflow.python.types.core.TensorProto\n            col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndarray,\n            numpy.generic]],Union[tensorflow.python.types.core.Tensor,tens\n            orflow.python.types.core.TensorProtocol,int,float,bool,str,byt\n            es,complex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=\n            None, monotonicity_indicator:Union[numpy.__array_like._Support\n            sArray[numpy.dtype],numpy.__nested_sequence._NestedSequence[nu\n            mpy.__array_like._SupportsArray[numpy.dtype]],bool,int,float,c\n            omplex,str,bytes,numpy.__nested_sequence._NestedSequence[Union\n            [bool,int,float,complex,str,bytes]]]=1, is_convex:bool=False,\n            is_concave:bool=False,\n            activation_weights:Tuple[float,float,float]=(7.0, 7.0, 2.0),\n            **kwargs:Dict[str,Any])\n</code></pre> <p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> <pre><code>units = 18\nactivation = \"relu\"\nbatch_size = 9\nx_len = 11\n\nx = np.random.default_rng(42).normal(size=(batch_size, x_len))\n\ntf.keras.utils.set_random_seed(42)\n\nfor monotonicity_indicator in [[1]*4+[0]*4+[-1]*3, 1, np.ones((x_len,)), -1, -np.ones((x_len,))]:\n    print(\"*\"*120)    \n    mono_layer = MonoDense(\n        units=units,\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator,\n        activation_weights=(7, 7, 4),\n    )\n    print(\"input:\")\n    display_kernel(x)\n\n    y = mono_layer(x)\n    print(f\"monotonicity_indicator = {monotonicity_indicator}\")\n    display_kernel(mono_layer.monotonicity_indicator)\n\n    print(\"kernel:\")\n    with replace_kernel_using_monotonicity_indicator(\n        mono_layer, mono_layer.monotonicity_indicator\n    ):\n        display_kernel(mono_layer.kernel)\n\n    print(\"output:\")\n    display_kernel(y)\nprint(\"ok\")\n</code></pre> <pre><code>************************************************************************************************************************\ninput:\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function apply_activations&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nmonotonicity_indicator = [1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = 1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = -1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\nkernel:\noutput:\nok\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 0.00 5 0.00 6 0.00 7 0.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.33 0.15 0.13 0.41 0.38 0.14 0.43 0.30 0.02 0.12 0.38 0.05 0.42 0.03 0.00 0.24 0.44 0.28 1 0.01 0.39 0.42 0.32 0.38 0.22 0.33 0.34 0.03 0.06 0.06 0.27 0.26 0.45 0.35 0.05 0.21 0.34 2 0.21 0.29 0.16 0.14 0.42 0.06 0.15 0.10 0.41 0.08 0.03 0.22 0.34 0.20 0.11 0.01 0.43 0.35 3 0.27 0.33 0.06 0.17 0.42 0.42 0.24 0.30 0.11 0.20 0.17 0.25 0.17 0.07 0.32 0.30 0.17 0.36 4 0.32 -0.25 0.12 -0.37 0.41 0.20 0.06 -0.28 -0.27 0.43 -0.41 -0.17 -0.24 -0.31 0.33 0.31 0.11 0.03 5 0.04 0.19 -0.02 -0.34 0.36 -0.12 0.28 0.32 -0.11 -0.40 0.41 0.30 0.06 -0.28 -0.27 0.23 -0.41 -0.12 6 0.35 -0.04 -0.28 0.16 -0.03 0.35 -0.03 -0.16 0.39 -0.36 -0.31 -0.18 0.02 -0.38 -0.40 0.39 0.35 -0.19 7 0.33 -0.34 0.11 -0.29 0.25 -0.21 0.11 0.08 -0.19 -0.39 0.01 0.10 0.39 -0.25 -0.37 -0.27 0.04 0.34 8 -0.27 -0.09 -0.02 -0.45 -0.16 -0.12 -0.09 -0.43 -0.36 -0.09 -0.23 -0.42 -0.28 -0.24 -0.30 -0.31 -0.07 -0.07 9 -0.38 -0.34 -0.44 -0.42 -0.32 -0.06 -0.27 -0.28 -0.22 -0.05 -0.08 -0.07 -0.21 -0.39 -0.01 -0.26 -0.24 -0.42 10 -0.09 -0.45 -0.41 -0.36 -0.19 -0.09 -0.00 -0.34 -0.17 -0.18 -0.05 -0.39 -0.06 -0.20 -0.40 -0.33 -0.18 -0.01 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.01 0.40 0.00 1.38 0.00 0.10 0.00 -0.00 -0.00 -0.13 -0.00 -0.26 -0.00 -0.00 -0.55 -0.52 0.79 0.64 1 0.45 1.02 0.96 0.71 1.22 0.00 0.86 -0.00 -0.00 -0.09 -0.00 -0.00 -0.00 -0.00 0.26 -0.17 0.54 1.00 2 0.30 0.00 0.33 0.00 0.41 0.00 0.42 -0.53 -0.89 -0.29 -0.23 -0.84 -0.16 -0.93 -0.90 0.08 0.37 0.08 3 0.21 0.26 0.33 0.42 0.00 0.00 0.00 -0.16 -0.00 -0.61 -0.53 -0.07 -0.00 -0.00 -0.55 -0.66 0.83 0.78 4 1.38 0.49 0.70 0.82 1.47 0.54 0.63 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.73 0.97 0.94 0.91 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.86 -0.25 -0.00 -1.57 -1.19 -0.61 -0.23 0.13 -1.00 0.50 -0.06 6 0.00 0.00 0.00 0.17 0.00 0.00 0.00 -0.15 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.06 -1.00 0.00 0.12 7 0.00 0.96 0.35 0.93 0.00 0.32 0.17 -0.00 -0.00 -0.00 -0.00 -0.00 -0.17 -0.00 0.67 0.06 0.12 0.17 8 0.00 1.33 0.92 1.63 0.52 0.00 0.66 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.23 0.18 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.44 0.02 0.24 0.22 0.29 0.35 0.18 0.03 0.39 0.17 0.25 0.02 0.10 0.13 0.00 0.42 0.21 0.31 1 0.35 0.06 0.26 0.42 0.05 0.41 0.16 0.33 0.03 0.26 0.11 0.03 0.23 0.04 0.37 0.27 0.32 0.40 2 0.37 0.30 0.36 0.14 0.21 0.40 0.01 0.28 0.16 0.44 0.43 0.23 0.27 0.22 0.23 0.25 0.43 0.05 3 0.32 0.25 0.05 0.45 0.08 0.18 0.26 0.24 0.34 0.07 0.07 0.14 0.04 0.19 0.29 0.23 0.43 0.09 4 0.36 0.05 0.20 0.41 0.38 0.29 0.01 0.44 0.17 0.04 0.31 0.34 0.29 0.16 0.25 0.18 0.01 0.28 5 0.34 0.31 0.38 0.34 0.08 0.40 0.15 0.16 0.14 0.25 0.15 0.20 0.10 0.06 0.44 0.19 0.42 0.21 6 0.01 0.38 0.43 0.18 0.00 0.43 0.45 0.28 0.25 0.18 0.03 0.26 0.22 0.26 0.08 0.23 0.45 0.42 7 0.04 0.12 0.28 0.17 0.11 0.00 0.15 0.24 0.05 0.05 0.27 0.32 0.33 0.11 0.09 0.40 0.19 0.06 8 0.30 0.17 0.21 0.42 0.21 0.29 0.19 0.38 0.03 0.34 0.32 0.30 0.34 0.15 0.28 0.11 0.44 0.19 9 0.10 0.10 0.35 0.32 0.24 0.28 0.30 0.28 0.10 0.12 0.30 0.41 0.15 0.00 0.10 0.40 0.18 0.24 10 0.00 0.22 0.21 0.09 0.10 0.13 0.18 0.37 0.24 0.29 0.25 0.23 0.32 0.14 0.27 0.34 0.25 0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.01 0.00 0.00 0.00 0.00 0.00 -0.93 -0.00 -0.07 -0.58 -0.88 -0.58 -0.00 -0.87 -0.49 -0.05 -1.00 1 0.73 0.10 0.22 0.18 0.18 0.16 0.00 -0.23 -0.00 -0.00 -0.00 -0.09 -0.00 -0.00 0.16 0.47 0.53 -0.27 2 1.15 0.36 0.82 1.20 0.80 1.06 0.61 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.53 0.61 1.00 0.94 3 0.00 0.45 0.28 0.00 0.00 0.11 0.14 -0.00 -0.21 -0.00 -0.00 -0.00 -0.00 -0.00 0.15 0.08 0.72 -0.08 4 0.34 0.19 0.36 0.05 0.15 0.30 0.00 -0.00 -0.00 -0.08 -0.00 -0.00 -0.00 -0.00 0.06 0.38 0.04 0.14 5 0.00 0.00 0.26 0.00 0.67 0.05 0.00 -0.00 -0.16 -0.00 -0.00 -0.00 -0.00 -0.00 -0.08 0.30 -0.17 -0.17 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.76 -0.68 -0.28 -0.11 -0.37 -0.42 -0.40 -0.88 -0.41 -0.67 -1.00 7 0.01 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.17 -0.04 -0.57 -0.82 -0.50 -0.22 -0.07 -0.62 -0.13 -0.18 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.32 -0.35 -0.39 -0.77 -1.63 -1.12 -0.60 -0.47 -0.99 -1.00 -1.00 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 1.00 5 1.00 6 1.00 7 1.00 8 1.00 9 1.00 10 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.31 0.02 0.11 0.29 0.10 0.33 0.37 0.06 0.39 0.35 0.15 0.13 0.15 0.45 0.07 0.19 0.03 0.06 1 0.12 0.02 0.06 0.41 0.32 0.24 0.34 0.28 0.22 0.06 0.33 0.27 0.25 0.23 0.43 0.09 0.45 0.27 2 0.19 0.11 0.19 0.25 0.07 0.42 0.32 0.35 0.15 0.05 0.00 0.24 0.22 0.39 0.44 0.11 0.19 0.10 3 0.15 0.37 0.21 0.41 0.25 0.04 0.37 0.04 0.05 0.22 0.31 0.35 0.35 0.08 0.38 0.01 0.25 0.29 4 0.17 0.45 0.24 0.32 0.01 0.00 0.19 0.34 0.17 0.19 0.18 0.34 0.02 0.24 0.03 0.41 0.26 0.00 5 0.29 0.10 0.07 0.34 0.04 0.30 0.39 0.27 0.39 0.16 0.33 0.45 0.06 0.19 0.23 0.04 0.36 0.04 6 0.13 0.15 0.22 0.40 0.14 0.30 0.11 0.45 0.14 0.17 0.26 0.16 0.36 0.10 0.17 0.32 0.14 0.08 7 0.25 0.25 0.24 0.45 0.17 0.45 0.30 0.35 0.41 0.40 0.11 0.26 0.32 0.08 0.22 0.34 0.05 0.09 8 0.16 0.27 0.10 0.23 0.08 0.21 0.19 0.16 0.06 0.04 0.17 0.05 0.39 0.11 0.26 0.25 0.13 0.05 9 0.17 0.17 0.00 0.13 0.12 0.03 0.39 0.11 0.01 0.29 0.43 0.20 0.21 0.43 0.39 0.18 0.19 0.27 10 0.26 0.23 0.43 0.04 0.25 0.36 0.21 0.36 0.37 0.36 0.08 0.14 0.25 0.24 0.30 0.33 0.04 0.07 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.00 0.08 0.00 0.00 0.00 0.00 -0.82 -0.58 -0.32 -1.07 -1.09 -0.00 -0.63 -0.21 -0.74 -1.00 -0.15 1 0.36 0.00 0.00 0.51 0.11 0.72 0.76 -0.12 -0.00 -0.00 -0.05 -0.00 -0.00 -0.00 0.56 -0.34 0.13 0.22 2 0.72 0.68 0.32 1.10 0.10 0.84 0.68 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.20 0.97 0.33 -0.07 3 0.00 0.00 0.36 0.35 0.36 0.82 0.00 -0.00 -0.00 -0.19 -0.29 -0.13 -0.00 -0.20 0.67 0.20 -0.00 0.14 4 0.18 0.14 0.26 0.68 0.09 0.38 0.36 -0.00 -0.00 -0.00 -0.00 -0.00 -0.07 -0.00 0.14 0.15 0.33 0.10 5 0.01 0.55 0.50 0.00 0.00 0.21 0.00 -0.00 -0.27 -0.00 -0.44 -0.25 -0.00 -0.00 0.44 0.83 -0.24 -0.01 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.89 -0.85 -0.48 -0.77 -0.90 -0.21 -0.30 -0.09 -0.69 -0.83 -0.03 7 0.00 0.00 0.00 0.00 0.01 0.00 0.00 -0.79 -0.59 -0.65 -0.21 -0.55 -0.19 -0.37 -0.17 -0.71 -0.10 0.03 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.24 -0.48 -0.95 -1.13 -0.71 -1.40 -0.30 -0.76 -1.00 -0.47 -0.39 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.29 -0.12 -0.00 -0.17 -0.33 -0.17 -0.33 -0.36 -0.28 -0.16 -0.24 -0.22 -0.10 -0.13 -0.02 -0.38 -0.23 -0.02 1 -0.36 -0.13 -0.05 -0.07 -0.41 -0.30 -0.38 -0.06 -0.40 -0.42 -0.44 -0.03 -0.27 -0.03 -0.32 -0.31 -0.35 -0.40 2 -0.30 -0.07 -0.40 -0.06 -0.10 -0.21 -0.16 -0.22 -0.06 -0.36 -0.40 -0.42 -0.23 -0.22 -0.20 -0.33 -0.45 -0.06 3 -0.05 -0.08 -0.07 -0.30 -0.44 -0.23 -0.40 -0.25 -0.13 -0.31 -0.11 -0.13 -0.13 -0.34 -0.15 -0.05 -0.36 -0.13 4 -0.45 -0.34 -0.41 -0.39 -0.15 -0.10 -0.40 -0.32 -0.19 -0.13 -0.29 -0.39 -0.43 -0.29 -0.13 -0.05 -0.39 -0.01 5 -0.09 -0.38 -0.00 -0.12 -0.07 -0.42 -0.01 -0.12 -0.26 -0.28 -0.16 -0.06 -0.08 -0.43 -0.23 -0.28 -0.28 -0.07 6 -0.34 -0.38 -0.15 -0.44 -0.41 -0.19 -0.25 -0.41 -0.34 -0.22 -0.43 -0.36 -0.25 -0.28 -0.06 -0.12 -0.15 -0.16 7 -0.17 -0.39 -0.40 -0.26 -0.40 -0.20 -0.10 -0.14 -0.42 -0.21 -0.18 -0.25 -0.15 -0.21 -0.13 -0.41 -0.14 -0.14 8 -0.38 -0.03 -0.10 -0.21 -0.13 -0.04 -0.19 -0.00 -0.09 -0.38 -0.01 -0.27 -0.24 -0.24 -0.13 -0.18 -0.37 -0.21 9 -0.43 -0.08 -0.20 -0.29 -0.10 -0.27 -0.08 -0.43 -0.22 -0.37 -0.27 -0.24 -0.15 -0.22 -0.01 -0.45 -0.35 -0.31 10 -0.38 -0.44 -0.20 -0.31 -0.42 -0.23 -0.03 -0.31 -0.11 -0.35 -0.01 -0.00 -0.00 -0.39 -0.45 -0.14 -0.03 -0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 1.05 0.88 0.59 0.61 0.00 0.70 0.64 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.24 0.74 1.00 0.55 1 0.27 0.26 0.00 0.41 0.00 0.00 0.00 -0.00 -0.23 -0.33 -0.21 -0.20 -0.00 -0.02 -0.04 -0.82 -0.52 -0.02 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.36 -0.77 -0.71 -0.39 -1.00 -0.82 -0.67 -0.11 -0.74 -0.97 -0.31 3 0.00 0.00 0.00 0.00 0.00 0.01 0.00 -0.00 -0.15 -0.50 -0.38 -0.33 -0.20 -0.00 -0.39 -0.20 -0.12 -0.36 4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.46 -0.00 -0.84 -0.48 -0.36 -0.13 -0.08 -0.28 -0.33 0.13 5 0.00 0.02 0.00 0.00 0.12 0.33 0.00 -0.41 -0.00 -0.44 -0.33 -0.90 -0.56 -0.04 -0.24 -0.27 -0.48 -0.16 6 0.74 1.20 0.11 0.90 0.84 0.65 0.87 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.60 0.01 0.53 0.12 7 0.47 0.89 0.91 0.62 0.26 0.37 0.01 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.07 0.61 0.29 0.01 8 1.30 1.17 0.98 1.61 1.09 0.59 0.65 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.09 0.93 0.94 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 1 -1.00 2 -1.00 3 -1.00 4 -1.00 5 -1.00 6 -1.00 7 -1.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.45 -0.28 -0.30 -0.41 -0.17 -0.39 -0.22 -0.45 -0.28 -0.40 -0.18 -0.20 -0.16 -0.18 -0.10 -0.13 -0.14 -0.35 1 -0.09 -0.27 -0.09 -0.14 -0.02 -0.36 -0.21 -0.05 -0.05 -0.01 -0.02 -0.45 -0.03 -0.09 -0.01 -0.05 -0.39 -0.05 2 -0.17 -0.15 -0.37 -0.35 -0.32 -0.03 -0.24 -0.31 -0.35 -0.41 -0.00 -0.37 -0.18 -0.26 -0.09 -0.44 -0.09 -0.17 3 -0.42 -0.17 -0.11 -0.31 -0.32 -0.11 -0.20 -0.10 -0.34 -0.15 -0.24 -0.22 -0.22 -0.08 -0.40 -0.02 -0.23 -0.38 4 -0.13 -0.17 -0.06 -0.13 -0.32 -0.42 -0.28 -0.44 -0.03 -0.26 -0.38 -0.45 -0.08 -0.06 -0.04 -0.33 -0.27 -0.38 5 -0.32 -0.38 -0.19 -0.19 -0.33 -0.01 -0.15 -0.08 -0.31 -0.27 -0.07 -0.11 -0.21 -0.22 -0.18 -0.27 -0.19 -0.15 6 -0.30 -0.16 -0.09 -0.25 -0.23 -0.44 -0.25 -0.16 -0.05 -0.13 -0.20 -0.09 -0.14 -0.18 -0.15 -0.22 -0.37 -0.38 7 -0.20 -0.14 -0.12 -0.10 -0.42 -0.42 -0.14 -0.04 -0.44 -0.11 -0.10 -0.17 -0.06 -0.29 -0.22 -0.24 -0.01 -0.45 8 -0.31 -0.11 -0.16 -0.21 -0.16 -0.39 -0.12 -0.36 -0.36 -0.29 -0.24 -0.24 -0.20 -0.18 -0.33 -0.39 -0.20 -0.02 9 -0.41 -0.14 -0.12 -0.21 -0.01 -0.37 -0.03 -0.22 -0.38 -0.22 -0.09 -0.22 -0.19 -0.17 -0.13 -0.32 -0.30 -0.21 10 -0.31 -0.05 -0.02 -0.36 -0.04 -0.15 -0.03 -0.12 -0.36 -0.21 -0.40 -0.03 -0.04 -0.03 -0.23 -0.01 -0.02 -0.41 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.20 0.84 0.11 0.00 0.55 1.24 0.55 -0.00 -0.02 -0.00 -0.00 -0.00 -0.00 -0.00 -0.20 0.98 1.00 0.30 1 0.00 0.00 0.00 0.00 0.00 0.19 0.00 -0.14 -0.87 -0.50 -0.00 -0.34 -0.28 -0.53 -0.24 -0.34 0.23 -0.09 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.34 -0.82 -1.02 -0.75 -0.74 -0.56 -0.68 -0.71 -1.00 -0.65 -0.56 3 0.23 0.18 0.00 0.00 0.00 0.00 0.00 -0.00 -0.27 -0.00 -0.00 -0.21 -0.00 -0.28 -0.21 -0.24 0.02 0.00 4 0.09 0.00 0.00 0.00 0.00 0.00 0.00 -0.08 -0.00 -0.14 -0.00 -0.50 -0.01 -0.25 0.23 -0.20 -0.14 -0.66 5 0.18 0.49 0.00 0.00 0.03 0.00 0.00 -0.79 -0.36 -0.49 -0.39 -0.69 -0.00 -0.09 0.08 -0.84 0.10 -0.25 6 0.64 0.76 0.08 0.50 0.62 0.79 0.68 -0.00 -0.06 -0.00 -0.00 -0.00 -0.00 -0.00 0.28 0.24 0.86 0.87 7 0.32 0.24 0.23 0.18 0.76 0.62 0.28 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.13 0.73 0.09 0.87 8 1.23 0.50 0.27 0.51 1.08 2.00 0.60 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 1.00 1.00 1.00 <pre><code>x = Input(shape=(5, 7, 8))\n\nlayer = MonoDense(\n    units=12,\n    activation=activation,\n    monotonicity_indicator=[1]*3+[-1]*3+[0]*2,\n    is_convex=False,\n    is_concave=False,\n)\n\ny = layer(x)\n\nmodel = Model(inputs=x, outputs=y)\n\nmodel.summary()\n\ndisplay_kernel(layer.monotonicity_indicator)\n</code></pre> <pre><code>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 5, 7, 8)]         0\n\n mono_dense_5 (MonoDense)    (None, 5, 7, 12)          108\n\n=================================================================\nTotal params: 108\nTrainable params: 108\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> 0 0 1.00 1 1.00 2 1.00 3 -1.00 4 -1.00 5 -1.00 6 0.00 7 0.00"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Guide</li> <li>API<ul> <li>mono_dense_keras<ul> <li>MonoDense</li> </ul> </li> </ul> </li> <li>Releases</li> </ul>"},{"location":"TopLevel/","title":"TopLevel","text":""},{"location":"TopLevel/#dummy","title":"dummy","text":"<pre><code> dummy ()\n</code></pre>"},{"location":"changelog_not_found/","title":"Releases","text":""},{"location":"changelog_not_found/#changelogmd-file-not-found","title":"CHANGELOG.md file not found.","text":"<p>To generate the changelog file, please run the following command from the project root directory. </p> <pre><code>nbdev_changelog\n</code></pre> <p>If you do not want this page to be rendered as part of the documentation, please remove the following line from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- [Releases]{changelog}\n</code></pre>"},{"location":"cli_commands_not_found/","title":"No CLI commands found in console_scripts in settings.ini file.","text":"<p>For documenting CLI commands, please add command line executables in <code>console_scripts</code> in <code>settings.ini</code> file. </p> <p>If you do not want this page to be rendered as part of the documentation, please remove the following lines from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- CLI\n{cli}\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/","title":"MonoDense","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense","title":"<code>mono_dense_keras.MonoDense</code>","text":"<p>         Bases: <code>Dense</code></p> <p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>class MonoDense(Dense):\n\"\"\"Monotonic counterpart of the regular Dense Layer of tf.keras\"\"\"\n\n    def __init__(\n        self,\n        units: int,\n        *,\n        activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n        monotonicity_indicator: ArrayLike = 1,\n        is_convex: bool = False,\n        is_concave: bool = False,\n        activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0),\n        **kwargs: Dict[str, Any],\n    ):\n\"\"\"Constructs a new MonoDense instance.\n\n        Params:\n            units: Positive integer, dimensionality of the output space.\n            activation: Activation function to use, it is assumed to be convex monotonically\n                increasing function such as \"relu\" or \"elu\"\n            monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n                monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n                -1 for monotonically decreasing and 0 for non-monotonic.\n            is_convex: convex if set to True\n            is_concave: concave if set to True\n            activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n                Ignored if is_convex or is_concave is set to True\n            **kwargs: passed as kwargs to the constructor of `Dense`\n\n        Raise:\n            ValueError:\n                - if both **is_concave** and **is_convex** are set to **True**, or\n                - if any component of activation_weights is negative or there is not exactly three components\n        \"\"\"\n        if is_convex and is_concave:\n            raise ValueError(\n                \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n            )\n\n        if len(activation_weights) != 3:\n            raise ValueError(\n                f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n            )\n\n        if (np.array(activation_weights) &lt; 0).any():\n            raise ValueError(\n                f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n            )\n\n        super(MonoDense, self).__init__(units=units, activation=None, **kwargs)\n\n        self.units = units\n        self.org_activation = activation\n        self.activation_weights = activation_weights\n        self.monotonicity_indicator = monotonicity_indicator\n        self.is_convex = is_convex\n        self.is_concave = is_concave\n\n        (\n            self.convex_activation,\n            self.concave_activation,\n            self.saturated_activation,\n        ) = get_activation_functions(self.org_activation)\n\n    def build(\n        self, input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]\n    ) -&gt; None:\n\"\"\"Build\n\n        Args:\n            input_shape: input tensor\n            args: positional arguments passed to Dense.build()\n            kwargs: keyword arguments passed to Dense.build()\n        \"\"\"\n        super(MonoDense, self).build(input_shape, *args, **kwargs)\n        self.monotonicity_indicator = get_monotonicity_indicator(\n            monotonicity_indicator=self.monotonicity_indicator,\n            input_shape=input_shape,\n            units=self.units,\n        )\n\n    def call(self, inputs: TensorLike) -&gt; TensorLike:\n\"\"\"Call\n\n        Args:\n            inputs: input tensor of shape (batch_size, ..., x_length)\n\n        Returns:\n            N-D tensor with shape: `(batch_size, ..., units)`.\n\n        \"\"\"\n        # calculate W'*x+y after we replace the kernal according to monotonicity vector\n        with replace_kernel_using_monotonicity_indicator(\n            self, monotonicity_indicator=self.monotonicity_indicator\n        ):\n            h = super(MonoDense, self).call(inputs)\n\n        y = apply_activations(\n            h,\n            units=self.units,\n            convex_activation=self.convex_activation,\n            concave_activation=self.concave_activation,\n            saturated_activation=self.saturated_activation,\n            is_convex=self.is_convex,\n            is_concave=self.is_concave,\n            activation_weights=self.activation_weights,\n        )\n\n        return y\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense-attributes","title":"Attributes","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.activation_weights","title":"<code>activation_weights = activation_weights</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.is_concave","title":"<code>is_concave = is_concave</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.is_convex","title":"<code>is_convex = is_convex</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.monotonicity_indicator","title":"<code>monotonicity_indicator = monotonicity_indicator</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.org_activation","title":"<code>org_activation = activation</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.units","title":"<code>units = units</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense-functions","title":"Functions","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.__init__","title":"<code>__init__(units: int, *, activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None, monotonicity_indicator: ArrayLike = 1, is_convex: bool = False, is_concave: bool = False, activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0), **kwargs: Dict[str, Any])</code>","text":"<p>Constructs a new MonoDense instance.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Positive integer, dimensionality of the output space.</p> required <code>activation</code> <code>Optional[Union[str, Callable[[TensorLike], TensorLike]]]</code> <p>Activation function to use, it is assumed to be convex monotonically increasing function such as \"relu\" or \"elu\"</p> <code>None</code> <code>monotonicity_indicator</code> <code>ArrayLike</code> <p>Vector to indicate which of the inputs are monotonically increasing or monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing, -1 for monotonically decreasing and 0 for non-monotonic.</p> <code>1</code> <code>is_convex</code> <code>bool</code> <p>convex if set to True</p> <code>False</code> <code>is_concave</code> <code>bool</code> <p>concave if set to True</p> <code>False</code> <code>activation_weights</code> <code>Tuple[float, float, float]</code> <p>relative weights for each type of activation, the default is (1.0, 1.0, 1.0). Ignored if is_convex or is_concave is set to True</p> <code>(7.0, 7.0, 2.0)</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>passed as kwargs to the constructor of <code>Dense</code></p> <code>{}</code> Raise <p>ValueError:     - if both is_concave and is_convex are set to True, or     - if any component of activation_weights is negative or there is not exactly three components</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def __init__(\n    self,\n    units: int,\n    *,\n    activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    monotonicity_indicator: ArrayLike = 1,\n    is_convex: bool = False,\n    is_concave: bool = False,\n    activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0),\n    **kwargs: Dict[str, Any],\n):\n\"\"\"Constructs a new MonoDense instance.\n\n    Params:\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use, it is assumed to be convex monotonically\n            increasing function such as \"relu\" or \"elu\"\n        monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n            monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n            -1 for monotonically decreasing and 0 for non-monotonic.\n        is_convex: convex if set to True\n        is_concave: concave if set to True\n        activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n            Ignored if is_convex or is_concave is set to True\n        **kwargs: passed as kwargs to the constructor of `Dense`\n\n    Raise:\n        ValueError:\n            - if both **is_concave** and **is_convex** are set to **True**, or\n            - if any component of activation_weights is negative or there is not exactly three components\n    \"\"\"\n    if is_convex and is_concave:\n        raise ValueError(\n            \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n        )\n\n    if len(activation_weights) != 3:\n        raise ValueError(\n            f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n        )\n\n    if (np.array(activation_weights) &lt; 0).any():\n        raise ValueError(\n            f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n        )\n\n    super(MonoDense, self).__init__(units=units, activation=None, **kwargs)\n\n    self.units = units\n    self.org_activation = activation\n    self.activation_weights = activation_weights\n    self.monotonicity_indicator = monotonicity_indicator\n    self.is_convex = is_convex\n    self.is_concave = is_concave\n\n    (\n        self.convex_activation,\n        self.concave_activation,\n        self.saturated_activation,\n    ) = get_activation_functions(self.org_activation)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_loss","title":"<code>keras.engine.base_layer.Layer.add_loss(losses, **kwargs)</code>","text":"<p>Add loss tensor(s), potentially dependent on layer inputs.</p> <p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p> <p>This method can be used inside a subclassed layer or model's <code>call</code> function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p> <p>Example:</p> <pre><code>class MyLayer(tf.keras.layers.Layer):\n  def call(self, inputs):\n    self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n    return inputs\n</code></pre> <p>The same code works in distributed training: the input to <code>add_loss()</code> is treated like a regularization loss and averaged across replicas by the training loop (both built-in <code>Model.fit()</code> and compliant custom training loops).</p> <p>The <code>add_loss</code> method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's <code>Input</code>s. These losses become part of the model's topology and are tracked in <code>get_config</code>.</p> <p>Example:</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\n# Activity regularization.\nmodel.add_loss(tf.abs(tf.reduce_mean(x)))\n</code></pre> <p>If this is not the case for your loss (if, for example, your loss references a <code>Variable</code> of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized.</p> <p>Example:</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nd = tf.keras.layers.Dense(10)\nx = d(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\n# Weight regularization.\nmodel.add_loss(lambda: tf.reduce_mean(d.kernel))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>losses</code> <p>Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor.</p> required <code>**kwargs</code> <p>Used for backwards compatibility only.</p> <code>{}</code> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def add_loss(self, losses, **kwargs):\n\"\"\"Add loss tensor(s), potentially dependent on layer inputs.\n\n    Some losses (for instance, activity regularization losses) may be\n    dependent on the inputs passed when calling a layer. Hence, when reusing\n    the same layer on different inputs `a` and `b`, some entries in\n    `layer.losses` may be dependent on `a` and some on `b`. This method\n    automatically keeps track of dependencies.\n\n    This method can be used inside a subclassed layer or model's `call`\n    function, in which case `losses` should be a Tensor or list of Tensors.\n\n    Example:\n\n    ```python\n    class MyLayer(tf.keras.layers.Layer):\n      def call(self, inputs):\n        self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n        return inputs\n    ```\n\n    The same code works in distributed training: the input to `add_loss()`\n    is treated like a regularization loss and averaged across replicas\n    by the training loop (both built-in `Model.fit()` and compliant custom\n    training loops).\n\n    The `add_loss` method can also be called directly on a Functional Model\n    during construction. In this case, any loss Tensors passed to this Model\n    must be symbolic and be able to be traced back to the model's `Input`s.\n    These losses become part of the model's topology and are tracked in\n    `get_config`.\n\n    Example:\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    # Activity regularization.\n    model.add_loss(tf.abs(tf.reduce_mean(x)))\n    ```\n\n    If this is not the case for your loss (if, for example, your loss\n    references a `Variable` of one of the model's layers), you can wrap your\n    loss in a zero-argument lambda. These losses are not tracked as part of\n    the model's topology since they can't be serialized.\n\n    Example:\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    d = tf.keras.layers.Dense(10)\n    x = d(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    # Weight regularization.\n    model.add_loss(lambda: tf.reduce_mean(d.kernel))\n    ```\n\n    Args:\n      losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n        losses may also be zero-argument callables which create a loss\n        tensor.\n      **kwargs: Used for backwards compatibility only.\n    \"\"\"\n    kwargs.pop(\"inputs\", None)\n    if kwargs:\n        raise TypeError(f\"Unknown keyword arguments: {kwargs.keys()}\")\n\n    def _tag_callable(loss):\n\"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\"\n        if callable(loss):\n            # We run the loss without autocasting, as regularizers are often\n            # numerically unstable in float16.\n            with autocast_variable.enable_auto_cast_variables(None):\n                loss = loss()\n        if loss is None:\n            # Will be filtered out when computing the .losses property\n            return None\n        if not tf.is_tensor(loss):\n            loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n        loss._unconditional_loss = True\n        return loss\n\n    losses = tf.nest.flatten(losses)\n\n    callable_losses = []\n    eager_losses = []\n    symbolic_losses = []\n    for loss in losses:\n        if callable(loss):\n            callable_losses.append(functools.partial(_tag_callable, loss))\n            continue\n        if loss is None:\n            continue\n        if not tf.is_tensor(loss) and not isinstance(\n            loss, keras_tensor.KerasTensor\n        ):\n            loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n        # TF Functions should take the eager path.\n        if (\n            tf_utils.is_symbolic_tensor(loss)\n            or isinstance(loss, keras_tensor.KerasTensor)\n        ) and not base_layer_utils.is_in_tf_function():\n            symbolic_losses.append(loss)\n        elif tf.is_tensor(loss):\n            eager_losses.append(loss)\n\n    self._callable_losses.extend(callable_losses)\n\n    in_call_context = base_layer_utils.call_context().in_call\n    if eager_losses and not in_call_context:\n        raise ValueError(\n            \"Expected a symbolic Tensors or a callable for the loss value. \"\n            \"Please wrap your loss computation in a zero argument `lambda`.\"\n        )\n\n    self._eager_losses.extend(eager_losses)\n\n    for symbolic_loss in symbolic_losses:\n        if getattr(self, \"_is_graph_network\", False):\n            self._graph_network_add_loss(symbolic_loss)\n        else:\n            # Possible a loss was added in a Layer's `build`.\n            self._losses.append(symbolic_loss)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_metric","title":"<code>keras.engine.base_layer.Layer.add_metric(value, name = None, **kwargs)</code>","text":"<p>Adds metric tensor to the layer.</p> <p>This method can be used inside the <code>call()</code> method of a subclassed layer or model.</p> <pre><code>class MyMetricLayer(tf.keras.layers.Layer):\n  def __init__(self):\n    super(MyMetricLayer, self).__init__(name='my_metric_layer')\n    self.mean = tf.keras.metrics.Mean(name='metric_1')\n\n  def call(self, inputs):\n    self.add_metric(self.mean(inputs))\n    self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n    return inputs\n</code></pre> <p>This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's <code>Input</code>s. These metrics become part of the model's topology and are tracked when you save the model via <code>save()</code>.</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.add_metric(math_ops.reduce_sum(x), name='metric_1')\n</code></pre> <p>Note: Calling <code>add_metric()</code> with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs.</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>Metric tensor.</p> required <code>name</code> <p>String metric name.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for backward compatibility. Accepted values: <code>aggregation</code> - When the <code>value</code> tensor provided is not the result of calling a <code>keras.Metric</code> instance, it will be aggregated by default using a <code>keras.Metric.Mean</code>.</p> <code>{}</code> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def add_metric(self, value, name=None, **kwargs):\n\"\"\"Adds metric tensor to the layer.\n\n    This method can be used inside the `call()` method of a subclassed layer\n    or model.\n\n    ```python\n    class MyMetricLayer(tf.keras.layers.Layer):\n      def __init__(self):\n        super(MyMetricLayer, self).__init__(name='my_metric_layer')\n        self.mean = tf.keras.metrics.Mean(name='metric_1')\n\n      def call(self, inputs):\n        self.add_metric(self.mean(inputs))\n        self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n        return inputs\n    ```\n\n    This method can also be called directly on a Functional Model during\n    construction. In this case, any tensor passed to this Model must\n    be symbolic and be able to be traced back to the model's `Input`s. These\n    metrics become part of the model's topology and are tracked when you\n    save the model via `save()`.\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n    ```\n\n    Note: Calling `add_metric()` with the result of a metric object on a\n    Functional Model, as shown in the example below, is not supported. This\n    is because we cannot trace the metric result tensor back to the model's\n    inputs.\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n    ```\n\n    Args:\n      value: Metric tensor.\n      name: String metric name.\n      **kwargs: Additional keyword arguments for backward compatibility.\n        Accepted values:\n        `aggregation` - When the `value` tensor provided is not the result\n        of calling a `keras.Metric` instance, it will be aggregated by\n        default using a `keras.Metric.Mean`.\n    \"\"\"\n    kwargs_keys = list(kwargs.keys())\n    if len(kwargs_keys) &gt; 1 or (\n        len(kwargs_keys) == 1 and kwargs_keys[0] != \"aggregation\"\n    ):\n        raise TypeError(\n            f\"Unknown keyword arguments: {kwargs.keys()}. \"\n            \"Expected `aggregation`.\"\n        )\n\n    from_metric_obj = hasattr(value, \"_metric_obj\")\n    is_symbolic = isinstance(value, keras_tensor.KerasTensor)\n    in_call_context = base_layer_utils.call_context().in_call\n\n    if name is None and not from_metric_obj:\n        # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we\n        # use metric name to lookup a metric. Without a name, a new Mean\n        # metric wrapper will be created on every model/layer call. So, we\n        # raise an error when no name is provided. We will do the same for\n        # symbolic mode for consistency although a name will be generated if\n        # no name is provided.\n\n        # We will not raise this error in the foll use case for the sake of\n        # consistency as name in provided in the metric constructor.\n        # mean = metrics.Mean(name='my_metric')\n        # model.add_metric(mean(outputs))\n        raise ValueError(\n            \"Please provide a name for your metric like \"\n            \"`self.add_metric(tf.reduce_sum(inputs), \"\n            \"name='mean_activation')`\"\n        )\n    elif from_metric_obj:\n        name = value._metric_obj.name\n\n    if not in_call_context and not is_symbolic:\n        raise ValueError(\n            \"Expected a symbolic Tensor for the metric value, received: \"\n            + str(value)\n        )\n\n    # If a metric was added in a Layer's `call` or `build`.\n    if in_call_context or not getattr(self, \"_is_graph_network\", False):\n        # TF Function path should take the eager path.\n\n        # If the given metric is available in `metrics` list we just update\n        # state on it, otherwise we create a new metric instance and\n        # add it to the `metrics` list.\n        metric_obj = getattr(value, \"_metric_obj\", None)\n        # Tensors that come from a Metric object already updated the Metric\n        # state.\n        should_update_state = not metric_obj\n        name = metric_obj.name if metric_obj else name\n\n        with self._metrics_lock:\n            match = self._get_existing_metric(name)\n            if match:\n                metric_obj = match\n            elif metric_obj:\n                self._metrics.append(metric_obj)\n            else:\n                # Build the metric object with the value's dtype if it\n                # defines one\n                metric_obj = metrics_mod.Mean(\n                    name=name, dtype=getattr(value, \"dtype\", None)\n                )\n                self._metrics.append(metric_obj)\n\n        if should_update_state:\n            metric_obj(value)\n    else:\n        if from_metric_obj:\n            raise ValueError(\n                \"Using the result of calling a `Metric` object \"\n                \"when calling `add_metric` on a Functional \"\n                \"Model is not supported. Please pass the \"\n                \"Tensor to monitor directly.\"\n            )\n\n        # Insert layers into the Keras Graph Network.\n        aggregation = None if from_metric_obj else \"mean\"\n        self._graph_network_add_metric(value, aggregation, name)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_update","title":"<code>keras.engine.base_layer.Layer.add_update(updates)</code>","text":"<p>Add update op(s), potentially dependent on layer inputs.</p> <p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p> <p>This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution).</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <p>Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting <code>trainable=False</code> on this Layer, when executing in Eager mode.</p> required Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef add_update(self, updates):\n\"\"\"Add update op(s), potentially dependent on layer inputs.\n\n    Weight updates (for instance, the updates of the moving mean and\n    variance in a BatchNormalization layer) may be dependent on the inputs\n    passed when calling a layer. Hence, when reusing the same layer on\n    different inputs `a` and `b`, some entries in `layer.updates` may be\n    dependent on `a` and some on `b`. This method automatically keeps track\n    of dependencies.\n\n    This call is ignored when eager execution is enabled (in that case,\n    variable updates are run on the fly and thus do not need to be tracked\n    for later execution).\n\n    Args:\n      updates: Update op, or list/tuple of update ops, or zero-arg callable\n        that returns an update op. A zero-arg callable should be passed in\n        order to disable running the updates by setting `trainable=False`\n        on this Layer, when executing in Eager mode.\n    \"\"\"\n    call_context = base_layer_utils.call_context()\n    # No need to run updates during Functional API construction.\n    if call_context.in_keras_graph:\n        return\n\n    # Callable updates are disabled by setting `trainable=False`.\n    if not call_context.frozen:\n        for update in tf.nest.flatten(updates):\n            if callable(update):\n                update()\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_variable","title":"<code>keras.engine.base_layer.Layer.add_variable(*args, **kwargs)</code>","text":"<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef add_variable(self, *args, **kwargs):\n\"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\n    warnings.warn(\n        \"`layer.add_variable` is deprecated and \"\n        \"will be removed in a future version. \"\n        \"Please use the `layer.add_weight()` method instead.\",\n        stacklevel=2,\n    )\n    return self.add_weight(*args, **kwargs)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_weight","title":"<code>keras.engine.base_layer.Layer.add_weight(name = None, shape = None, dtype = None, initializer = None, regularizer = None, trainable = None, constraint = None, use_resource = None, synchronization = tf.VariableSynchronization.AUTO, aggregation = tf.VariableAggregation.NONE, **kwargs)</code>","text":"<p>Adds a new variable to the layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Variable name.</p> <code>None</code> <code>shape</code> <p>Variable shape. Defaults to scalar if unspecified.</p> <code>None</code> <code>dtype</code> <p>The type of the variable. Defaults to <code>self.dtype</code>.</p> <code>None</code> <code>initializer</code> <p>Initializer instance (callable).</p> <code>None</code> <code>regularizer</code> <p>Regularizer instance (callable).</p> <code>None</code> <code>trainable</code> <p>Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code> is set to <code>ON_READ</code>.</p> <code>None</code> <code>constraint</code> <p>Constraint instance (callable).</p> <code>None</code> <code>use_resource</code> <p>Whether to use a <code>ResourceVariable</code> or not. See this guide  for more information.</p> <code>None</code> <code>synchronization</code> <p>Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class <code>tf.VariableSynchronization</code>. By default the synchronization is set to <code>AUTO</code> and the current <code>DistributionStrategy</code> chooses when to synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>, <code>trainable</code> must not be set to <code>True</code>.</p> <code>tf.VariableSynchronization.AUTO</code> <code>aggregation</code> <p>Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class <code>tf.VariableAggregation</code>.</p> <code>tf.VariableAggregation.NONE</code> <code>**kwargs</code> <p>Additional keyword arguments. Accepted values are <code>getter</code>, <code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The variable created.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as <code>ON_READ</code>.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.for_subclass_implementers\ndef add_weight(\n    self,\n    name=None,\n    shape=None,\n    dtype=None,\n    initializer=None,\n    regularizer=None,\n    trainable=None,\n    constraint=None,\n    use_resource=None,\n    synchronization=tf.VariableSynchronization.AUTO,\n    aggregation=tf.VariableAggregation.NONE,\n    **kwargs,\n):\n\"\"\"Adds a new variable to the layer.\n\n    Args:\n      name: Variable name.\n      shape: Variable shape. Defaults to scalar if unspecified.\n      dtype: The type of the variable. Defaults to `self.dtype`.\n      initializer: Initializer instance (callable).\n      regularizer: Regularizer instance (callable).\n      trainable: Boolean, whether the variable should be part of the layer's\n        \"trainable_variables\" (e.g. variables, biases)\n        or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n        Note that `trainable` cannot be `True` if `synchronization`\n        is set to `ON_READ`.\n      constraint: Constraint instance (callable).\n      use_resource: Whether to use a `ResourceVariable` or not.\n        See [this guide](\n        https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n         for more information.\n      synchronization: Indicates when a distributed a variable will be\n        aggregated. Accepted values are constants defined in the class\n        `tf.VariableSynchronization`. By default the synchronization is set\n        to `AUTO` and the current `DistributionStrategy` chooses when to\n        synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n        must not be set to `True`.\n      aggregation: Indicates how a distributed variable will be aggregated.\n        Accepted values are constants defined in the class\n        `tf.VariableAggregation`.\n      **kwargs: Additional keyword arguments. Accepted values are `getter`,\n        `collections`, `experimental_autocast` and `caching_device`.\n\n    Returns:\n      The variable created.\n\n    Raises:\n      ValueError: When giving unsupported dtype and no initializer or when\n        trainable has been set to True with synchronization set as\n        `ON_READ`.\n    \"\"\"\n    if shape is None:\n        shape = ()\n    kwargs.pop(\"partitioner\", None)  # Ignored.\n    # Validate optional keyword arguments.\n    for kwarg in kwargs:\n        if kwarg not in [\n            \"collections\",\n            \"experimental_autocast\",\n            \"caching_device\",\n            \"getter\",\n            \"layout\",\n        ]:\n            raise TypeError(\"Unknown keyword argument:\", kwarg)\n    collections_arg = kwargs.pop(\"collections\", None)\n    # 'experimental_autocast' can be set to False by the caller to indicate\n    # an AutoCastVariable should never be created.\n    autocast = kwargs.pop(\"experimental_autocast\", True)\n    # See the docstring for tf.Variable about the details for\n    # caching_device.\n    caching_device = kwargs.pop(\"caching_device\", None)\n\n    layout = kwargs.pop(\"layout\", None)\n    # Specially handling of auto layout fetch, based on the variable name\n    # and attribute name. For built-in keras layers, usually the variable\n    # name, eg 'kernel', will match with a 'kernel_layout' attribute name on\n    # the instance. We will try to do this auto fetch if layout is not\n    # explicitly specified. This is mainly a quick workaround for not\n    # applying too many interface change to built-in layers, until DTensor\n    # is a public API.  Also see dtensor.utils.allow_initializer_layout for\n    # more details.\n    # TODO(scottzhu): Remove this once dtensor is public to end user.\n    if not layout and name:\n        layout = getattr(self, name + \"_layout\", None)\n\n    if dtype is None:\n        dtype = self.dtype or backend.floatx()\n    dtype = tf.as_dtype(dtype)\n    if self._dtype_policy.variable_dtype is None:\n        # The policy is \"_infer\", so we infer the policy from the variable\n        # dtype.\n        self._set_dtype_policy(policy.Policy(dtype.base_dtype.name))\n    initializer = initializers.get(initializer)\n    regularizer = regularizers.get(regularizer)\n    constraint = constraints.get(constraint)\n\n    if synchronization == tf.VariableSynchronization.ON_READ:\n        if trainable:\n            raise ValueError(\n                \"Synchronization value can be set to \"\n                \"VariableSynchronization.ON_READ only for non-trainable \"\n                \"variables. You have specified trainable=True and \"\n                \"synchronization=VariableSynchronization.ON_READ.\"\n            )\n        else:\n            # Set trainable to be false when variable is to be synced on\n            # read.\n            trainable = False\n    elif trainable is None:\n        trainable = True\n\n    # Initialize variable when no initializer provided\n    if initializer is None:\n        # If dtype is DT_FLOAT, provide a uniform unit scaling initializer\n        if dtype.is_floating:\n            initializer = initializers.get(\"glorot_uniform\")\n        # If dtype is DT_INT/DT_UINT, provide a default value `zero`\n        # If dtype is DT_BOOL, provide a default value `FALSE`\n        elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n            initializer = initializers.get(\"zeros\")\n        # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX\n        # here?\n        elif \"getter\" not in kwargs:\n            # When `getter` is specified, it's possibly fine for\n            # `initializer` to be None since it's up to the custom `getter`\n            # to raise error in case it indeed needs `initializer`.\n            raise ValueError(\n                f\"An initializer for variable {name} of type \"\n                f\"{dtype.base_dtype} is required for layer \"\n                f\"{self.name}. Received: {initializer}.\"\n            )\n\n    getter = kwargs.pop(\"getter\", base_layer_utils.make_variable)\n    if (\n        autocast\n        and self._dtype_policy.compute_dtype\n        != self._dtype_policy.variable_dtype\n        and dtype.is_floating\n    ):\n        old_getter = getter\n\n        # Wrap variable constructor to return an AutoCastVariable.\n        def getter(*args, **kwargs):\n            variable = old_getter(*args, **kwargs)\n            return autocast_variable.create_autocast_variable(variable)\n\n        # Also the caching_device does not work with the mixed precision\n        # API, disable it if it is specified.\n        # TODO(b/142020079): Re-enable it once the bug is fixed.\n        if caching_device is not None:\n            tf_logging.warning(\n                \"`caching_device` does not work with mixed precision API. \"\n                \"Ignoring user specified `caching_device`.\"\n            )\n            caching_device = None\n    if layout:\n        getter = functools.partial(getter, layout=layout)\n\n    variable = self._add_variable_with_custom_getter(\n        name=name,\n        shape=shape,\n        # TODO(allenl): a `make_variable` equivalent should be added as a\n        # `Trackable` method.\n        getter=getter,\n        # Manage errors in Layer rather than Trackable.\n        overwrite=True,\n        initializer=initializer,\n        dtype=dtype,\n        constraint=constraint,\n        trainable=trainable,\n        use_resource=use_resource,\n        collections=collections_arg,\n        synchronization=synchronization,\n        aggregation=aggregation,\n        caching_device=caching_device,\n    )\n    if regularizer is not None:\n        # TODO(fchollet): in the future, this should be handled at the\n        # level of variable creation, and weight regularization losses\n        # should be variable attributes.\n        name_in_scope = variable.name[: variable.name.find(\":\")]\n        self._handle_weight_regularization(\n            name_in_scope, variable, regularizer\n        )\n    if base_layer_utils.is_split_variable(variable):\n        for v in variable:\n            backend.track_variable(v)\n            if trainable:\n                self._trainable_weights.append(v)\n            else:\n                self._non_trainable_weights.append(v)\n    else:\n        backend.track_variable(variable)\n        if trainable:\n            self._trainable_weights.append(variable)\n        else:\n            self._non_trainable_weights.append(variable)\n    return variable\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.build","title":"<code>mono_dense_keras._components.mono_dense_layer.MonoDense.build(input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; None</code>","text":"<p>Build</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple</code> <p>input tensor</p> required <code>args</code> <code>List[Any]</code> <p>positional arguments passed to Dense.build()</p> <code>()</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>keyword arguments passed to Dense.build()</p> <code>{}</code> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def build(\n    self, input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]\n) -&gt; None:\n\"\"\"Build\n\n    Args:\n        input_shape: input tensor\n        args: positional arguments passed to Dense.build()\n        kwargs: keyword arguments passed to Dense.build()\n    \"\"\"\n    super(MonoDense, self).build(input_shape, *args, **kwargs)\n    self.monotonicity_indicator = get_monotonicity_indicator(\n        monotonicity_indicator=self.monotonicity_indicator,\n        input_shape=input_shape,\n        units=self.units,\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.call","title":"<code>mono_dense_keras._components.mono_dense_layer.MonoDense.call(inputs: TensorLike) -&gt; TensorLike</code>","text":"<p>Call</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>TensorLike</code> <p>input tensor of shape (batch_size, ..., x_length)</p> required <p>Returns:</p> Type Description <code>TensorLike</code> <p>N-D tensor with shape: <code>(batch_size, ..., units)</code>.</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def call(self, inputs: TensorLike) -&gt; TensorLike:\n\"\"\"Call\n\n    Args:\n        inputs: input tensor of shape (batch_size, ..., x_length)\n\n    Returns:\n        N-D tensor with shape: `(batch_size, ..., units)`.\n\n    \"\"\"\n    # calculate W'*x+y after we replace the kernal according to monotonicity vector\n    with replace_kernel_using_monotonicity_indicator(\n        self, monotonicity_indicator=self.monotonicity_indicator\n    ):\n        h = super(MonoDense, self).call(inputs)\n\n    y = apply_activations(\n        h,\n        units=self.units,\n        convex_activation=self.convex_activation,\n        concave_activation=self.concave_activation,\n        saturated_activation=self.saturated_activation,\n        is_convex=self.is_convex,\n        is_concave=self.is_concave,\n        activation_weights=self.activation_weights,\n    )\n\n    return y\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.compute_mask","title":"<code>keras.engine.base_layer.Layer.compute_mask(inputs, mask = None)</code>","text":"<p>Computes an output mask tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Tensor or list of tensors.</p> required <code>mask</code> <p>Tensor or list of tensors.</p> <code>None</code> <p>Returns:</p> Type Description <p>None or a tensor (or list of tensors, one per output tensor of the layer).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@generic_utils.default\ndef compute_mask(self, inputs, mask=None):\n\"\"\"Computes an output mask tensor.\n\n    Args:\n        inputs: Tensor or list of tensors.\n        mask: Tensor or list of tensors.\n\n    Returns:\n        None or a tensor (or list of tensors,\n            one per output tensor of the layer).\n    \"\"\"\n    if not self._supports_masking:\n        if any(m is not None for m in tf.nest.flatten(mask)):\n            raise TypeError(\n                \"Layer \" + self.name + \" does not support masking, \"\n                \"but was passed an input_mask: \" + str(mask)\n            )\n        # masking not explicitly supported: return None as mask.\n        return None\n    # if masking is explicitly supported, by default\n    # carry over the input mask\n    return mask\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.compute_output_signature","title":"<code>keras.engine.base_layer.Layer.compute_output_signature(input_signature)</code>","text":"<p>Compute the output tensor signature of the layer based on the inputs.</p> <p>Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use <code>compute_output_shape</code>, and will assume that the output dtype matches the input dtype.</p> <p>Parameters:</p> Name Type Description Default <code>input_signature</code> <p>Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer.</p> required <p>Returns:</p> Type Description <p>Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_signature contains a non-TensorSpec object.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.for_subclass_implementers\ndef compute_output_signature(self, input_signature):\n\"\"\"Compute the output tensor signature of the layer based on the inputs.\n\n    Unlike a TensorShape object, a TensorSpec object contains both shape\n    and dtype information for a tensor. This method allows layers to provide\n    output dtype information if it is different from the input dtype.\n    For any layer that doesn't implement this function,\n    the framework will fall back to use `compute_output_shape`, and will\n    assume that the output dtype matches the input dtype.\n\n    Args:\n      input_signature: Single TensorSpec or nested structure of TensorSpec\n        objects, describing a candidate input for the layer.\n\n    Returns:\n      Single TensorSpec or nested structure of TensorSpec objects,\n        describing how the layer would transform the provided input.\n\n    Raises:\n      TypeError: If input_signature contains a non-TensorSpec object.\n    \"\"\"\n\n    def check_type_return_shape(s):\n        if not isinstance(s, tf.TensorSpec):\n            raise TypeError(\n                \"Only TensorSpec signature types are supported. \"\n                f\"Received: {s}.\"\n            )\n        return s.shape\n\n    input_shape = tf.nest.map_structure(\n        check_type_return_shape, input_signature\n    )\n    output_shape = self.compute_output_shape(input_shape)\n    dtype = self._compute_dtype\n    if dtype is None:\n        input_dtypes = [s.dtype for s in tf.nest.flatten(input_signature)]\n        # Default behavior when self.dtype is None, is to use the first\n        # input's dtype.\n        dtype = input_dtypes[0]\n    return tf.nest.map_structure(\n        lambda s: tf.TensorSpec(dtype=dtype, shape=s), output_shape\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.count_params","title":"<code>keras.engine.base_layer.Layer.count_params()</code>","text":"<p>Count the total number of scalars composing the weights.</p> <p>Returns:</p> Type Description <p>An integer count.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the layer isn't yet built (in which case its weights aren't yet defined).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def count_params(self):\n\"\"\"Count the total number of scalars composing the weights.\n\n    Returns:\n        An integer count.\n\n    Raises:\n        ValueError: if the layer isn't yet built\n          (in which case its weights aren't yet defined).\n    \"\"\"\n    if not self.built:\n        if getattr(self, \"_is_graph_network\", False):\n            with tf_utils.maybe_init_scope(self):\n                self._maybe_build(self.inputs)\n        else:\n            raise ValueError(\n                \"You tried to call `count_params` \"\n                f\"on layer {self.name}\"\n                \", but the layer isn't built. \"\n                \"You can build it manually via: \"\n                f\"`{self.name}.build(batch_input_shape)`.\"\n            )\n    return layer_utils.count_params(self.weights)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.finalize_state","title":"<code>keras.engine.base_layer.Layer.finalize_state()</code>","text":"<p>Finalizes the layers state after updating layer weights.</p> <p>This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update.</p> <p>This function will be called after weights of a layer have been restored from a loaded model.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef finalize_state(self):\n\"\"\"Finalizes the layers state after updating layer weights.\n\n    This function can be subclassed in a layer and will be called after\n    updating a layer weights. It can be overridden to finalize any\n    additional layer state after a weight update.\n\n    This function will be called after weights of a layer have been restored\n    from a loaded model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_at","title":"<code>keras.engine.base_layer.Layer.get_input_at(node_index)</code>","text":"<p>Retrieves the input tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first input node of the layer.</p> required <p>Returns:</p> Type Description <p>A tensor (or list of tensors if the layer has multiple inputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_at(self, node_index):\n\"\"\"Retrieves the input tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first input node of the layer.\n\n    Returns:\n        A tensor (or list of tensors if the layer has multiple inputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"input_tensors\", \"input\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_mask_at","title":"<code>keras.engine.base_layer.Layer.get_input_mask_at(node_index)</code>","text":"<p>Retrieves the input mask tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A mask tensor</p> <p>(or list of tensors if the layer has multiple inputs).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_mask_at(self, node_index):\n\"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A mask tensor\n        (or list of tensors if the layer has multiple inputs).\n    \"\"\"\n    inputs = self.get_input_at(node_index)\n    if isinstance(inputs, list):\n        return [getattr(x, \"_keras_mask\", None) for x in inputs]\n    else:\n        return getattr(inputs, \"_keras_mask\", None)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_shape_at","title":"<code>keras.engine.base_layer.Layer.get_input_shape_at(node_index)</code>","text":"<p>Retrieves the input shape(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A shape tuple</p> <p>(or list of shape tuples if the layer has multiple inputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_shape_at(self, node_index):\n\"\"\"Retrieves the input shape(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A shape tuple\n        (or list of shape tuples if the layer has multiple inputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"input_shapes\", \"input shape\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_at","title":"<code>keras.engine.base_layer.Layer.get_output_at(node_index)</code>","text":"<p>Retrieves the output tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first output node of the layer.</p> required <p>Returns:</p> Type Description <p>A tensor (or list of tensors if the layer has multiple outputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_at(self, node_index):\n\"\"\"Retrieves the output tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first output node of the layer.\n\n    Returns:\n        A tensor (or list of tensors if the layer has multiple outputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"output_tensors\", \"output\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_mask_at","title":"<code>keras.engine.base_layer.Layer.get_output_mask_at(node_index)</code>","text":"<p>Retrieves the output mask tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A mask tensor</p> <p>(or list of tensors if the layer has multiple outputs).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_mask_at(self, node_index):\n\"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A mask tensor\n        (or list of tensors if the layer has multiple outputs).\n    \"\"\"\n    output = self.get_output_at(node_index)\n    if isinstance(output, list):\n        return [getattr(x, \"_keras_mask\", None) for x in output]\n    else:\n        return getattr(output, \"_keras_mask\", None)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_shape_at","title":"<code>keras.engine.base_layer.Layer.get_output_shape_at(node_index)</code>","text":"<p>Retrieves the output shape(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A shape tuple</p> <p>(or list of shape tuples if the layer has multiple outputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_shape_at(self, node_index):\n\"\"\"Retrieves the output shape(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A shape tuple\n        (or list of shape tuples if the layer has multiple outputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"output_shapes\", \"output shape\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_weights","title":"<code>keras.engine.base_layer.Layer.get_weights()</code>","text":"<p>Returns the current weights of the layer, as NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers.</p> <p>For example, a <code>Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code>Dense</code> layer:</p> <p>layer_a = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.],        [2.],        [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)]</p> <p>Returns:</p> Type Description <p>Weights values as a list of NumPy arrays.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def get_weights(self):\n\"\"\"Returns the current weights of the layer, as NumPy arrays.\n\n    The weights of a layer represent the state of the layer. This function\n    returns both trainable and non-trainable weight values associated with\n    this layer as a list of NumPy arrays, which can in turn be used to load\n    state into similarly parameterized layers.\n\n    For example, a `Dense` layer returns a list of two values: the kernel\n    matrix and the bias vector. These can be used to set the weights of\n    another `Dense` layer:\n\n    &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(1.))\n    &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n    &gt;&gt;&gt; layer_a.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(2.))\n    &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[2.],\n           [2.],\n           [2.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n\n    Returns:\n        Weights values as a list of NumPy arrays.\n    \"\"\"\n    weights = self.weights\n    output_weights = []\n    for weight in weights:\n        if isinstance(weight, base_layer_utils.TrackableWeightHandler):\n            output_weights.extend(weight.get_tensors())\n        else:\n            output_weights.append(weight)\n    return backend.batch_get_value(output_weights)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.set_weights","title":"<code>keras.engine.base_layer.Layer.set_weights(weights)</code>","text":"<p>Sets the weights of the layer, from NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer.</p> <p>For example, a <code>Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code>Dense</code> layer:</p> <p>layer_a = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.],        [2.],        [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)]</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <p>a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of <code>get_weights</code>).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided weights list does not match the layer's specifications.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def set_weights(self, weights):\n\"\"\"Sets the weights of the layer, from NumPy arrays.\n\n    The weights of a layer represent the state of the layer. This function\n    sets the weight values from numpy arrays. The weight values should be\n    passed in the order they are created by the layer. Note that the layer's\n    weights must be instantiated before calling this function, by calling\n    the layer.\n\n    For example, a `Dense` layer returns a list of two values: the kernel\n    matrix and the bias vector. These can be used to set the weights of\n    another `Dense` layer:\n\n    &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(1.))\n    &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n    &gt;&gt;&gt; layer_a.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(2.))\n    &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[2.],\n           [2.],\n           [2.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n\n    Args:\n      weights: a list of NumPy arrays. The number\n        of arrays and their shape must match\n        number of the dimensions of the weights\n        of the layer (i.e. it should match the\n        output of `get_weights`).\n\n    Raises:\n      ValueError: If the provided weights list does not match the\n        layer's specifications.\n    \"\"\"\n    params = self.weights\n\n    expected_num_weights = 0\n    for param in params:\n        if isinstance(param, base_layer_utils.TrackableWeightHandler):\n            expected_num_weights += param.num_tensors\n        else:\n            expected_num_weights += 1\n\n    if expected_num_weights != len(weights):\n        raise ValueError(\n            'You called `set_weights(weights)` on layer \"%s\" '\n            \"with a weight list of length %s, but the layer was \"\n            \"expecting %s weights. Provided weights: %s...\"\n            % (\n                self.name,\n                len(weights),\n                expected_num_weights,\n                str(weights)[:50],\n            )\n        )\n\n    weight_index = 0\n    weight_value_tuples = []\n    for param in params:\n        if isinstance(param, base_layer_utils.TrackableWeightHandler):\n            num_tensors = param.num_tensors\n            tensors = weights[weight_index : weight_index + num_tensors]\n            param.set_weights(tensors)\n            weight_index += num_tensors\n        else:\n            weight = weights[weight_index]\n            weight_shape = weight.shape if hasattr(weight, \"shape\") else ()\n            ref_shape = param.shape\n            if not ref_shape.is_compatible_with(weight_shape):\n                raise ValueError(\n                    f\"Layer {self.name} weight shape {ref_shape} \"\n                    \"is not compatible with provided weight \"\n                    f\"shape {weight_shape}.\"\n                )\n            weight_value_tuples.append((param, weight))\n            weight_index += 1\n\n    backend.batch_set_value(weight_value_tuples)\n\n    # Perform any layer defined finalization of the layer state.\n    for layer in self._flatten_layers():\n        layer.finalize_state()\n</code></pre>"}]}