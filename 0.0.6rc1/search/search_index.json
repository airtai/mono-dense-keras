{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Monotonic Dense Layer","text":"<p>This Python library implements Monotonic Dense Layer as described in Davor Runje, Sharath M. Shankaranarayana, \u201cConstrained Monotonic Neural Networks\u201d, https://https://arxiv.org/abs/2205.11775.</p> <p>If you use this library, please cite:</p> <pre><code>@misc{https://doi.org/10.48550/arxiv.2205.11775,\n  doi = {10.48550/ARXIV.2205.11775},\n  url = {https://arxiv.org/abs/2205.11775},\n  author = {Davor Runje and Sharath M. Shankaranarayana},\n  title = {Constrained Monotonic Neural Networks},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}\n</code></pre>"},{"location":"#install","title":"Install","text":"<pre><code>pip install mono-dense-keras\n</code></pre>"},{"location":"#how-to-use","title":"How to use","text":"<p>First, we\u2019ll create a simple dataset for testing using numpy. Inputs values \\(x_1\\), \\(x_2\\) and \\(x_3\\) will be sampled from the normal distribution, while the output value \\(y\\) will be calculated according to the following formula before adding noise to it:</p> <p>\\(y = x_1^3 + \\sin\\left(\\frac{x_2}{2 \\pi}\\right) + e^{-x_3}\\)</p> <pre><code>import numpy as np\n\nrng = np.random.default_rng(42)\n\ndef generate_data(no_samples: int, noise: float):\n    x = rng.normal(size=(no_samples, 3))\n    y = x[:, 0] ** 3\n    y += np.sin(x[:, 1] / (2*np.pi))\n    y += np.exp(-x[:, 2])\n    y += noise * rng.normal(size=no_samples)\n    return x, y\n\nx_train, y_train = generate_data(10_000, noise=0.1)\nx_val, y_val = generate_data(10_000, noise=0.)\n</code></pre> <p>Now, we\u2019ll use the <code>MonoDense</code> layer instead of <code>Dense</code> layer. By default, the <code>MonoDense</code> layer assumes the output of the layer is monotonically increasing with all inputs. This assumtion is always true for all layers except possibly the first one. For the first layer, we use <code>monotonicity_indicator</code> to specify which input parameters are monotonic and to specify are they increasingly or decreasingly monotonic: - set 1 for increasingly monotonic parameter,</p> <ul> <li> <p>set -1 for decreasingly monotonic parameter, and</p> </li> <li> <p>set 0 otherwise.</p> </li> </ul> <p>In our case, the <code>monotonicity_indicator</code> is <code>[1, 0, -1]</code> because \\(y\\) is: - monotonically increasing w.r.t. \\(x_1\\) \\(\\left(\\frac{\\partial y}{x_1} = 3 {x_1}^2 \\geq 0\\right)\\), and</p> <ul> <li>monotonically decreasing w.r.t. \\(x_3\\) \\(\\left(\\frac{\\partial y}{x_3} = - e^{-x_2} \\leq 0\\right)\\).</li> </ul> <pre><code>from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, Dense\nfrom mono_dense_keras import MonoDense\n\n# build a simple model with 3 hidden layer, but this using MonotonicDense layer\nmodel = Sequential()\n\nmodel.add(Input(shape=(3,)))\nmonotonicity_indicator = [1, 0, -1]\nmodel.add(MonoDense(128, activation=\"elu\", monotonicity_indicator=monotonicity_indicator))\nmodel.add(MonoDense(128, activation=\"elu\"))\nmodel.add(MonoDense(1))\n\nmodel.summary()\n</code></pre> <pre><code>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n mono_dense_2 (MonoDense)    (None, 128)               512\n\n mono_dense_3 (MonoDense)    (None, 128)               16512\n\n mono_dense_4 (MonoDense)    (None, 1)                 129\n\n=================================================================\nTotal params: 17,153\nTrainable params: 17,153\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <pre><code>from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ndef train_model(model, initial_learning_rate):\n    # train the model\n    lr_schedule = ExponentialDecay(\n        initial_learning_rate=initial_learning_rate,\n        decay_steps=10_000 // 32,\n        decay_rate=0.9,\n    )\n    optimizer = Adam(learning_rate=lr_schedule)\n    model.compile(optimizer=\"adam\", loss=\"mse\")\n\n    model.fit(x=x_train, y=y_train, batch_size=32, validation_data=(x_val, y_val), epochs=10)\n\ntrain_model(model, initial_learning_rate=1.)\n</code></pre> <pre><code>Epoch 1/10\n313/313 [==============================] - 2s 5ms/step - loss: 0.2590 - val_loss: 0.4990\nEpoch 2/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2875 - val_loss: 0.1390\nEpoch 3/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2241 - val_loss: 0.0790\nEpoch 4/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2297 - val_loss: 0.1043\nEpoch 5/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2502 - val_loss: 0.1089\nEpoch 6/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2231 - val_loss: 0.0590\nEpoch 7/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1715 - val_loss: 0.5466\nEpoch 8/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1890 - val_loss: 0.0863\nEpoch 9/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1655 - val_loss: 0.1200\nEpoch 10/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2332 - val_loss: 0.1196\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The full text of the license is available at:</p> <p>https://github.com/airtai/mono-dense-keras/blob/main/LICENSE</p> <p>You are free to: - Share \u2014 copy and redistribute the material in any medium or format</p> <ul> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p> <p>Under the following terms: - Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> <ul> <li> <p>NonCommercial \u2014 You may not use the material for commercial purposes.</p> </li> <li> <p>ShareAlike \u2014 If you remix, transform, or build upon the material, you   must distribute your contributions under the same license as the   original.</p> </li> <li> <p>No additional restrictions \u2014 You may not apply legal terms or   technological measures that legally restrict others from doing   anything the license permits.</p> </li> </ul>"},{"location":"Experiments/","title":"Experiments","text":""},{"location":"Experiments/#imports","title":"Imports","text":"<pre><code>from keras_tuner import RandomSearch\n</code></pre> <pre><code>environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n</code></pre>"},{"location":"Experiments/#monotonic-dense-layer","title":"Monotonic Dense Layer","text":""},{"location":"Experiments/#monotonic-dense-layer_1","title":"Monotonic Dense Layer","text":"<p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p> <pre><code>units = 18\nactivation = \"relu\"\nbatch_size = 9\nx_len = 11\n\ntf.keras.utils.set_random_seed(42)\n\n\ndef display_kernel(kernel: Union[tf.Variable, np.typing.NDArray[float]]) -&gt; None:\n    cm = sns.color_palette(\"coolwarm_r\", as_cmap=True)\n\n    df = pd.DataFrame(kernel)\n\n    display(\n        df.style.format(\"{:.2f}\").background_gradient(cmap=cm, vmin=-1e-8, vmax=1e-8)\n    )\n\n\nx = np.random.default_rng(42).normal(size=(batch_size, x_len))\n\nfor monotonicity_indicator in [\n    [1] * 4 + [0] * 4 + [-1] * 3,\n    1,\n    np.ones((x_len,)),\n    -1,\n    -np.ones((x_len,)),\n]:\n    print(\"*\" * 120)\n    mono_layer = MonoDense(\n        units=units,\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator,\n        activation_weights=(7, 7, 4),\n    )\n    print(\"input:\")\n    display_kernel(x)\n\n    y = mono_layer(x)\n    print(f\"monotonicity_indicator = {monotonicity_indicator}\")\n    display_kernel(mono_layer.monotonicity_indicator)\n\n    print(\"kernel:\")\n    with replace_kernel_using_monotonicity_indicator(\n        mono_layer, mono_layer.monotonicity_indicator\n    ):\n        display_kernel(mono_layer.kernel)\n\n    print(\"output:\")\n    display_kernel(y)\nprint(\"ok\")\n</code></pre> <pre><code>************************************************************************************************************************\ninput:\nmonotonicity_indicator = [1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = 1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = -1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\nkernel:\noutput:\nok\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 0.00 5 0.00 6 0.00 7 0.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.33 0.15 0.13 0.41 0.38 0.14 0.43 0.30 0.02 0.12 0.38 0.05 0.42 0.03 0.00 0.24 0.44 0.28 1 0.01 0.39 0.42 0.32 0.38 0.22 0.33 0.34 0.03 0.06 0.06 0.27 0.26 0.45 0.35 0.05 0.21 0.34 2 0.21 0.29 0.16 0.14 0.42 0.06 0.15 0.10 0.41 0.08 0.03 0.22 0.34 0.20 0.11 0.01 0.43 0.35 3 0.27 0.33 0.06 0.17 0.42 0.42 0.24 0.30 0.11 0.20 0.17 0.25 0.17 0.07 0.32 0.30 0.17 0.36 4 0.32 -0.25 0.12 -0.37 0.41 0.20 0.06 -0.28 -0.27 0.43 -0.41 -0.17 -0.24 -0.31 0.33 0.31 0.11 0.03 5 0.04 0.19 -0.02 -0.34 0.36 -0.12 0.28 0.32 -0.11 -0.40 0.41 0.30 0.06 -0.28 -0.27 0.23 -0.41 -0.12 6 0.35 -0.04 -0.28 0.16 -0.03 0.35 -0.03 -0.16 0.39 -0.36 -0.31 -0.18 0.02 -0.38 -0.40 0.39 0.35 -0.19 7 0.33 -0.34 0.11 -0.29 0.25 -0.21 0.11 0.08 -0.19 -0.39 0.01 0.10 0.39 -0.25 -0.37 -0.27 0.04 0.34 8 -0.27 -0.09 -0.02 -0.45 -0.16 -0.12 -0.09 -0.43 -0.36 -0.09 -0.23 -0.42 -0.28 -0.24 -0.30 -0.31 -0.07 -0.07 9 -0.38 -0.34 -0.44 -0.42 -0.32 -0.06 -0.27 -0.28 -0.22 -0.05 -0.08 -0.07 -0.21 -0.39 -0.01 -0.26 -0.24 -0.42 10 -0.09 -0.45 -0.41 -0.36 -0.19 -0.09 -0.00 -0.34 -0.17 -0.18 -0.05 -0.39 -0.06 -0.20 -0.40 -0.33 -0.18 -0.01 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.01 0.40 0.00 1.38 0.00 0.10 0.00 -0.00 -0.00 -0.13 -0.00 -0.26 -0.00 -0.00 -0.55 -0.52 0.79 0.64 1 0.45 1.02 0.96 0.71 1.22 0.00 0.86 -0.00 -0.00 -0.09 -0.00 -0.00 -0.00 -0.00 0.26 -0.17 0.54 1.00 2 0.30 0.00 0.33 0.00 0.41 0.00 0.42 -0.53 -0.89 -0.29 -0.23 -0.84 -0.16 -0.93 -0.90 0.08 0.37 0.08 3 0.21 0.26 0.33 0.42 0.00 0.00 0.00 -0.16 -0.00 -0.61 -0.53 -0.07 -0.00 -0.00 -0.55 -0.66 0.83 0.78 4 1.38 0.49 0.70 0.82 1.47 0.54 0.63 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.73 0.97 0.94 0.91 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.86 -0.25 -0.00 -1.57 -1.19 -0.61 -0.23 0.13 -1.00 0.50 -0.06 6 0.00 0.00 0.00 0.17 0.00 0.00 0.00 -0.15 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.06 -1.00 0.00 0.12 7 0.00 0.96 0.35 0.93 0.00 0.32 0.17 -0.00 -0.00 -0.00 -0.00 -0.00 -0.17 -0.00 0.67 0.06 0.12 0.17 8 0.00 1.33 0.92 1.63 0.52 0.00 0.66 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.23 0.18 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.44 0.02 0.24 0.22 0.29 0.35 0.18 0.03 0.39 0.17 0.25 0.02 0.10 0.13 0.00 0.42 0.21 0.31 1 0.35 0.06 0.26 0.42 0.05 0.41 0.16 0.33 0.03 0.26 0.11 0.03 0.23 0.04 0.37 0.27 0.32 0.40 2 0.37 0.30 0.36 0.14 0.21 0.40 0.01 0.28 0.16 0.44 0.43 0.23 0.27 0.22 0.23 0.25 0.43 0.05 3 0.32 0.25 0.05 0.45 0.08 0.18 0.26 0.24 0.34 0.07 0.07 0.14 0.04 0.19 0.29 0.23 0.43 0.09 4 0.36 0.05 0.20 0.41 0.38 0.29 0.01 0.44 0.17 0.04 0.31 0.34 0.29 0.16 0.25 0.18 0.01 0.28 5 0.34 0.31 0.38 0.34 0.08 0.40 0.15 0.16 0.14 0.25 0.15 0.20 0.10 0.06 0.44 0.19 0.42 0.21 6 0.01 0.38 0.43 0.18 0.00 0.43 0.45 0.28 0.25 0.18 0.03 0.26 0.22 0.26 0.08 0.23 0.45 0.42 7 0.04 0.12 0.28 0.17 0.11 0.00 0.15 0.24 0.05 0.05 0.27 0.32 0.33 0.11 0.09 0.40 0.19 0.06 8 0.30 0.17 0.21 0.42 0.21 0.29 0.19 0.38 0.03 0.34 0.32 0.30 0.34 0.15 0.28 0.11 0.44 0.19 9 0.10 0.10 0.35 0.32 0.24 0.28 0.30 0.28 0.10 0.12 0.30 0.41 0.15 0.00 0.10 0.40 0.18 0.24 10 0.00 0.22 0.21 0.09 0.10 0.13 0.18 0.37 0.24 0.29 0.25 0.23 0.32 0.14 0.27 0.34 0.25 0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.01 0.00 0.00 0.00 0.00 0.00 -0.93 -0.00 -0.07 -0.58 -0.88 -0.58 -0.00 -0.87 -0.49 -0.05 -1.00 1 0.73 0.10 0.22 0.18 0.18 0.16 0.00 -0.23 -0.00 -0.00 -0.00 -0.09 -0.00 -0.00 0.16 0.47 0.53 -0.27 2 1.15 0.36 0.82 1.20 0.80 1.06 0.61 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.53 0.61 1.00 0.94 3 0.00 0.45 0.28 0.00 0.00 0.11 0.14 -0.00 -0.21 -0.00 -0.00 -0.00 -0.00 -0.00 0.15 0.08 0.72 -0.08 4 0.34 0.19 0.36 0.05 0.15 0.30 0.00 -0.00 -0.00 -0.08 -0.00 -0.00 -0.00 -0.00 0.06 0.38 0.04 0.14 5 0.00 0.00 0.26 0.00 0.67 0.05 0.00 -0.00 -0.16 -0.00 -0.00 -0.00 -0.00 -0.00 -0.08 0.30 -0.17 -0.17 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.76 -0.68 -0.28 -0.11 -0.37 -0.42 -0.40 -0.88 -0.41 -0.67 -1.00 7 0.01 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.17 -0.04 -0.57 -0.82 -0.50 -0.22 -0.07 -0.62 -0.13 -0.18 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.32 -0.35 -0.39 -0.77 -1.63 -1.12 -0.60 -0.47 -0.99 -1.00 -1.00 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 1.00 5 1.00 6 1.00 7 1.00 8 1.00 9 1.00 10 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.31 0.02 0.11 0.29 0.10 0.33 0.37 0.06 0.39 0.35 0.15 0.13 0.15 0.45 0.07 0.19 0.03 0.06 1 0.12 0.02 0.06 0.41 0.32 0.24 0.34 0.28 0.22 0.06 0.33 0.27 0.25 0.23 0.43 0.09 0.45 0.27 2 0.19 0.11 0.19 0.25 0.07 0.42 0.32 0.35 0.15 0.05 0.00 0.24 0.22 0.39 0.44 0.11 0.19 0.10 3 0.15 0.37 0.21 0.41 0.25 0.04 0.37 0.04 0.05 0.22 0.31 0.35 0.35 0.08 0.38 0.01 0.25 0.29 4 0.17 0.45 0.24 0.32 0.01 0.00 0.19 0.34 0.17 0.19 0.18 0.34 0.02 0.24 0.03 0.41 0.26 0.00 5 0.29 0.10 0.07 0.34 0.04 0.30 0.39 0.27 0.39 0.16 0.33 0.45 0.06 0.19 0.23 0.04 0.36 0.04 6 0.13 0.15 0.22 0.40 0.14 0.30 0.11 0.45 0.14 0.17 0.26 0.16 0.36 0.10 0.17 0.32 0.14 0.08 7 0.25 0.25 0.24 0.45 0.17 0.45 0.30 0.35 0.41 0.40 0.11 0.26 0.32 0.08 0.22 0.34 0.05 0.09 8 0.16 0.27 0.10 0.23 0.08 0.21 0.19 0.16 0.06 0.04 0.17 0.05 0.39 0.11 0.26 0.25 0.13 0.05 9 0.17 0.17 0.00 0.13 0.12 0.03 0.39 0.11 0.01 0.29 0.43 0.20 0.21 0.43 0.39 0.18 0.19 0.27 10 0.26 0.23 0.43 0.04 0.25 0.36 0.21 0.36 0.37 0.36 0.08 0.14 0.25 0.24 0.30 0.33 0.04 0.07 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.00 0.08 0.00 0.00 0.00 0.00 -0.82 -0.58 -0.32 -1.07 -1.09 -0.00 -0.63 -0.21 -0.74 -1.00 -0.15 1 0.36 0.00 0.00 0.51 0.11 0.72 0.76 -0.12 -0.00 -0.00 -0.05 -0.00 -0.00 -0.00 0.56 -0.34 0.13 0.22 2 0.72 0.68 0.32 1.10 0.10 0.84 0.68 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.20 0.97 0.33 -0.07 3 0.00 0.00 0.36 0.35 0.36 0.82 0.00 -0.00 -0.00 -0.19 -0.29 -0.13 -0.00 -0.20 0.67 0.20 -0.00 0.14 4 0.18 0.14 0.26 0.68 0.09 0.38 0.36 -0.00 -0.00 -0.00 -0.00 -0.00 -0.07 -0.00 0.14 0.15 0.33 0.10 5 0.01 0.55 0.50 0.00 0.00 0.21 0.00 -0.00 -0.27 -0.00 -0.44 -0.25 -0.00 -0.00 0.44 0.83 -0.24 -0.01 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.89 -0.85 -0.48 -0.77 -0.90 -0.21 -0.30 -0.09 -0.69 -0.83 -0.03 7 0.00 0.00 0.00 0.00 0.01 0.00 0.00 -0.79 -0.59 -0.65 -0.21 -0.55 -0.19 -0.37 -0.17 -0.71 -0.10 0.03 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.24 -0.48 -0.95 -1.13 -0.71 -1.40 -0.30 -0.76 -1.00 -0.47 -0.39 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.29 -0.12 -0.00 -0.17 -0.33 -0.17 -0.33 -0.36 -0.28 -0.16 -0.24 -0.22 -0.10 -0.13 -0.02 -0.38 -0.23 -0.02 1 -0.36 -0.13 -0.05 -0.07 -0.41 -0.30 -0.38 -0.06 -0.40 -0.42 -0.44 -0.03 -0.27 -0.03 -0.32 -0.31 -0.35 -0.40 2 -0.30 -0.07 -0.40 -0.06 -0.10 -0.21 -0.16 -0.22 -0.06 -0.36 -0.40 -0.42 -0.23 -0.22 -0.20 -0.33 -0.45 -0.06 3 -0.05 -0.08 -0.07 -0.30 -0.44 -0.23 -0.40 -0.25 -0.13 -0.31 -0.11 -0.13 -0.13 -0.34 -0.15 -0.05 -0.36 -0.13 4 -0.45 -0.34 -0.41 -0.39 -0.15 -0.10 -0.40 -0.32 -0.19 -0.13 -0.29 -0.39 -0.43 -0.29 -0.13 -0.05 -0.39 -0.01 5 -0.09 -0.38 -0.00 -0.12 -0.07 -0.42 -0.01 -0.12 -0.26 -0.28 -0.16 -0.06 -0.08 -0.43 -0.23 -0.28 -0.28 -0.07 6 -0.34 -0.38 -0.15 -0.44 -0.41 -0.19 -0.25 -0.41 -0.34 -0.22 -0.43 -0.36 -0.25 -0.28 -0.06 -0.12 -0.15 -0.16 7 -0.17 -0.39 -0.40 -0.26 -0.40 -0.20 -0.10 -0.14 -0.42 -0.21 -0.18 -0.25 -0.15 -0.21 -0.13 -0.41 -0.14 -0.14 8 -0.38 -0.03 -0.10 -0.21 -0.13 -0.04 -0.19 -0.00 -0.09 -0.38 -0.01 -0.27 -0.24 -0.24 -0.13 -0.18 -0.37 -0.21 9 -0.43 -0.08 -0.20 -0.29 -0.10 -0.27 -0.08 -0.43 -0.22 -0.37 -0.27 -0.24 -0.15 -0.22 -0.01 -0.45 -0.35 -0.31 10 -0.38 -0.44 -0.20 -0.31 -0.42 -0.23 -0.03 -0.31 -0.11 -0.35 -0.01 -0.00 -0.00 -0.39 -0.45 -0.14 -0.03 -0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 1.05 0.88 0.59 0.61 0.00 0.70 0.64 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.24 0.74 1.00 0.55 1 0.27 0.26 0.00 0.41 0.00 0.00 0.00 -0.00 -0.23 -0.33 -0.21 -0.20 -0.00 -0.02 -0.04 -0.82 -0.52 -0.02 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.36 -0.77 -0.71 -0.39 -1.00 -0.82 -0.67 -0.11 -0.74 -0.97 -0.31 3 0.00 0.00 0.00 0.00 0.00 0.01 0.00 -0.00 -0.15 -0.50 -0.38 -0.33 -0.20 -0.00 -0.39 -0.20 -0.12 -0.36 4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.46 -0.00 -0.84 -0.48 -0.36 -0.13 -0.08 -0.28 -0.33 0.13 5 0.00 0.02 0.00 0.00 0.12 0.33 0.00 -0.41 -0.00 -0.44 -0.33 -0.90 -0.56 -0.04 -0.24 -0.27 -0.48 -0.16 6 0.74 1.20 0.11 0.90 0.84 0.65 0.87 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.60 0.01 0.53 0.12 7 0.47 0.89 0.91 0.62 0.26 0.37 0.01 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.07 0.61 0.29 0.01 8 1.30 1.17 0.98 1.61 1.09 0.59 0.65 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.09 0.93 0.94 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 1 -1.00 2 -1.00 3 -1.00 4 -1.00 5 -1.00 6 -1.00 7 -1.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.45 -0.28 -0.30 -0.41 -0.17 -0.39 -0.22 -0.45 -0.28 -0.40 -0.18 -0.20 -0.16 -0.18 -0.10 -0.13 -0.14 -0.35 1 -0.09 -0.27 -0.09 -0.14 -0.02 -0.36 -0.21 -0.05 -0.05 -0.01 -0.02 -0.45 -0.03 -0.09 -0.01 -0.05 -0.39 -0.05 2 -0.17 -0.15 -0.37 -0.35 -0.32 -0.03 -0.24 -0.31 -0.35 -0.41 -0.00 -0.37 -0.18 -0.26 -0.09 -0.44 -0.09 -0.17 3 -0.42 -0.17 -0.11 -0.31 -0.32 -0.11 -0.20 -0.10 -0.34 -0.15 -0.24 -0.22 -0.22 -0.08 -0.40 -0.02 -0.23 -0.38 4 -0.13 -0.17 -0.06 -0.13 -0.32 -0.42 -0.28 -0.44 -0.03 -0.26 -0.38 -0.45 -0.08 -0.06 -0.04 -0.33 -0.27 -0.38 5 -0.32 -0.38 -0.19 -0.19 -0.33 -0.01 -0.15 -0.08 -0.31 -0.27 -0.07 -0.11 -0.21 -0.22 -0.18 -0.27 -0.19 -0.15 6 -0.30 -0.16 -0.09 -0.25 -0.23 -0.44 -0.25 -0.16 -0.05 -0.13 -0.20 -0.09 -0.14 -0.18 -0.15 -0.22 -0.37 -0.38 7 -0.20 -0.14 -0.12 -0.10 -0.42 -0.42 -0.14 -0.04 -0.44 -0.11 -0.10 -0.17 -0.06 -0.29 -0.22 -0.24 -0.01 -0.45 8 -0.31 -0.11 -0.16 -0.21 -0.16 -0.39 -0.12 -0.36 -0.36 -0.29 -0.24 -0.24 -0.20 -0.18 -0.33 -0.39 -0.20 -0.02 9 -0.41 -0.14 -0.12 -0.21 -0.01 -0.37 -0.03 -0.22 -0.38 -0.22 -0.09 -0.22 -0.19 -0.17 -0.13 -0.32 -0.30 -0.21 10 -0.31 -0.05 -0.02 -0.36 -0.04 -0.15 -0.03 -0.12 -0.36 -0.21 -0.40 -0.03 -0.04 -0.03 -0.23 -0.01 -0.02 -0.41 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.20 0.84 0.11 0.00 0.55 1.24 0.55 -0.00 -0.02 -0.00 -0.00 -0.00 -0.00 -0.00 -0.20 0.98 1.00 0.30 1 0.00 0.00 0.00 0.00 0.00 0.19 0.00 -0.14 -0.87 -0.50 -0.00 -0.34 -0.28 -0.53 -0.24 -0.34 0.23 -0.09 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.34 -0.82 -1.02 -0.75 -0.74 -0.56 -0.68 -0.71 -1.00 -0.65 -0.56 3 0.23 0.18 0.00 0.00 0.00 0.00 0.00 -0.00 -0.27 -0.00 -0.00 -0.21 -0.00 -0.28 -0.21 -0.24 0.02 0.00 4 0.09 0.00 0.00 0.00 0.00 0.00 0.00 -0.08 -0.00 -0.14 -0.00 -0.50 -0.01 -0.25 0.23 -0.20 -0.14 -0.66 5 0.18 0.49 0.00 0.00 0.03 0.00 0.00 -0.79 -0.36 -0.49 -0.39 -0.69 -0.00 -0.09 0.08 -0.84 0.10 -0.25 6 0.64 0.76 0.08 0.50 0.62 0.79 0.68 -0.00 -0.06 -0.00 -0.00 -0.00 -0.00 -0.00 0.28 0.24 0.86 0.87 7 0.32 0.24 0.23 0.18 0.76 0.62 0.28 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.13 0.73 0.09 0.87 8 1.23 0.50 0.27 0.51 1.08 2.00 0.60 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 1.00 1.00 1.00 <pre><code>x = Input(shape=(5, 7, 8))\n\nlayer = MonoDense(\n    units=12,\n    activation=activation,\n    monotonicity_indicator=[1] * 3 + [-1] * 3 + [0] * 2,\n    is_convex=False,\n    is_concave=False,\n)\n\ny = layer(x)\n\nmodel = Model(inputs=x, outputs=y)\n\nmodel.summary()\n\ndisplay_kernel(layer.monotonicity_indicator)\n</code></pre> <pre><code>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 5, 7, 8)]         0\n\n mono_dense_5 (MonoDense)    (None, 5, 7, 12)          108\n\n=================================================================\nTotal params: 108\nTrainable params: 108\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> 0 0 1.00 1 1.00 2 1.00 3 -1.00 4 -1.00 5 -1.00 6 0.00 7 0.00"},{"location":"Experiments/#experiments_1","title":"Experiments","text":"<p>For our experiments, we employ the datasets used by the authors of Certified Monotonic Network [1] and COMET [2]. We use the exact train-test split provided by the authors. Their respective repositories are linked below in the references. We directly load the saved train-test data split which have been saved after running the codes from respective papers\u2019 authors.</p> <p>References:</p> <ol> <li>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</li> </ol> <p>Github repo: https://github.com/gnobitab/CertifiedMonotonicNetwork</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>Github repo: https://github.com/AishwaryaSivaraman/COMET</p> <p>source</p>"},{"location":"Experiments/#download_data","title":"download_data","text":"<pre><code> download_data (dataset_name:str,\n                data_path:Union[pathlib.Path,str,NoneType]='data',\n                force_download:bool=False)\n</code></pre> <p>source</p>"},{"location":"Experiments/#get_data_path","title":"get_data_path","text":"<pre><code> get_data_path (data_path:Union[pathlib.Path,str,NoneType]=None)\n</code></pre> <pre><code>download_data(\"auto\", force_download=True)\n\n!ls -l data\n\nassert (Path(\"data\") / \"train_auto.csv\").exists()\n</code></pre> <pre><code>train_auto.csv: 49.2kB [00:00, 55.5kB/s]                            \ntest_auto.csv: 16.4kB [00:00, 16.7kB/s]\n\ntotal 257812\n-rw-rw-r-- 1 davor davor    11161 May 30 09:26 test_auto.csv\n-rw-rw-r-- 1 davor davor 11340054 May 25 04:48 test_blog.csv\n-rw-rw-r-- 1 davor davor   101210 May 25 04:48 test_compas.csv\n-rw-rw-r-- 1 davor davor    15798 May 25 04:48 test_heart.csv\n-rw-rw-r-- 1 davor davor 13339777 May 25 04:48 test_loan.csv\n-rw-rw-r-- 1 davor davor    44626 May 30 09:26 train_auto.csv\n-rw-rw-r-- 1 davor davor 79478767 May 25 04:48 train_blog.csv\n-rw-rw-r-- 1 davor davor   405660 May 25 04:48 train_compas.csv\n-rw-rw-r-- 1 davor davor    62282 May 25 04:48 train_heart.csv\n-rw-rw-r-- 1 davor davor 79588030 May 25 04:48 train_loan.csv\n-rw-rw-r-- 1 davor davor 79588030 May 29 13:57 {prefix}_{name}.csv\n</code></pre> <p>source</p>"},{"location":"Experiments/#sanitize_col_names","title":"sanitize_col_names","text":"<pre><code> sanitize_col_names (df:pandas.core.frame.DataFrame)\n</code></pre> <pre><code>sanitize_col_names(pd.DataFrame({\"a b\": [1, 2, 3]}))\n</code></pre>   |     | a_b | |-----|-----| | 0   | 1   | | 1   | 2   | | 2   | 3   |   <p>source</p>"},{"location":"Experiments/#get_train_n_test_data","title":"get_train_n_test_data","text":"<pre><code> get_train_n_test_data (dataset_name:str,\n                        data_path:Union[pathlib.Path,str,NoneType]='./data\n                        ')\n</code></pre> <pre><code>train_df, test_df = get_train_n_test_data(\"auto\")\ndisplay(train_df)\ndisplay(test_df)\n</code></pre>   |     | Cylinders | Displacement | Horsepower | Weight    | Acceleration | Model_Year | Origin    | ground_truth | |-----|-----------|--------------|------------|-----------|--------------|------------|-----------|--------------| | 0   | 1.482807  | 1.073028     | 0.650564   | 0.606625  | -1.275546    | -1.631803  | -0.701669 | 18.0         | | 1   | 1.482807  | 1.482902     | 1.548993   | 0.828131  | -1.452517    | -1.631803  | -0.701669 | 15.0         | | 2   | 1.482807  | 1.044432     | 1.163952   | 0.523413  | -1.275546    | -1.631803  | -0.701669 | 16.0         | | 3   | 1.482807  | 1.025368     | 0.907258   | 0.542165  | -1.806460    | -1.631803  | -0.701669 | 17.0         | | 4   | 1.482807  | 2.235927     | 2.396084   | 1.587581  | -1.983431    | -1.631803  | -0.701669 | 15.0         | | ... | ...       | ...          | ...        | ...       | ...          | ...        | ...       | ...          | | 309 | 0.310007  | 0.358131     | 0.188515   | -0.177437 | -0.319901    | 1.720778   | -0.701669 | 22.0         | | 310 | -0.862792 | -0.566468    | -0.530229  | -0.722413 | -0.921604    | 1.720778   | -0.701669 | 36.0         | | 311 | -0.862792 | -0.928683    | -1.351650  | -1.003691 | 3.184131     | 1.720778   | 0.557325  | 44.0         | | 312 | -0.862792 | -0.566468    | -0.530229  | -0.810312 | -1.417123    | 1.720778   | -0.701669 | 32.0         | | 313 | -0.862792 | -0.709448    | -0.658576  | -0.423555 | 1.060475     | 1.720778   | -0.701669 | 28.0         |  <p>314 rows \u00d7 8 columns</p>   |     | Cylinders | Displacement | Horsepower | Weight    | Acceleration | Model_Year | Origin    | ground_truth | |-----|-----------|--------------|------------|-----------|--------------|------------|-----------|--------------| | 0   | -0.862792 | -1.043066    | -1.017947  | -1.027131 | 1.272841     | 1.162014   | 1.816319  | 40.8         | | 1   | 1.482807  | 1.177880     | 1.163952   | 0.526929  | -1.629489    | -1.631803  | -0.701669 | 18.0         | | 2   | 1.482807  | 1.482902     | 1.934034   | 0.794143  | -1.629489    | -0.793657  | -0.701669 | 11.0         | | 3   | 0.310007  | 0.529707     | -0.119518  | 0.346443  | -0.213718    | -1.352421  | -0.701669 | 19.0         | | 4   | -0.862792 | -1.004939    | -0.863931  | -1.243949 | -0.567661    | 0.882633   | 0.557325  | 31.9         | | ... | ...       | ...          | ...        | ...       | ...          | ...        | ...       | ...          | | 73  | -0.862792 | -0.699916    | 0.188515   | -0.062582 | -0.390690    | -1.073039  | 0.557325  | 18.0         | | 74  | -0.862792 | -0.518809    | -0.838261  | -0.686081 | 1.379024     | -0.793657  | -0.701669 | 21.0         | | 75  | 0.310007  | -0.251914    | 0.701903   | -0.089538 | -1.487912    | 1.162014   | 1.816319  | 32.7         | | 76  | 1.482807  | 1.492434     | 1.138283   | 1.580549  | -0.390690    | 0.323869   | -0.701669 | 16.0         | | 77  | 0.310007  | -0.375829    | 0.060168   | -0.602870 | -0.567661    | -0.793657  | -0.701669 | 21.0         |  <p>78 rows \u00d7 8 columns</p> <p>source</p>"},{"location":"Experiments/#peek","title":"peek","text":"<pre><code> peek (ds:tensorflow.python.data.ops.dataset_ops.DatasetV2)\n</code></pre> <p>source</p>"},{"location":"Experiments/#df2ds","title":"df2ds","text":"<pre><code> df2ds (df:pandas.core.frame.DataFrame)\n</code></pre> <pre><code>x, y = peek(df2ds(train_df).batch(8))\ndisplay(x)\ndisplay(y)\n\nexpected = {\n    \"Acceleration\",\n    \"Cylinders\",\n    \"Displacement\",\n    \"Horsepower\",\n    \"Model_Year\",\n    \"Origin\",\n    \"Weight\",\n}\nassert set(x.keys()) == expected\nfor k in expected:\n    assert x[k].shape == (8,)\nassert y.shape == (8,)\n</code></pre> <pre><code>{'Cylinders': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([1.4828068, 1.4828068, 1.4828068, 1.4828068, 1.4828068, 1.4828068,\n        1.4828068, 1.4828068], dtype=float32)&gt;,\n 'Displacement': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([1.0730283, 1.4829025, 1.0444324, 1.0253685, 2.235927 , 2.474226 ,\n        2.3407786, 1.8641808], dtype=float32)&gt;,\n 'Horsepower': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([0.65056413, 1.5489933 , 1.1639522 , 0.9072582 , 2.3960838 ,\n        2.9608107 , 2.8324637 , 2.1907284 ], dtype=float32)&gt;,\n 'Weight': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([0.6066247, 0.828131 , 0.5234134, 0.5421652, 1.5875812, 1.602817 ,\n        1.5535934, 1.0121336], dtype=float32)&gt;,\n 'Acceleration': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([-1.2755462, -1.4525175, -1.2755462, -1.8064601, -1.9834315,\n        -2.3373742, -2.5143454, -2.5143454], dtype=float32)&gt;,\n 'Model_Year': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([-1.6318026, -1.6318026, -1.6318026, -1.6318026, -1.6318026,\n        -1.6318026, -1.6318026, -1.6318026], dtype=float32)&gt;,\n 'Origin': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([-0.7016686, -0.7016686, -0.7016686, -0.7016686, -0.7016686,\n        -0.7016686, -0.7016686, -0.7016686], dtype=float32)&gt;}\n\n&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([18., 15., 16., 17., 15., 14., 14., 15.], dtype=float32)&gt;\n</code></pre> <p>source</p>"},{"location":"Experiments/#build_mono_model_f","title":"build_mono_model_f","text":"<pre><code> build_mono_model_f (monotonicity_indicator:Dict[str,int], final_activatio\n                     n:Union[str,Callable[[Union[tensorflow.python.types.c\n                     ore.Tensor,tensorflow.python.types.core.TensorProtoco\n                     l,int,float,bool,str,bytes,complex,tuple,list,numpy.n\n                     darray,numpy.generic],Union[tensorflow.python.types.c\n                     ore.Tensor,tensorflow.python.types.core.TensorProtoco\n                     l,int,float,bool,str,bytes,complex,tuple,list,numpy.n\n                     darray,numpy.generic]],Union[tensorflow.python.types.\n                     core.Tensor,tensorflow.python.types.core.TensorProtoc\n                     ol,int,float,bool,str,bytes,complex,tuple,list,numpy.\n                     ndarray,numpy.generic]]], loss:Union[str,Callable[[Un\n                     ion[tensorflow.python.types.core.Tensor,tensorflow.py\n                     thon.types.core.TensorProtocol,int,float,bool,str,byt\n                     es,complex,tuple,list,numpy.ndarray,numpy.generic],Un\n                     ion[tensorflow.python.types.core.Tensor,tensorflow.py\n                     thon.types.core.TensorProtocol,int,float,bool,str,byt\n                     es,complex,tuple,list,numpy.ndarray,numpy.generic]],U\n                     nion[tensorflow.python.types.core.Tensor,tensorflow.p\n                     ython.types.core.TensorProtocol,int,float,bool,str,by\n                     tes,complex,tuple,list,numpy.ndarray,numpy.generic]]]\n                     , metrics:Union[str,Callable[[Union[tensorflow.python\n                     .types.core.Tensor,tensorflow.python.types.core.Tenso\n                     rProtocol,int,float,bool,str,bytes,complex,tuple,list\n                     ,numpy.ndarray,numpy.generic],Union[tensorflow.python\n                     .types.core.Tensor,tensorflow.python.types.core.Tenso\n                     rProtocol,int,float,bool,str,bytes,complex,tuple,list\n                     ,numpy.ndarray,numpy.generic]],Union[tensorflow.pytho\n                     n.types.core.Tensor,tensorflow.python.types.core.Tens\n                     orProtocol,int,float,bool,str,bytes,complex,tuple,lis\n                     t,numpy.ndarray,numpy.generic]]], train_ds:tensorflow\n                     .python.data.ops.dataset_ops.DatasetV2, batch_size,\n                     units:int, n_layers:int, activation:Union[str,Callabl\n                     e[[Union[tensorflow.python.types.core.Tensor,tensorfl\n                     ow.python.types.core.TensorProtocol,int,float,bool,st\n                     r,bytes,complex,tuple,list,numpy.ndarray,numpy.generi\n                     c],Union[tensorflow.python.types.core.Tensor,tensorfl\n                     ow.python.types.core.TensorProtocol,int,float,bool,st\n                     r,bytes,complex,tuple,list,numpy.ndarray,numpy.generi\n                     c]],Union[tensorflow.python.types.core.Tensor,tensorf\n                     low.python.types.core.TensorProtocol,int,float,bool,s\n                     tr,bytes,complex,tuple,list,numpy.ndarray,numpy.gener\n                     ic]]], learning_rate:float, weight_decay:float,\n                     dropout:float, decay_rate:float)\n</code></pre> <pre><code>train_df, test_df = get_train_n_test_data(\"auto\")\ntrain_ds = df2ds(train_df)\ntest_ds = df2ds(test_df)\n\nbuild_model_f = lambda: build_mono_model_f(\n    monotonicity_indicator = {\n        \"Cylinders\": 0,\n        \"Displacement\": -1,\n        \"Horsepower\": -1,\n        \"Weight\": -1,\n        \"Acceleration\": 0,\n        \"Model_Year\": 0,\n        \"Origin\": 0,\n    },\n    final_activation = None,\n    loss = \"mse\",\n    metrics = \"mse\",\n    train_ds=train_ds,\n    batch_size=8,\n    units = 16,\n    n_layers = 3,\n    activation = \"elu\",\n    learning_rate = 0.01,\n    weight_decay = 0.001,\n    dropout = 0.25,\n    decay_rate = 0.95,\n)\nmodel = build_model_f()\nmodel.summary()\nmodel.fit(train_ds.batch(8), validation_data=test_ds.batch(256), epochs=1)\n</code></pre> <pre><code>Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n Displacement (InputLayer)      [(None, 1)]          0           []\n\n Horsepower (InputLayer)        [(None, 1)]          0           []\n\n Weight (InputLayer)            [(None, 1)]          0           []\n\n mono_dense_Displacement_decrea  (None, 4)           8           ['Displacement[0][0]']           \n sing (MonoDense)\n\n mono_dense_Horsepower_decreasi  (None, 4)           8           ['Horsepower[0][0]']             \n ng (MonoDense)\n\n mono_dense_Weight_decreasing (  (None, 4)           8           ['Weight[0][0]']                 \n MonoDense)\n\n Acceleration (InputLayer)      [(None, 1)]          0           []\n\n Cylinders (InputLayer)         [(None, 1)]          0           []\n\n Model_Year (InputLayer)        [(None, 1)]          0           []\n\n Origin (InputLayer)            [(None, 1)]          0           []\n\n concat_mono (Concatenate)      (None, 12)           0           ['mono_dense_Displacement_decreas\n                                                                 ing[0][0]',                      \n                                                                  'mono_dense_Horsepower_decreasin\n                                                                 g[0][0]',                        \n                                                                  'mono_dense_Weight_decreasing[0]\n                                                                 [0]']\n\n concat_non_mono (Concatenate)  (None, 4)            0           ['Acceleration[0][0]',           \n                                                                  'Cylinders[0][0]',              \n                                                                  'Model_Year[0][0]',             \n                                                                  'Origin[0][0]']\n\n dropout (Dropout)              (None, 12)           0           ['concat_mono[0][0]']\n\n non_mono_dense_0 (Dense)       (None, 16)           80          ['concat_non_mono[0][0]']\n\n concat_preprocess (Concatenate  (None, 28)          0           ['dropout[0][0]',                \n )                                                                'non_mono_dense_0[0][0]']\n\n mono_dense_0 (MonoDense)       (None, 16)           464         ['concat_preprocess[0][0]']\n\n dropout_2 (Dropout)            (None, 16)           0           ['mono_dense_0[0][0]']\n\n mono_dense_1_increasing (MonoD  (None, 16)          272         ['dropout_2[0][0]']              \n ense)\n\n dropout_3 (Dropout)            (None, 16)           0           ['mono_dense_1_increasing[0][0]']\n\n mono_dense_2_increasing (MonoD  (None, 1)           17          ['dropout_3[0][0]']              \n ense)\n\n==================================================================================================\nTotal params: 857\nTrainable params: 857\nNon-trainable params: 0\n__________________________________________________________________________________________________\n40/40 [==============================] - 3s 11ms/step - loss: 191.5380 - mse: 191.5380 - val_loss: 396.7859 - val_mse: 396.7859\n\n&lt;keras.callbacks.History&gt;\n</code></pre> <p>source</p>"},{"location":"Experiments/#testhypermodel","title":"TestHyperModel","text":"<pre><code> TestHyperModel (**kwargs:Any)\n</code></pre> <p>Defines a search space of models.</p> <p>A search space is a collection of models. The <code>build</code> function will build one of the models from the space using the given <code>HyperParameters</code> object.</p> <p>Users should subclass the <code>HyperModel</code> class to define their search spaces by overriding <code>build()</code>, which creates and returns the Keras model. Optionally, you may also override <code>fit()</code> to customize the training process of the model.</p> <p>Examples:</p> <p>In <code>build()</code>, you can create the model using the hyperparameters.</p> <pre><code>class MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        model = keras.Sequential()\n        model.add(keras.layers.Dense(\n            hp.Choice('units', [8, 16, 32]),\n            activation='relu'))\n        model.add(keras.layers.Dense(1, activation='relu'))\n        model.compile(loss='mse')\n        return model\n</code></pre> <p>When overriding <code>HyperModel.fit()</code>, if you use <code>model.fit()</code> to train your model, which returns the training history, you can return it directly. You may use <code>hp</code> to specify any hyperparameters to tune.</p> <pre><code>class MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        ...\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            epochs=hp.Int(\"epochs\", 5, 20),\n            **kwargs)\n</code></pre> <p>If you have a customized training process, you can return the objective value as a float.</p> <p>If you want to keep track of more metrics, you can return a dictionary of the metrics to track.</p> <pre><code>class MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        ...\n\n    def fit(self, hp, model, *args, **kwargs):\n        ...\n        return {\n            \"loss\": loss,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_accuracy\n        }\n</code></pre> <p>Args: name: Optional string, the name of this HyperModel. tunable: Boolean, whether the hyperparameters defined in this hypermodel should be added to search space. If <code>False</code>, either the search space for these parameters must be defined in advance, or the default values will be used. Defaults to True.</p> <p>source</p>"},{"location":"Experiments/#get_build_model_with_hp_f","title":"get_build_model_with_hp_f","text":"<pre><code> get_build_model_with_hp_f\n                            (build_model_f:Callable[[],keras.engine.traini\n                            ng.Model], **kwargs:Any)\n</code></pre> <pre><code>with TemporaryDirectory() as d:\n    tuner = RandomSearch(\n        hypermodel=TestHyperModel(\n            monotonicity_indicator={\n                \"Cylinders\": 0,\n                \"Displacement\": -1,\n                \"Horsepower\": -1,\n                \"Weight\": -1,\n                \"Acceleration\": 0,\n                \"Model_Year\": 0,\n                \"Origin\": 0,\n            },\n            final_activation=None,\n            loss=\"mse\",\n            metrics=\"mse\",\n            train_ds=train_ds,\n            batch_size=8,\n        ),\n        directory=d,\n        project_name=\"testing\",\n        max_trials=2,\n        objective=\"val_loss\",\n    )\n    tuner.search(\n        train_ds.shuffle(len(train_ds)).batch(8).prefetch(2),\n        validation_data=test_ds.batch(256),\n        epochs=2,\n    )\n</code></pre> <pre><code>Trial 2 Complete [00h 00m 04s]\nval_loss: 14.47925090789795\n\nBest val_loss So Far: 14.47925090789795\nTotal elapsed time: 00h 00m 07s\nINFO:tensorflow:Oracle triggered exit\n</code></pre> <p>source</p>"},{"location":"Experiments/#find_hyperparameters","title":"find_hyperparameters","text":"<pre><code> find_hyperparameters (dataset_name:str,\n                       monotonicity_indicator:Dict[str,int], final_activat\n                       ion:Union[str,Callable[[Union[tensorflow.python.typ\n                       es.core.Tensor,tensorflow.python.types.core.TensorP\n                       rotocol,int,float,bool,str,bytes,complex,tuple,list\n                       ,numpy.ndarray,numpy.generic],Union[tensorflow.pyth\n                       on.types.core.Tensor,tensorflow.python.types.core.T\n                       ensorProtocol,int,float,bool,str,bytes,complex,tupl\n                       e,list,numpy.ndarray,numpy.generic]],Union[tensorfl\n                       ow.python.types.core.Tensor,tensorflow.python.types\n                       .core.TensorProtocol,int,float,bool,str,bytes,compl\n                       ex,tuple,list,numpy.ndarray,numpy.generic]]], loss:\n                       Union[str,Callable[[Union[tensorflow.python.types.c\n                       ore.Tensor,tensorflow.python.types.core.TensorProto\n                       col,int,float,bool,str,bytes,complex,tuple,list,num\n                       py.ndarray,numpy.generic],Union[tensorflow.python.t\n                       ypes.core.Tensor,tensorflow.python.types.core.Tenso\n                       rProtocol,int,float,bool,str,bytes,complex,tuple,li\n                       st,numpy.ndarray,numpy.generic]],Union[tensorflow.p\n                       ython.types.core.Tensor,tensorflow.python.types.cor\n                       e.TensorProtocol,int,float,bool,str,bytes,complex,t\n                       uple,list,numpy.ndarray,numpy.generic]]], metrics:U\n                       nion[str,Callable[[Union[tensorflow.python.types.co\n                       re.Tensor,tensorflow.python.types.core.TensorProtoc\n                       ol,int,float,bool,str,bytes,complex,tuple,list,nump\n                       y.ndarray,numpy.generic],Union[tensorflow.python.ty\n                       pes.core.Tensor,tensorflow.python.types.core.Tensor\n                       Protocol,int,float,bool,str,bytes,complex,tuple,lis\n                       t,numpy.ndarray,numpy.generic]],Union[tensorflow.py\n                       thon.types.core.Tensor,tensorflow.python.types.core\n                       .TensorProtocol,int,float,bool,str,bytes,complex,tu\n                       ple,list,numpy.ndarray,numpy.generic]]],\n                       max_trials:int=100, max_epochs:int=50,\n                       batch_size:int=8, objective:Union[str,keras_tuner.e\n                       ngine.objective.Objective],\n                       dir_root:Union[pathlib.Path,str]='tuner',\n                       seed:int=42, executions_per_trial:int=3,\n                       max_consecutive_failed_trials:int=5,\n                       patience:int=10)\n</code></pre> <pre><code>tuner = find_hyperparameters(\n    \"auto\",\n    monotonicity_indicator={\n        \"Cylinders\": 0,\n        \"Displacement\": -1,\n        \"Horsepower\": -1,\n        \"Weight\": -1,\n        \"Acceleration\": 0,\n        \"Model_Year\": 0,\n        \"Origin\": 0,\n    },\n    max_trials=2,\n    final_activation=None,\n    loss=\"mse\",\n    metrics=\"mse\",\n    objective=\"val_mse\",\n)\n</code></pre> <pre><code>Trial 2 Complete [00h 00m 16s]\nval_mse: 20.31526056925456\n\nBest val_mse So Far: 16.89842955271403\nTotal elapsed time: 00h 00m 56s\nINFO:tensorflow:Oracle triggered exit\n</code></pre> <p>source</p>"},{"location":"Experiments/#create_tuner_stats","title":"create_tuner_stats","text":"<pre><code> create_tuner_stats (tuner:keras_tuner.engine.tuner.Tuner,\n                     stats:Optional[pandas.core.frame.DataFrame]=None,\n                     num_models:int=10, top_models:int=5, epochs:int=50,\n                     batch_size:int=8, patience:int=10, verbose:int=0)\n</code></pre> <p>source</p>"},{"location":"Experiments/#create_model_stats","title":"create_model_stats","text":"<pre><code> create_model_stats (tuner:keras_tuner.engine.tuner.Tuner,\n                     hp:Dict[str,Any],\n                     stats:Optional[pandas.core.frame.DataFrame]=None,\n                     epochs:int, num_runs:int, top_runs:int,\n                     batch_size:int, patience:int, verbose:int, train_ds:t\n                     ensorflow.python.data.ops.dataset_ops.DatasetV2, test\n                     _ds:tensorflow.python.data.ops.dataset_ops.DatasetV2)\n</code></pre> <p>source</p>"},{"location":"Experiments/#count_model_params","title":"count_model_params","text":"<pre><code> count_model_params (model:keras.engine.training.Model)\n</code></pre> <pre><code>stats = create_tuner_stats(tuner, verbose=0)\n</code></pre>   |     | units | n_layers | activation | learning_rate | weight_decay | dropout  | decay_rate | val_mse_mean | val_mse_std | val_mse_min | val_mse_max | params | |-----|-------|----------|------------|---------------|--------------|----------|------------|--------------|-------------|-------------|-------------|--------| | 0   | 23    | 1        | elu        | 0.004715      | 0.265345     | 0.175923 | 0.816107   | 16.454607    | 0.659872    | 15.446076   | 17.069242   | 184    |     |     | units | n_layers | activation | learning_rate | weight_decay | dropout  | decay_rate | val_mse_mean | val_mse_std | val_mse_min | val_mse_max | params | |-----|-------|----------|------------|---------------|--------------|----------|------------|--------------|-------------|-------------|-------------|--------| | 0   | 23    | 1        | elu        | 0.004715      | 0.265345     | 0.175923 | 0.816107   | 16.454607    | 0.659872    | 15.446076   | 17.069242   | 184    | | 1   | 9     | 2        | elu        | 0.265157      | 0.196993     | 0.456821 | 0.560699   | 16.494637    | 1.219302    | 14.531968   | 17.621958   | 211    |   <pre><code>CPU times: user 3min 28s, sys: 28.3 s, total: 3min 56s\nWall time: 2min 50s\n</code></pre> <pre><code># !rm -rf tuner\n</code></pre> <pre><code>assert False\n</code></pre> <pre><code>AssertionError:\n</code></pre> <pre><code># !ls /tmp/tuner/auto_tuner/2023-02-28T13:02:31.787216\n\n\ndef load_latest_tuner(\n    build_model_f: Callable[..., Model],\n    tuner_name: str = \"BayesianOptimization\",\n    *,\n    max_trials: Optional[int] = None,\n    max_epochs: Optional[int] = None,\n    train_ds: tf.data.Dataset,\n    test_ds: tf.data.Dataset,\n    objective: Union[str, Objective],\n    dir_root: Union[Path, str],\n    project_name: str,\n    factor: int = 2,\n    seed: int = 42,\n    executions_per_trial: int = 1,\n    hyperband_iterations: int = 1,\n    max_consecutive_failed_trials: int = 5,\n) -&gt; Tuner:\n    directory = Path(dir_root)\n    print(f\"Loading tuner saved at: {directory}\")\n\n    if tuner_name == \"BayesianOptimization\":\n        tuner = BayesianOptimization(\n            build_model_f,\n            objective=objective,\n            max_trials=max_trials,\n            seed=seed,\n            directory=directory,\n            project_name=project_name,\n            executions_per_trial=executions_per_trial,\n            max_consecutive_failed_trials=max_consecutive_failed_trials,\n        )\n        kwargs = dict(epochs=max_epochs)\n    elif tuner_name == \"Hyperband\":\n        tuner = Hyperband(\n            build_model_f,\n            objective=objective,\n            max_epochs=max_epochs,\n            factor=factor,\n            seed=seed,\n            directory=directory,\n            project_name=project_name,\n            executions_per_trial=executions_per_trial,\n            hyperband_iterations=hyperband_iterations,\n            max_consecutive_failed_trials=max_consecutive_failed_trials,\n        )\n        kwargs = dict()\n    else:\n        raise ValueError(f\"tuner_name={tuner_name}\")\n\n    return tuner\n</code></pre> <pre><code>def hyperparameter_search(epochs, num_runs=10, num_models=10, **tuner_search_kwargs):\n    tuner = find_hyperparameters(**tuner_search_kwargs)\n    tuner = load_latest_tuner(**tuner_search_kwargs)\n\n    stats = create_tuner_stats(\n        tuner,\n        epochs=epochs,\n        num_runs=num_runs,\n        num_models=num_models,\n        train_ds=tuner_search_kwargs[\"train_ds\"],\n        test_ds=tuner_search_kwargs[\"test_ds\"],\n    )\n\n    return stats, tuner\n</code></pre>"},{"location":"Experiments/#comparison-with-methods-and-datasets-from-comet-1-reference-20-in-our-paper","title":"Comparison with methods and datasets from COMET [1] (Reference #20 in our paper)","text":"<p>References:</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>Github repo: https://github.com/AishwaryaSivaraman/COMET</p>"},{"location":"Experiments/#experiment-for-auto-mpg-dataset","title":"Experiment for Auto MPG dataset","text":"<p>The Auto MPG Dataset is a regression dataset [1] with 7 features - Cylinders, Displacement, Horsepower,Weight, Acceleration, Model Year, Origin. And the dependant variable is monotonically decreasing with respect to features weigh, displacement, and horsepower. The <code>monotonicity_indicator</code> corrsponding to these features are set to -1, since the relationship is a monotonically decreasing one with respect to the dependant variable.</p> <p>References:</p> <ol> <li>Quinlan,R. (1993). Combining Instance-Based and Model-Based     Learning. In Proceedings on the Tenth International Conference of     Machine Learning, 236-243, University of Massachusetts, Amherst.     Morgan Kaufmann.</li> </ol> <p>https://archive.ics.uci.edu/ml/datasets/auto+mpg</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <pre><code>auto_train_df, auto_test_df = get_train_n_test_data(\n    data_path=data_path, dataset_name=\"auto\"\n)\ndisplay(auto_train_df)\n</code></pre> <pre><code>auto_train_ds = (\n    df2ds(auto_train_df).repeat(10).shuffle(10 * auto_train_df.shape[0]).batch(16)\n)\nauto_test_ds = df2ds(auto_test_df).batch(16)\n</code></pre> <pre><code>def build_auto_model_f(\n    **kwargs,\n) -&gt; Model:\n    monotonicity_indicator = {\n        \"Cylinders\": 0,\n        \"Displacement\": -1,\n        \"Horsepower\": -1,\n        \"Weight\": -1,\n        \"Acceleration\": 0,\n        \"Model_Year\": 0,\n        \"Origin\": 0,\n    }\n\n    metrics = \"mse\"\n    loss = \"mse\"\n\n    return build_mono_model_f(\n        monotonicity_indicator=monotonicity_indicator,\n        metrics=metrics,\n        loss=loss,\n        train_ds=auto_train_ds,\n        **kwargs,\n    )\n</code></pre> <pre><code>auto_model = build_auto_model_f(\n    units=16,\n    n_layers=3,\n    activation=\"relu\",\n    dropout=0.1,\n    weight_decay=0.1,\n    learning_rate=0.1,\n    decay_rate=0.8,\n)\nauto_model.summary()\nauto_model.fit(\n    auto_train_ds,\n    validation_data=auto_test_ds,\n    epochs=1,\n)\n</code></pre> <pre><code>def build_auto_model(hp) -&gt; Model:\n    return build_auto_model_f(\n        units=hp.Int(\"units\", min_value=8, max_value=32, step=1),\n        n_layers=hp.Int(\"n_layers\", min_value=1, max_value=4),\n        activation=hp.Choice(\"activation\", values=[\"elu\"]),\n        learning_rate=hp.Float(\n            \"learning_rate\", min_value=1e-2, max_value=0.3, sampling=\"log\"\n        ),\n        weight_decay=hp.Float(\n            \"weight_decay\", min_value=1e-2, max_value=0.3, sampling=\"log\"\n        ),\n        dropout=hp.Float(\"dropout\", min_value=0.0, max_value=0.25, sampling=\"linear\"),\n        decay_rate=hp.Float(\n            \"decay_rate\", min_value=0.1, max_value=1.0, sampling=\"reverse_log\"\n        ),\n    )\n\n\ndef get_auto_tuner_search_kwargs(build_auto_model, *, max_trials, executions_per_trial):\n    auto_tuner_search_kwargs = dict(\n        build_model_f=build_auto_model,\n        tuner_name=\"BayesianOptimization\",\n        train_ds=auto_train_ds,\n        test_ds=auto_test_ds,\n        objective=Objective(\"val_mse\", direction=\"min\"),\n        max_epochs=5,\n        executions_per_trial=executions_per_trial,\n        dir_root=\"/tmp/tuner/auto_tuner\",\n        project_name=\"auto_tuner\",\n        max_trials=max_trials,\n    )\n    return auto_tuner_search_kwargs\n</code></pre> <pre><code>if should_find_hyperparam[\"auto\"]:\n    auto_tuner = find_hyperparameters(\n        **get_auto_tuner_search_kwargs(\n            build_auto_model, max_trials=100, executions_per_trial=5\n        )\n    )genesis8\n</code></pre> <pre><code>if should_find_hyperparam[\"auto\"]:\n    auto_stats = create_tuner_stats(\n        auto_tuner,\n        epochs=5,\n        num_runs=10,\n        num_models=10,\n        train_ds=auto_train_ds,\n        test_ds=auto_test_ds,\n    )\n</code></pre> <pre><code>def final_build_auto_model(hp) -&gt; Model:\n    return build_auto_model_f(\n        units=hp.Fixed(\"units\", 16),\n        n_layers=hp.Fixed(\"n_layers\", 2),\n        activation=hp.Fixed(\"activation\", \"elu\"),\n        learning_rate=hp.Fixed(\"learning_rate\", 0.124332),\n        weight_decay=hp.Fixed(\"weight_decay\", 0.026992),\n        dropout=hp.Fixed(\"dropout\", 0.032543),\n        decay_rate=hp.Fixed(\"decay_rate\", 0.303206),\n    )\n\n\nfinal_auto_tuner = find_hyperparameters(\n    **get_auto_tuner_search_kwargs(\n        final_build_auto_model, max_trials=1, executions_per_trial=1\n    )\n)\nfinal_auto_stats = create_tuner_stats(\n    final_auto_tuner,\n    epochs=5,\n    num_runs=10,\n    num_models=1,\n    train_ds=auto_train_ds,\n    test_ds=auto_test_ds,\n)\n</code></pre>"},{"location":"Experiments/#experiment-for-heart-disease-dataset-1","title":"Experiment for Heart Disease Dataset [1]","text":"<p>Heart Disease [1] is a classification dataset used for predicting the presence of heart disease with 13 features (age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal) and monotonically increasing with respect to features- trestbps and cholestrol (chol). The <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li>John H. Gennari, Pat Langley, and Douglas H. Fisher. Models of     incremental concept formation. Artif. Intell., 40(1-3):11\u201361, 1989.</li> </ol> <p>https://archive.ics.uci.edu/ml/datasets/heart+disease</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <pre><code>heart_train_df, heart_test_df = get_train_n_test_data(\n    data_path=data_path, dataset_name=\"heart\"\n)\ndisplay(heart_train_df)\n</code></pre> <pre><code>heart_train_ds = (\n    df2ds(heart_train_df).repeat(10).shuffle(10 * heart_train_df.shape[0]).batch(16)\n)\nheart_test_ds = df2ds(heart_test_df).batch(16)\n\n# peek(heart_train_ds), len(heart_train_ds)\n</code></pre> <pre><code>def build_heart_model_f(\n    **kwargs,\n) -&gt; Model:\n    monotonicity_indicator = {\n        \"age\": 0,\n        \"sex\": 0,\n        \"cp\": 0,\n        \"trestbps\": 1,\n        \"chol\": 1,\n        \"fbs\": 0,\n        \"restecg\": 0,\n        \"thalach\": 0,\n        \"exang\": 0,\n        \"oldpeak\": 0,\n        \"slope\": 0,\n        \"ca\": 0,\n        \"thal\": 0,\n    }\n\n    metrics = \"accuracy\"\n    loss = \"binary_crossentropy\"\n\n    return build_mono_model_f(\n        monotonicity_indicator=monotonicity_indicator,\n        metrics=metrics,\n        loss=loss,\n        final_activation=\"sigmoid\",\n        train_ds=heart_train_ds,\n        **kwargs,\n    )\n</code></pre> <pre><code>heart_model = build_heart_model_f(\n    units=16,\n    n_layers=3,\n    activation=\"relu\",\n    dropout=0.1,\n    weight_decay=0.1,\n    learning_rate=0.1,\n    decay_rate=0.8,\n)\nheart_model.summary()\nheart_model.fit(\n    heart_train_ds,\n    validation_data=heart_test_ds,\n    epochs=1,\n)\n</code></pre> <pre><code>def build_heart_model(hp) -&gt; Model:\n    return build_heart_model_f(\n        units=hp.Int(\"units\", min_value=12, max_value=24, step=1),\n        n_layers=hp.Int(\"n_layers\", min_value=2, max_value=3),\n        activation=hp.Choice(\"activation\", values=[\"elu\"]),\n        learning_rate=hp.Float(\n            \"learning_rate\", min_value=1e-2, max_value=0.3, sampling=\"log\"\n        ),\n        weight_decay=hp.Float(\n            \"weight_decay\", min_value=1e-2, max_value=0.3, sampling=\"log\"\n        ),\n        dropout=hp.Float(\"dropout\", min_value=0.0, max_value=0.5, sampling=\"linear\"),\n        decay_rate=hp.Float(\n            \"decay_rate\", min_value=0.1, max_value=1.0, sampling=\"reverse_log\"\n        ),\n    )\n\n\ndef get_heart_tuner_search_kwargs(\n    build_heart_model, *, max_trials, executions_per_trial\n):\n    heart_tuner_search_kwargs = dict(\n        build_model_f=build_heart_model,\n        tuner_name=\"BayesianOptimization\",\n        train_ds=heart_train_ds,\n        test_ds=heart_test_ds,\n        objective=Objective(\"val_accuracy\", direction=\"max\"),\n        max_epochs=10,\n        executions_per_trial=executions_per_trial,\n        dir_root=\"/tmp/tuner/heart_tuner\",\n        project_name=\"heart_tuner\",\n        max_trials=max_trials,\n    )\n    return heart_tuner_search_kwargs\n</code></pre> <pre><code>heart_tuner = find_hyperparameters(\n    **get_heart_tuner_search_kwargs(\n        build_heart_model, max_trials=100, executions_per_trial=5\n    )\n)\n</code></pre> <pre><code>create_tuner_stats(\n    heart_tuner,\n    epochs=5,\n    num_runs=10,\n    num_models=10,\n    train_ds=heart_train_ds,\n    test_ds=heart_test_ds,\n)\n</code></pre> <pre><code>def final_build_heart_model(hp) -&gt; Model:\n    return build_heart_model_f(\n        units=hp.Fixed(\"units\", 16),\n        n_layers=hp.Fixed(\"n_layers\", 2),\n        activation=hp.Fixed(\"activation\", \"elu\"),\n        learning_rate=hp.Fixed(\"learning_rate\", 0.06543),\n        weight_decay=hp.Fixed(\"weight_decay\", 0.01),\n        dropout=hp.Fixed(\"dropout\", 0.25),\n        decay_rate=hp.Fixed(\"decay_rate\", 0.99983),\n    )\n\n\nfinal_heart_tuner = find_hyperparameters(\n    **get_heart_tuner_search_kwargs(\n        final_build_heart_model, max_trials=1, executions_per_trial=1\n    )\n)\ncreate_tuner_stats(\n    final_heart_tuner,\n    epochs=5,\n    num_runs=10,\n    num_models=1,\n    train_ds=heart_train_ds,\n    test_ds=heart_test_ds,\n)\n</code></pre> <p></p> <p>The figure above shows the table from our paper for reference. As can be seen from our experiments above, our proposed methodology performs comparable to or better than state-of-the-art</p>"},{"location":"Experiments/#comparison-with-methods-and-datasets-from-certified-monotonic-network-1-reference-20-in-our-paper","title":"Comparison with methods and datasets from Certified Monotonic Network [1] (Reference #20 in our paper)","text":"<p>References:</p> <ol> <li>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</li> </ol>"},{"location":"Experiments/#experiment-for-compas-dataset-1","title":"Experiment for Compas Dataset [1]","text":"<p>COMPAS [1] is a dataset containing the criminal records of 6,172 individuals arrested in Florida. The task is to predict whether the individual will commit a crime again in 2 years. The probability predicted by the system will be used as a risk score. As mentioned in [2] 13 attributes for prediction. The risk score should be monotonically increasing w.r.t. four attributes, number of prior adult convictions, number of juvenile felony, number of juvenile misdemeanor, and number of other convictions. The <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li> <p>S. Mattu J. Angwin, J. Larson and L. Kirchner. Machine bias: There\u2019s     software used across the country to predict future criminals. and     it\u2019s biased against blacks. ProPublica, 2016.</p> </li> <li> <p>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</p> </li> </ol> <pre><code>compas_train_df, compas_test_df = get_train_n_test_data(\n    data_path=data_path, dataset_name=\"compas\"\n)\ndisplay(compas_train_df)\n</code></pre> <pre><code># compas_train_ds = df2ds(compas_train_df).repeat(10).shuffle(10 * compas_train_df.shape[0]).batch(16)\n# compas_test_ds = df2ds(compas_test_df).batch(16)\n\ncompas_train_ds = df2ds(compas_train_df).shuffle(compas_train_df.shape[0]).batch(16)\ncompas_test_ds = df2ds(compas_test_df).batch(16)\n\npeek(compas_train_ds), len(compas_train_ds)\n</code></pre> <pre><code>def build_compas_model_f(\n    **kwargs,\n) -&gt; Model:\n    monotonicity_indicator = {\n        \"priors_count\": 1,\n        \"juv_fel_count\": 1,\n        \"juv_misd_count\": 1,\n        \"juv_other_count\": 1,\n        \"age\": 0,\n        \"race_0\": 0,\n        \"race_1\": 0,\n        \"race_2\": 0,\n        \"race_3\": 0,\n        \"race_4\": 0,\n        \"race_5\": 0,\n        \"sex_0\": 0,\n        \"sex_1\": 0,\n    }\n\n    metrics = \"accuracy\"\n    loss = \"binary_crossentropy\"\n\n    return build_mono_model_f(\n        monotonicity_indicator=monotonicity_indicator,\n        metrics=metrics,\n        loss=loss,\n        final_activation=\"sigmoid\",\n        train_ds=compas_train_ds,\n        **kwargs,\n    )\n</code></pre> <pre><code>compas_model = build_compas_model_f(\n    units=16,\n    n_layers=3,\n    activation=\"relu\",\n    dropout=0.1,\n    weight_decay=0.1,\n    learning_rate=0.1,\n    decay_rate=0.8,\n)\ncompas_model.summary()\n</code></pre> <pre><code>compas_model.fit(\n    compas_train_ds,\n    validation_data=compas_test_ds,\n    epochs=2,\n)\n</code></pre> <pre><code>def build_compas_model(hp) -&gt; Model:\n    return build_compas_model_f(\n        units=hp.Int(\"units\", min_value=8, max_value=32, step=1),\n        n_layers=hp.Int(\"n_layers\", min_value=1, max_value=3),\n        activation=hp.Choice(\"activation\", values=[\"elu\"]),\n        learning_rate=hp.Float(\n            \"learning_rate\", min_value=1e-2, max_value=0.3, sampling=\"log\"\n        ),\n        weight_decay=hp.Float(\n            \"weight_decay\", min_value=1e-2, max_value=0.3, sampling=\"log\"\n        ),\n        dropout=hp.Float(\"dropout\", min_value=0.0, max_value=0.5, sampling=\"linear\"),\n        decay_rate=hp.Float(\n            \"decay_rate\", min_value=0.1, max_value=1.0, sampling=\"reverse_log\"\n        ),\n    )\n\n\ndef get_compas_tuner_search_kwargs(\n    build_compas_model, *, max_trials, executions_per_trial\n):\n    compas_tuner_search_kwargs = dict(\n        build_model_f=build_compas_model,\n        tuner_name=\"BayesianOptimization\",\n        train_ds=compas_train_ds,\n        test_ds=compas_test_ds,\n        objective=Objective(\"val_accuracy\", direction=\"max\"),\n        max_epochs=20,\n        executions_per_trial=executions_per_trial,\n        dir_root=\"/tmp/tuner/compas_tuner\",\n        project_name=\"compas_tuner\",\n        max_trials=max_trials,\n    )\n    return compas_tuner_search_kwargs\n</code></pre> <pre><code>compas_tuner = find_hyperparameters(\n    **get_compas_tuner_search_kwargs(\n        build_compas_model, max_trials=100, executions_per_trial=5\n    )\n)\n</code></pre>"},{"location":"Experiments/#experiment-for-blog-dataset-1","title":"Experiment for Blog Dataset [1]","text":"<p>Blog Feedback [1] is a dataset containing 54,270 data points from blog posts. The raw HTML-documents of the blog posts were crawled and processed. The prediction task associated with the data is the prediction of the number of comments in the upcoming 24 hours. The feature of the dataset has 276 dimensions, and 8 attributes among them should be monotonically non-decreasing with the prediction. They are A51, A52, A53, A54, A56, A57, A58, A59. Thus the <code>monotonicity_indicator</code> corrsponding to these features are set to 1. As done in [2], we only use the data points with targets smaller than the 90th percentile.</p> <p>References:</p> <ol> <li>Krisztian Buza. Feedback prediction for blogs. In Data analysis,     machine learning and knowledge discovery, pages 145\u2013152. Springer,     2014</li> <li>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</li> </ol> <pre><code>tf.keras.utils.set_random_seed(42)\n\nmonotonicity_indicator = np.zeros((276))\nmonotonicity_indicator[50:54] = 1.0\nmonotonicity_indicator[55:59] = 1.0\n\n# convexity_indicator = None\n\ntrain_params = dict(\n    batch_size=256,\n    num_epochs=100,\n    units=4,\n    n_layers=2,\n    activation=\"elu\",\n    loss=\"mean_squared_error\",\n    metrics=tf.keras.metrics.RootMeanSquaredError(),\n    learning_rate=0.01,\n    is_classification=False,\n)\n\n\nhistory, monotonic_model = train_dataset(\n    dataset_name=\"blog\",\n    monotonicity_indicator=monotonicity_indicator,\n    #     convexity_indicator=convexity_indicator,\n    train_params=train_params,\n)\n</code></pre>"},{"location":"Experiments/#experiment-for-loan-dataset-1","title":"Experiment for Loan Dataset [1]","text":"<p>Lending club loan data contains complete loan data for all loans issued through 2007-2015 of several banks. Each data point is a 28-dimensional feature including the current loan status, latest payment information, and other additional features. The task is to predict loan defaulters given the feature vector. The possibility of loan default should be nondecreasing w.r.t. number of public record bankruptcies, Debt-to-Income ratio, and non-increasing w.r.t. credit score, length of employment, annual income. Thus the <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li>https://www.kaggle.com/wendykan/lending-club-loan-data (Note:     Currently, the dataset seems to be withdrawn from kaggle)</li> </ol> <pre><code>tf.keras.utils.set_random_seed(42)\n\n# monotonicity_indicator = np.array([-1,  1, -1, -1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n#   0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\nmonotonicity_indicator = np.array([-1, 1, -1, -1, 1] + [0] * 24)\n\nconvexity_indicator = None\n\ntrain_params = dict(\n    batch_size=256,\n    num_epochs=20,\n    units=4,\n    n_layers=1,\n    activation=\"elu\",\n    loss=\"binary_crossentropy\",\n    metrics=\"accuracy\",\n    learning_rate=0.008,\n    is_classification=True,\n)\n\n\nhistory, monotonic_model = train_dataset(\n    dataset_name=\"loan\",\n    monotonicity_indicator=monotonicity_indicator,\n    #     convexity_indicator=convexity_indicator,\n    train_params=train_params,\n)\n</code></pre> <p>The figure above shows the table from our paper for reference. As can be seen from our experiments above, our proposed methodology performs comparable to or better than state-of-the-art</p> <p></p>"},{"location":"Helpers/","title":"Helpers","text":"<p>source</p>"},{"location":"Helpers/#export","title":"export","text":"<pre><code> export (o:~T)\n</code></pre> <pre><code>@export\ndef f():\n    pass\n\nassert f.__module__ == \"mono_dense_keras\"\n</code></pre>"},{"location":"MonoDenseLayer/","title":"Monotonic dense layer","text":""},{"location":"MonoDenseLayer/#imports","title":"Imports","text":"<pre><code>from pathlib import Path\nfrom os import environ\n\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport pytest\nimport seaborn as sns\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras import Model\n</code></pre> <pre><code>environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n</code></pre>"},{"location":"MonoDenseLayer/#monotonic-dense-layer_1","title":"Monotonic Dense Layer","text":""},{"location":"MonoDenseLayer/#actvation-functions","title":"Actvation Functions","text":"<p>We use \\(\\breve{\\mathcal{A}}\\) to denote the set of all zero-centred, monotonically increasing, convex, lower-bounded functions.</p> <p>Let \\(\\breve{\\rho} \\in \\breve{\\mathcal{A}}\\). Then</p> <p>In the code below, the following names are used for denotation of the above functions:</p> <ul> <li> <p><code>convex_activation</code> denotes \\(\\breve{\\rho}\\),</p> </li> <li> <p><code>concave_activation</code> denotes \\(\\hat{\\rho}\\), and</p> </li> <li> <p><code>saturated_activation</code> denotes \\(\\tilde{\\rho}\\).</p> </li> </ul> <p>source</p>"},{"location":"MonoDenseLayer/#get_activation_functions","title":"get_activation_functions","text":"<pre><code> get_activation_functions (activation:Union[str,Callable[[Union[tensorflow\n                           .python.types.core.Tensor,tensorflow.python.typ\n                           es.core.TensorProtocol,int,float,bool,str,bytes\n                           ,complex,tuple,list,numpy.ndarray,numpy.generic\n                           ]],Union[tensorflow.python.types.core.Tensor,te\n                           nsorflow.python.types.core.TensorProtocol,int,f\n                           loat,bool,str,bytes,complex,tuple,list,numpy.nd\n                           array,numpy.generic]],NoneType]=None)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#get_saturated_activation","title":"get_saturated_activation","text":"<pre><code> get_saturated_activation (convex_activation:Callable[[Union[tensorflow.py\n                           thon.types.core.Tensor,tensorflow.python.types.\n                           core.TensorProtocol,int,float,bool,str,bytes,co\n                           mplex,tuple,list,numpy.ndarray,numpy.generic]],\n                           Union[tensorflow.python.types.core.Tensor,tenso\n                           rflow.python.types.core.TensorProtocol,int,floa\n                           t,bool,str,bytes,complex,tuple,list,numpy.ndarr\n                           ay,numpy.generic]], concave_activation:Callable\n                           [[Union[tensorflow.python.types.core.Tensor,ten\n                           sorflow.python.types.core.TensorProtocol,int,fl\n                           oat,bool,str,bytes,complex,tuple,list,numpy.nda\n                           rray,numpy.generic]],Union[tensorflow.python.ty\n                           pes.core.Tensor,tensorflow.python.types.core.Te\n                           nsorProtocol,int,float,bool,str,bytes,complex,t\n                           uple,list,numpy.ndarray,numpy.generic]],\n                           a:float=1.0, c:float=1.0)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_activations","title":"apply_activations","text":"<pre><code> apply_activations (x:Union[tensorflow.python.types.core.Tensor,tensorflow\n                    .python.types.core.TensorProtocol,int,float,bool,str,b\n                    ytes,complex,tuple,list,numpy.ndarray,numpy.generic],\n                    units:int, convex_activation:Callable[[Union[tensorflo\n                    w.python.types.core.Tensor,tensorflow.python.types.cor\n                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl\n                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.\n                    python.types.core.Tensor,tensorflow.python.types.core.\n                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                    list,numpy.ndarray,numpy.generic]], concave_activation\n                    :Callable[[Union[tensorflow.python.types.core.Tensor,t\n                    ensorflow.python.types.core.TensorProtocol,int,float,b\n                    ool,str,bytes,complex,tuple,list,numpy.ndarray,numpy.g\n                    eneric]],Union[tensorflow.python.types.core.Tensor,ten\n                    sorflow.python.types.core.TensorProtocol,int,float,boo\n                    l,str,bytes,complex,tuple,list,numpy.ndarray,numpy.gen\n                    eric]], saturated_activation:Callable[[Union[tensorflo\n                    w.python.types.core.Tensor,tensorflow.python.types.cor\n                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl\n                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.\n                    python.types.core.Tensor,tensorflow.python.types.core.\n                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                    list,numpy.ndarray,numpy.generic]],\n                    is_convex:bool=False, is_concave:bool=False,\n                    activation_weights:Tuple[float,float,float]=(7.0, 7.0,\n                    2.0))\n</code></pre> <pre><code>def plot_applied_activation(\n    activation: str = \"relu\",\n    *,\n    save_pdf: bool = False,\n    save_path: Union[Path, str] = \"plots\",\n    font_size: int = 20,\n    linestyle=\"--\",\n    alpha=0.7,\n    linewidth=2.0,\n):\n    font = {\"size\": font_size}\n    matplotlib.rc(\"font\", **font)\n    plt.rcParams[\"figure.figsize\"] = (18, 3)\n\n    x = np.arange(-1.5, 1.5, step=3 / 256)\n    h = 3 * np.sin(2 * np.pi * x)\n\n    (\n        convex_activation,\n        concave_activation,\n        saturated_activation,\n    ) = get_activation_functions(activation)\n\n    y = apply_activations(\n        h,\n        convex_activation=convex_activation,\n        concave_activation=concave_activation,\n        saturated_activation=saturated_activation,\n        units=x.shape[0],\n        activation_weights=(1.0, 1.0, 1.0),\n    )\n\n    plot_kwargs = dict(linestyle=linestyle, alpha=alpha, linewidth=linewidth)\n\n    plt.plot(np.arange(x.shape[0]), h, label=\"$h$\", **plot_kwargs)\n    plt.plot(np.arange(x.shape[0]), y, label=r\"${\\rho}(h)$\", **plot_kwargs)\n    title = (\n        \"Applying \"\n        + (activation.__name__ if hasattr(activation, \"__name__\") else activation)\n        + f\"-based activations to {x.shape[0]}-dimensional vector\"\n        + r\" $h$\"\n    )\n    plt.title(title)\n\n    plt.legend()\n\n    if save_pdf:\n        path = Path(save_path) / (title.replace(\" \", \"_\") + \".pdf\")\n        path.parent.mkdir(exist_ok=True, parents=True)\n        plt.savefig(path, format=\"pdf\")\n    #         print(f\"Saved figure to: {path}\")\n\n    plt.show()\n</code></pre> <pre><code>for activation in [\"linear\", \"ReLU\", \"ELU\", \"SELU\"]:\n    plot_applied_activation(activation, save_pdf=True)\n</code></pre>"},{"location":"MonoDenseLayer/#monotonicity-indicator","title":"Monotonicity indicator","text":"<p>source</p>"},{"location":"MonoDenseLayer/#get_monotonicity_indicator","title":"get_monotonicity_indicator","text":"<pre><code> get_monotonicity_indicator (monotonicity_indicator:Union[numpy.__array_li\n                             ke._SupportsArray[numpy.dtype],numpy.__nested\n                             _sequence._NestedSequence[numpy.__array_like.\n                             _SupportsArray[numpy.dtype]],bool,int,float,c\n                             omplex,str,bytes,numpy.__nested_sequence._Nes\n                             tedSequence[Union[bool,int,float,complex,str,\n                             bytes]]], input_shape:Tuple[int,...],\n                             units:int)\n</code></pre> <pre><code>input_shape = (13, 2)\nunits = 3\n\nlayer = Dense(units=units)\nlayer.build(input_shape=input_shape)\n\nfor monotonicity_indicator in [\n    1,\n    [1],\n    [1, 1],\n    np.ones((2,)),\n    np.ones((2, 1)),\n    np.ones((2, 3)),\n]:\n    expected = np.ones((2, 3))\n    actual = get_monotonicity_indicator(\n        monotonicity_indicator, input_shape=(13, 2), units=3\n    )\n\n    # rank is 2\n    assert len(actual.shape) == 2\n    # it is broadcastable to the kernel shape of (input_shape[-1], units)\n    np.testing.assert_array_equal(np.broadcast_to(actual, (2, 3)), expected)\n</code></pre> <pre><code>expected = [[1], [0], [-1]]\nactual = get_monotonicity_indicator([1, 0, -1], input_shape=(13, 3), units=4)\nnp.testing.assert_array_equal(actual, expected)\n</code></pre> <pre><code>with pytest.raises(ValueError) as e:\n    get_monotonicity_indicator([0, 1, -1], input_shape=(13, 2), units=3)\nassert e.value.args == (\n    \"operands could not be broadcast together with remapped shapes [original-&gt;remapped]: (3,1)  and requested shape (2,3)\",\n)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#replace_kernel_using_monotonicity_indicator","title":"replace_kernel_using_monotonicity_indicator","text":"<pre><code> replace_kernel_using_monotonicity_indicator\n                                              (layer:keras.layers.core.den\n                                              se.Dense, monotonicity_indic\n                                              ator:Union[tensorflow.python\n                                              .types.core.Tensor,tensorflo\n                                              w.python.types.core.TensorPr\n                                              otocol,int,float,bool,str,by\n                                              tes,complex,tuple,list,numpy\n                                              .ndarray,numpy.generic])\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_monotonicity_indicator_to_kernel","title":"apply_monotonicity_indicator_to_kernel","text":"<pre><code> apply_monotonicity_indicator_to_kernel\n                                         (kernel:tensorflow.python.ops.var\n                                         iables.Variable, monotonicity_ind\n                                         icator:Union[numpy.__array_like._\n                                         SupportsArray[numpy.dtype],numpy.\n                                         __nested_sequence._NestedSequence\n                                         [numpy.__array_like._SupportsArra\n                                         y[numpy.dtype]],bool,int,float,co\n                                         mplex,str,bytes,numpy.__nested_se\n                                         quence._NestedSequence[Union[bool\n                                         ,int,float,complex,str,bytes]]])\n</code></pre> <pre><code>def display_kernel(kernel: Union[tf.Variable, np.typing.NDArray[float]]) -&gt; None:\n    cm = sns.color_palette(\"coolwarm_r\", as_cmap=True)\n\n    df = pd.DataFrame(kernel)\n\n    display(df.style.format(\"{:.2f}\").background_gradient(cmap=cm, vmin=-1e-8, vmax=1e-8))\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\n\nunits = 18\ninput_len = 7\n\nlayer = tf.keras.layers.Dense(units=units)\n\ninput_shape = (input_len,)\nlayer.build(input_shape=input_shape)\n\nprint(\"Original kernel:\")\ndisplay_kernel(layer.kernel)\n\nprint(\"Kernel after applying monotocity indicator 1 for all values:\")\nmonotonicity_indicator = get_monotonicity_indicator(\n    1, input_shape=input_shape, units=units\n)\nwith replace_kernel_using_monotonicity_indicator(layer, monotonicity_indicator):\n    display_kernel(layer.kernel)\n</code></pre> <pre><code>Original kernel:\nKernel after applying monotocity indicator 1 for all values:\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 -0.14 0.44 -0.41 0.15 0.46 -0.33 0.02 0.13 -0.41 -0.05 0.46 -0.03 0.00 0.26 -0.47 -0.30 1 0.01 -0.42 -0.45 0.34 0.41 -0.23 0.35 -0.36 -0.04 0.06 0.07 -0.29 -0.28 0.48 -0.38 -0.06 -0.23 -0.37 2 0.23 -0.31 0.18 0.15 -0.45 0.06 -0.16 -0.11 0.45 -0.09 0.03 -0.24 -0.37 0.21 0.11 0.01 -0.46 -0.37 3 0.29 0.36 -0.07 -0.18 -0.46 -0.45 0.25 0.32 -0.12 0.22 -0.18 0.27 -0.18 -0.07 0.35 0.32 0.18 0.39 4 0.35 -0.27 0.13 -0.40 0.44 0.21 0.06 -0.31 -0.30 0.46 -0.44 -0.18 -0.26 -0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 -0.02 -0.36 0.39 -0.13 0.30 0.35 -0.12 -0.43 0.44 0.32 0.06 -0.30 -0.29 0.24 -0.44 -0.13 6 0.38 -0.04 -0.30 0.17 -0.03 0.37 -0.03 -0.18 0.42 -0.39 -0.33 -0.19 0.02 -0.41 -0.44 0.42 0.38 -0.21 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 0.14 0.44 0.41 0.15 0.46 0.33 0.02 0.13 0.41 0.05 0.46 0.03 0.00 0.26 0.47 0.30 1 0.01 0.42 0.45 0.34 0.41 0.23 0.35 0.36 0.04 0.06 0.07 0.29 0.28 0.48 0.38 0.06 0.23 0.37 2 0.23 0.31 0.18 0.15 0.45 0.06 0.16 0.11 0.45 0.09 0.03 0.24 0.37 0.21 0.11 0.01 0.46 0.37 3 0.29 0.36 0.07 0.18 0.46 0.45 0.25 0.32 0.12 0.22 0.18 0.27 0.18 0.07 0.35 0.32 0.18 0.39 4 0.35 0.27 0.13 0.40 0.44 0.21 0.06 0.31 0.30 0.46 0.44 0.18 0.26 0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 0.02 0.36 0.39 0.13 0.30 0.35 0.12 0.43 0.44 0.32 0.06 0.30 0.29 0.24 0.44 0.13 6 0.38 0.04 0.30 0.17 0.03 0.37 0.03 0.18 0.42 0.39 0.33 0.19 0.02 0.41 0.44 0.42 0.38 0.21 <pre><code>monotonicity_indicator = [1] * 2 + [-1] * 2 + [0] * (input_shape[0] - 4)\nmonotonicity_indicator = get_monotonicity_indicator(\n    monotonicity_indicator, input_shape=input_shape, units=units\n)\n\nprint(\"Monotocity indicator:\")\ndisplay_kernel(monotonicity_indicator)\n\nprint(\"Kernel after applying the monotocity indicator:\")\nwith replace_kernel_using_monotonicity_indicator(\n    layer, monotonicity_indicator\n):\n    display_kernel(layer.kernel)\n</code></pre> <pre><code>Monotocity indicator:\nKernel after applying the monotocity indicator:\n</code></pre> 0 0 1.00 1 1.00 2 -1.00 3 -1.00 4 0.00 5 0.00 6 0.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 0.14 0.44 0.41 0.15 0.46 0.33 0.02 0.13 0.41 0.05 0.46 0.03 0.00 0.26 0.47 0.30 1 0.01 0.42 0.45 0.34 0.41 0.23 0.35 0.36 0.04 0.06 0.07 0.29 0.28 0.48 0.38 0.06 0.23 0.37 2 -0.23 -0.31 -0.18 -0.15 -0.45 -0.06 -0.16 -0.11 -0.45 -0.09 -0.03 -0.24 -0.37 -0.21 -0.11 -0.01 -0.46 -0.37 3 -0.29 -0.36 -0.07 -0.18 -0.46 -0.45 -0.25 -0.32 -0.12 -0.22 -0.18 -0.27 -0.18 -0.07 -0.35 -0.32 -0.18 -0.39 4 0.35 -0.27 0.13 -0.40 0.44 0.21 0.06 -0.31 -0.30 0.46 -0.44 -0.18 -0.26 -0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 -0.02 -0.36 0.39 -0.13 0.30 0.35 -0.12 -0.43 0.44 0.32 0.06 -0.30 -0.29 0.24 -0.44 -0.13 6 0.38 -0.04 -0.30 0.17 -0.03 0.37 -0.03 -0.18 0.42 -0.39 -0.33 -0.19 0.02 -0.41 -0.44 0.42 0.38 -0.21"},{"location":"MonoDenseLayer/#monotonic-dense-layer_2","title":"Monotonic Dense Layer","text":"<p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p>"},{"location":"MonoDenseLayer/#monodense","title":"MonoDense","text":"<pre><code> MonoDense (units:int, activation:Union[str,Callable[[Union[tensorflow.pyt\n            hon.types.core.Tensor,tensorflow.python.types.core.TensorProto\n            col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndarray,\n            numpy.generic]],Union[tensorflow.python.types.core.Tensor,tens\n            orflow.python.types.core.TensorProtocol,int,float,bool,str,byt\n            es,complex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=\n            None, monotonicity_indicator:Union[numpy.__array_like._Support\n            sArray[numpy.dtype],numpy.__nested_sequence._NestedSequence[nu\n            mpy.__array_like._SupportsArray[numpy.dtype]],bool,int,float,c\n            omplex,str,bytes,numpy.__nested_sequence._NestedSequence[Union\n            [bool,int,float,complex,str,bytes]]]=1, is_convex:bool=False,\n            is_concave:bool=False,\n            activation_weights:Tuple[float,float,float]=(7.0, 7.0, 2.0),\n            **kwargs:Dict[str,Any])\n</code></pre> <p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> <p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p> <pre><code>units = 18\nactivation = \"relu\"\nbatch_size = 9\nx_len = 11\n\nx = np.random.default_rng(42).normal(size=(batch_size, x_len))\n\ntf.keras.utils.set_random_seed(42)\n\nfor monotonicity_indicator in [[1]*4+[0]*4+[-1]*3, 1, np.ones((x_len,)), -1, -np.ones((x_len,))]:\n    print(\"*\"*120)    \n    mono_layer = MonoDense(\n        units=units,\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator,\n        activation_weights=(7, 7, 4),\n    )\n    print(\"input:\")\n    display_kernel(x)\n\n    y = mono_layer(x)\n    print(f\"monotonicity_indicator = {monotonicity_indicator}\")\n    display_kernel(mono_layer.monotonicity_indicator)\n\n    print(\"kernel:\")\n    with replace_kernel_using_monotonicity_indicator(\n        mono_layer, mono_layer.monotonicity_indicator\n    ):\n        display_kernel(mono_layer.kernel)\n\n    print(\"output:\")\n    display_kernel(y)\nprint(\"ok\")\n</code></pre> <pre><code>************************************************************************************************************************\ninput:\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function apply_activations&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nmonotonicity_indicator = [1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = 1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = -1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\nkernel:\noutput:\nok\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 0.00 5 0.00 6 0.00 7 0.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.33 0.15 0.13 0.41 0.38 0.14 0.43 0.30 0.02 0.12 0.38 0.05 0.42 0.03 0.00 0.24 0.44 0.28 1 0.01 0.39 0.42 0.32 0.38 0.22 0.33 0.34 0.03 0.06 0.06 0.27 0.26 0.45 0.35 0.05 0.21 0.34 2 0.21 0.29 0.16 0.14 0.42 0.06 0.15 0.10 0.41 0.08 0.03 0.22 0.34 0.20 0.11 0.01 0.43 0.35 3 0.27 0.33 0.06 0.17 0.42 0.42 0.24 0.30 0.11 0.20 0.17 0.25 0.17 0.07 0.32 0.30 0.17 0.36 4 0.32 -0.25 0.12 -0.37 0.41 0.20 0.06 -0.28 -0.27 0.43 -0.41 -0.17 -0.24 -0.31 0.33 0.31 0.11 0.03 5 0.04 0.19 -0.02 -0.34 0.36 -0.12 0.28 0.32 -0.11 -0.40 0.41 0.30 0.06 -0.28 -0.27 0.23 -0.41 -0.12 6 0.35 -0.04 -0.28 0.16 -0.03 0.35 -0.03 -0.16 0.39 -0.36 -0.31 -0.18 0.02 -0.38 -0.40 0.39 0.35 -0.19 7 0.33 -0.34 0.11 -0.29 0.25 -0.21 0.11 0.08 -0.19 -0.39 0.01 0.10 0.39 -0.25 -0.37 -0.27 0.04 0.34 8 -0.27 -0.09 -0.02 -0.45 -0.16 -0.12 -0.09 -0.43 -0.36 -0.09 -0.23 -0.42 -0.28 -0.24 -0.30 -0.31 -0.07 -0.07 9 -0.38 -0.34 -0.44 -0.42 -0.32 -0.06 -0.27 -0.28 -0.22 -0.05 -0.08 -0.07 -0.21 -0.39 -0.01 -0.26 -0.24 -0.42 10 -0.09 -0.45 -0.41 -0.36 -0.19 -0.09 -0.00 -0.34 -0.17 -0.18 -0.05 -0.39 -0.06 -0.20 -0.40 -0.33 -0.18 -0.01 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.01 0.40 0.00 1.38 0.00 0.10 0.00 -0.00 -0.00 -0.13 -0.00 -0.26 -0.00 -0.00 -0.55 -0.52 0.79 0.64 1 0.45 1.02 0.96 0.71 1.22 0.00 0.86 -0.00 -0.00 -0.09 -0.00 -0.00 -0.00 -0.00 0.26 -0.17 0.54 1.00 2 0.30 0.00 0.33 0.00 0.41 0.00 0.42 -0.53 -0.89 -0.29 -0.23 -0.84 -0.16 -0.93 -0.90 0.08 0.37 0.08 3 0.21 0.26 0.33 0.42 0.00 0.00 0.00 -0.16 -0.00 -0.61 -0.53 -0.07 -0.00 -0.00 -0.55 -0.66 0.83 0.78 4 1.38 0.49 0.70 0.82 1.47 0.54 0.63 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.73 0.97 0.94 0.91 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.86 -0.25 -0.00 -1.57 -1.19 -0.61 -0.23 0.13 -1.00 0.50 -0.06 6 0.00 0.00 0.00 0.17 0.00 0.00 0.00 -0.15 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.06 -1.00 0.00 0.12 7 0.00 0.96 0.35 0.93 0.00 0.32 0.17 -0.00 -0.00 -0.00 -0.00 -0.00 -0.17 -0.00 0.67 0.06 0.12 0.17 8 0.00 1.33 0.92 1.63 0.52 0.00 0.66 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.23 0.18 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.44 0.02 0.24 0.22 0.29 0.35 0.18 0.03 0.39 0.17 0.25 0.02 0.10 0.13 0.00 0.42 0.21 0.31 1 0.35 0.06 0.26 0.42 0.05 0.41 0.16 0.33 0.03 0.26 0.11 0.03 0.23 0.04 0.37 0.27 0.32 0.40 2 0.37 0.30 0.36 0.14 0.21 0.40 0.01 0.28 0.16 0.44 0.43 0.23 0.27 0.22 0.23 0.25 0.43 0.05 3 0.32 0.25 0.05 0.45 0.08 0.18 0.26 0.24 0.34 0.07 0.07 0.14 0.04 0.19 0.29 0.23 0.43 0.09 4 0.36 0.05 0.20 0.41 0.38 0.29 0.01 0.44 0.17 0.04 0.31 0.34 0.29 0.16 0.25 0.18 0.01 0.28 5 0.34 0.31 0.38 0.34 0.08 0.40 0.15 0.16 0.14 0.25 0.15 0.20 0.10 0.06 0.44 0.19 0.42 0.21 6 0.01 0.38 0.43 0.18 0.00 0.43 0.45 0.28 0.25 0.18 0.03 0.26 0.22 0.26 0.08 0.23 0.45 0.42 7 0.04 0.12 0.28 0.17 0.11 0.00 0.15 0.24 0.05 0.05 0.27 0.32 0.33 0.11 0.09 0.40 0.19 0.06 8 0.30 0.17 0.21 0.42 0.21 0.29 0.19 0.38 0.03 0.34 0.32 0.30 0.34 0.15 0.28 0.11 0.44 0.19 9 0.10 0.10 0.35 0.32 0.24 0.28 0.30 0.28 0.10 0.12 0.30 0.41 0.15 0.00 0.10 0.40 0.18 0.24 10 0.00 0.22 0.21 0.09 0.10 0.13 0.18 0.37 0.24 0.29 0.25 0.23 0.32 0.14 0.27 0.34 0.25 0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.01 0.00 0.00 0.00 0.00 0.00 -0.93 -0.00 -0.07 -0.58 -0.88 -0.58 -0.00 -0.87 -0.49 -0.05 -1.00 1 0.73 0.10 0.22 0.18 0.18 0.16 0.00 -0.23 -0.00 -0.00 -0.00 -0.09 -0.00 -0.00 0.16 0.47 0.53 -0.27 2 1.15 0.36 0.82 1.20 0.80 1.06 0.61 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.53 0.61 1.00 0.94 3 0.00 0.45 0.28 0.00 0.00 0.11 0.14 -0.00 -0.21 -0.00 -0.00 -0.00 -0.00 -0.00 0.15 0.08 0.72 -0.08 4 0.34 0.19 0.36 0.05 0.15 0.30 0.00 -0.00 -0.00 -0.08 -0.00 -0.00 -0.00 -0.00 0.06 0.38 0.04 0.14 5 0.00 0.00 0.26 0.00 0.67 0.05 0.00 -0.00 -0.16 -0.00 -0.00 -0.00 -0.00 -0.00 -0.08 0.30 -0.17 -0.17 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.76 -0.68 -0.28 -0.11 -0.37 -0.42 -0.40 -0.88 -0.41 -0.67 -1.00 7 0.01 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.17 -0.04 -0.57 -0.82 -0.50 -0.22 -0.07 -0.62 -0.13 -0.18 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.32 -0.35 -0.39 -0.77 -1.63 -1.12 -0.60 -0.47 -0.99 -1.00 -1.00 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 1.00 5 1.00 6 1.00 7 1.00 8 1.00 9 1.00 10 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.31 0.02 0.11 0.29 0.10 0.33 0.37 0.06 0.39 0.35 0.15 0.13 0.15 0.45 0.07 0.19 0.03 0.06 1 0.12 0.02 0.06 0.41 0.32 0.24 0.34 0.28 0.22 0.06 0.33 0.27 0.25 0.23 0.43 0.09 0.45 0.27 2 0.19 0.11 0.19 0.25 0.07 0.42 0.32 0.35 0.15 0.05 0.00 0.24 0.22 0.39 0.44 0.11 0.19 0.10 3 0.15 0.37 0.21 0.41 0.25 0.04 0.37 0.04 0.05 0.22 0.31 0.35 0.35 0.08 0.38 0.01 0.25 0.29 4 0.17 0.45 0.24 0.32 0.01 0.00 0.19 0.34 0.17 0.19 0.18 0.34 0.02 0.24 0.03 0.41 0.26 0.00 5 0.29 0.10 0.07 0.34 0.04 0.30 0.39 0.27 0.39 0.16 0.33 0.45 0.06 0.19 0.23 0.04 0.36 0.04 6 0.13 0.15 0.22 0.40 0.14 0.30 0.11 0.45 0.14 0.17 0.26 0.16 0.36 0.10 0.17 0.32 0.14 0.08 7 0.25 0.25 0.24 0.45 0.17 0.45 0.30 0.35 0.41 0.40 0.11 0.26 0.32 0.08 0.22 0.34 0.05 0.09 8 0.16 0.27 0.10 0.23 0.08 0.21 0.19 0.16 0.06 0.04 0.17 0.05 0.39 0.11 0.26 0.25 0.13 0.05 9 0.17 0.17 0.00 0.13 0.12 0.03 0.39 0.11 0.01 0.29 0.43 0.20 0.21 0.43 0.39 0.18 0.19 0.27 10 0.26 0.23 0.43 0.04 0.25 0.36 0.21 0.36 0.37 0.36 0.08 0.14 0.25 0.24 0.30 0.33 0.04 0.07 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.00 0.08 0.00 0.00 0.00 0.00 -0.82 -0.58 -0.32 -1.07 -1.09 -0.00 -0.63 -0.21 -0.74 -1.00 -0.15 1 0.36 0.00 0.00 0.51 0.11 0.72 0.76 -0.12 -0.00 -0.00 -0.05 -0.00 -0.00 -0.00 0.56 -0.34 0.13 0.22 2 0.72 0.68 0.32 1.10 0.10 0.84 0.68 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.20 0.97 0.33 -0.07 3 0.00 0.00 0.36 0.35 0.36 0.82 0.00 -0.00 -0.00 -0.19 -0.29 -0.13 -0.00 -0.20 0.67 0.20 -0.00 0.14 4 0.18 0.14 0.26 0.68 0.09 0.38 0.36 -0.00 -0.00 -0.00 -0.00 -0.00 -0.07 -0.00 0.14 0.15 0.33 0.10 5 0.01 0.55 0.50 0.00 0.00 0.21 0.00 -0.00 -0.27 -0.00 -0.44 -0.25 -0.00 -0.00 0.44 0.83 -0.24 -0.01 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.89 -0.85 -0.48 -0.77 -0.90 -0.21 -0.30 -0.09 -0.69 -0.83 -0.03 7 0.00 0.00 0.00 0.00 0.01 0.00 0.00 -0.79 -0.59 -0.65 -0.21 -0.55 -0.19 -0.37 -0.17 -0.71 -0.10 0.03 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.24 -0.48 -0.95 -1.13 -0.71 -1.40 -0.30 -0.76 -1.00 -0.47 -0.39 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.29 -0.12 -0.00 -0.17 -0.33 -0.17 -0.33 -0.36 -0.28 -0.16 -0.24 -0.22 -0.10 -0.13 -0.02 -0.38 -0.23 -0.02 1 -0.36 -0.13 -0.05 -0.07 -0.41 -0.30 -0.38 -0.06 -0.40 -0.42 -0.44 -0.03 -0.27 -0.03 -0.32 -0.31 -0.35 -0.40 2 -0.30 -0.07 -0.40 -0.06 -0.10 -0.21 -0.16 -0.22 -0.06 -0.36 -0.40 -0.42 -0.23 -0.22 -0.20 -0.33 -0.45 -0.06 3 -0.05 -0.08 -0.07 -0.30 -0.44 -0.23 -0.40 -0.25 -0.13 -0.31 -0.11 -0.13 -0.13 -0.34 -0.15 -0.05 -0.36 -0.13 4 -0.45 -0.34 -0.41 -0.39 -0.15 -0.10 -0.40 -0.32 -0.19 -0.13 -0.29 -0.39 -0.43 -0.29 -0.13 -0.05 -0.39 -0.01 5 -0.09 -0.38 -0.00 -0.12 -0.07 -0.42 -0.01 -0.12 -0.26 -0.28 -0.16 -0.06 -0.08 -0.43 -0.23 -0.28 -0.28 -0.07 6 -0.34 -0.38 -0.15 -0.44 -0.41 -0.19 -0.25 -0.41 -0.34 -0.22 -0.43 -0.36 -0.25 -0.28 -0.06 -0.12 -0.15 -0.16 7 -0.17 -0.39 -0.40 -0.26 -0.40 -0.20 -0.10 -0.14 -0.42 -0.21 -0.18 -0.25 -0.15 -0.21 -0.13 -0.41 -0.14 -0.14 8 -0.38 -0.03 -0.10 -0.21 -0.13 -0.04 -0.19 -0.00 -0.09 -0.38 -0.01 -0.27 -0.24 -0.24 -0.13 -0.18 -0.37 -0.21 9 -0.43 -0.08 -0.20 -0.29 -0.10 -0.27 -0.08 -0.43 -0.22 -0.37 -0.27 -0.24 -0.15 -0.22 -0.01 -0.45 -0.35 -0.31 10 -0.38 -0.44 -0.20 -0.31 -0.42 -0.23 -0.03 -0.31 -0.11 -0.35 -0.01 -0.00 -0.00 -0.39 -0.45 -0.14 -0.03 -0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 1.05 0.88 0.59 0.61 0.00 0.70 0.64 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.24 0.74 1.00 0.55 1 0.27 0.26 0.00 0.41 0.00 0.00 0.00 -0.00 -0.23 -0.33 -0.21 -0.20 -0.00 -0.02 -0.04 -0.82 -0.52 -0.02 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.36 -0.77 -0.71 -0.39 -1.00 -0.82 -0.67 -0.11 -0.74 -0.97 -0.31 3 0.00 0.00 0.00 0.00 0.00 0.01 0.00 -0.00 -0.15 -0.50 -0.38 -0.33 -0.20 -0.00 -0.39 -0.20 -0.12 -0.36 4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.46 -0.00 -0.84 -0.48 -0.36 -0.13 -0.08 -0.28 -0.33 0.13 5 0.00 0.02 0.00 0.00 0.12 0.33 0.00 -0.41 -0.00 -0.44 -0.33 -0.90 -0.56 -0.04 -0.24 -0.27 -0.48 -0.16 6 0.74 1.20 0.11 0.90 0.84 0.65 0.87 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.60 0.01 0.53 0.12 7 0.47 0.89 0.91 0.62 0.26 0.37 0.01 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.07 0.61 0.29 0.01 8 1.30 1.17 0.98 1.61 1.09 0.59 0.65 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.09 0.93 0.94 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 1 -1.00 2 -1.00 3 -1.00 4 -1.00 5 -1.00 6 -1.00 7 -1.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.45 -0.28 -0.30 -0.41 -0.17 -0.39 -0.22 -0.45 -0.28 -0.40 -0.18 -0.20 -0.16 -0.18 -0.10 -0.13 -0.14 -0.35 1 -0.09 -0.27 -0.09 -0.14 -0.02 -0.36 -0.21 -0.05 -0.05 -0.01 -0.02 -0.45 -0.03 -0.09 -0.01 -0.05 -0.39 -0.05 2 -0.17 -0.15 -0.37 -0.35 -0.32 -0.03 -0.24 -0.31 -0.35 -0.41 -0.00 -0.37 -0.18 -0.26 -0.09 -0.44 -0.09 -0.17 3 -0.42 -0.17 -0.11 -0.31 -0.32 -0.11 -0.20 -0.10 -0.34 -0.15 -0.24 -0.22 -0.22 -0.08 -0.40 -0.02 -0.23 -0.38 4 -0.13 -0.17 -0.06 -0.13 -0.32 -0.42 -0.28 -0.44 -0.03 -0.26 -0.38 -0.45 -0.08 -0.06 -0.04 -0.33 -0.27 -0.38 5 -0.32 -0.38 -0.19 -0.19 -0.33 -0.01 -0.15 -0.08 -0.31 -0.27 -0.07 -0.11 -0.21 -0.22 -0.18 -0.27 -0.19 -0.15 6 -0.30 -0.16 -0.09 -0.25 -0.23 -0.44 -0.25 -0.16 -0.05 -0.13 -0.20 -0.09 -0.14 -0.18 -0.15 -0.22 -0.37 -0.38 7 -0.20 -0.14 -0.12 -0.10 -0.42 -0.42 -0.14 -0.04 -0.44 -0.11 -0.10 -0.17 -0.06 -0.29 -0.22 -0.24 -0.01 -0.45 8 -0.31 -0.11 -0.16 -0.21 -0.16 -0.39 -0.12 -0.36 -0.36 -0.29 -0.24 -0.24 -0.20 -0.18 -0.33 -0.39 -0.20 -0.02 9 -0.41 -0.14 -0.12 -0.21 -0.01 -0.37 -0.03 -0.22 -0.38 -0.22 -0.09 -0.22 -0.19 -0.17 -0.13 -0.32 -0.30 -0.21 10 -0.31 -0.05 -0.02 -0.36 -0.04 -0.15 -0.03 -0.12 -0.36 -0.21 -0.40 -0.03 -0.04 -0.03 -0.23 -0.01 -0.02 -0.41 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.20 0.84 0.11 0.00 0.55 1.24 0.55 -0.00 -0.02 -0.00 -0.00 -0.00 -0.00 -0.00 -0.20 0.98 1.00 0.30 1 0.00 0.00 0.00 0.00 0.00 0.19 0.00 -0.14 -0.87 -0.50 -0.00 -0.34 -0.28 -0.53 -0.24 -0.34 0.23 -0.09 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.34 -0.82 -1.02 -0.75 -0.74 -0.56 -0.68 -0.71 -1.00 -0.65 -0.56 3 0.23 0.18 0.00 0.00 0.00 0.00 0.00 -0.00 -0.27 -0.00 -0.00 -0.21 -0.00 -0.28 -0.21 -0.24 0.02 0.00 4 0.09 0.00 0.00 0.00 0.00 0.00 0.00 -0.08 -0.00 -0.14 -0.00 -0.50 -0.01 -0.25 0.23 -0.20 -0.14 -0.66 5 0.18 0.49 0.00 0.00 0.03 0.00 0.00 -0.79 -0.36 -0.49 -0.39 -0.69 -0.00 -0.09 0.08 -0.84 0.10 -0.25 6 0.64 0.76 0.08 0.50 0.62 0.79 0.68 -0.00 -0.06 -0.00 -0.00 -0.00 -0.00 -0.00 0.28 0.24 0.86 0.87 7 0.32 0.24 0.23 0.18 0.76 0.62 0.28 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.13 0.73 0.09 0.87 8 1.23 0.50 0.27 0.51 1.08 2.00 0.60 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 1.00 1.00 1.00 <pre><code>x = Input(shape=(5, 7, 8))\n\nlayer = MonoDense(\n    units=12,\n    activation=activation,\n    monotonicity_indicator=[1]*3+[-1]*3+[0]*2,\n    is_convex=False,\n    is_concave=False,\n)\n\ny = layer(x)\n\nmodel = Model(inputs=x, outputs=y)\n\nmodel.summary()\n\ndisplay_kernel(layer.monotonicity_indicator)\n</code></pre> <pre><code>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 5, 7, 8)]         0\n\n mono_dense_5 (MonoDense)    (None, 5, 7, 12)          108\n\n=================================================================\nTotal params: 108\nTrainable params: 108\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> 0 0 1.00 1 1.00 2 1.00 3 -1.00 4 -1.00 5 -1.00 6 0.00 7 0.00"},{"location":"MonoDenseLayer/#mono-blocks","title":"Mono blocks","text":"<pre><code>x = Input(shape=(5, 7, 8))\n\n# monotonicity indicator must be broadcastable to input shape, so we use the vector of length 8\nmonotonicity_indicator = [1] * 3 + [0] * 2 + [-1] * 3\n\n# this mono block has 4 layers with the final one having the shape\nmono_block = _create_mono_block(\n    units=[16] * 3 + [3],\n    monotonicity_indicator=monotonicity_indicator,\n    activation=\"elu\",\n    dropout=0.1,\n)\ny = mono_block(x)\nmodel = Model(inputs=x, outputs=y)\nmodel.summary()\n\nmono_layers = [layer for layer in model.layers if isinstance(layer, MonoDense)]\nassert not (mono_layers[0].monotonicity_indicator == 1).all()\nfor mono_layer in mono_layers[1:]:\n    assert (mono_layer.monotonicity_indicator == 1).all()\n\nfor mono_layer in mono_layers[:-1]:\n    assert mono_layer.org_activation == \"elu\"\nassert mono_layers[-1].org_activation == None\n</code></pre> <pre><code>Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 5, 7, 8)]         0\n\n mono_dense_0 (MonoDense)    (None, 5, 7, 16)          144\n\n dropout (Dropout)           (None, 5, 7, 16)          0\n\n mono_dense_1_increasing (Mo  (None, 5, 7, 16)         272       \n noDense)\n\n dropout_1 (Dropout)         (None, 5, 7, 16)          0\n\n mono_dense_2_increasing (Mo  (None, 5, 7, 16)         272       \n noDense)\n\n dropout_2 (Dropout)         (None, 5, 7, 16)          0\n\n mono_dense_3_increasing (Mo  (None, 5, 7, 3)          51        \n noDense)\n\n=================================================================\nTotal params: 739\nTrainable params: 739\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <pre><code>inputs = Input(name=\"a\", shape=(1, ))\nparam = 0\n\nactual = _prepare_mono_input_n_param(inputs, param)\nexpected = [inputs], [0], [\"inputs\"]\nassert actual == expected, actual\n</code></pre> <pre><code>inputs = Input(name=\"a\", shape=(1, ))\nparam = {\"a\": 1}\n\nwith pytest.raises(ValueError) as e:\n    actual = _prepare_mono_input_n_param(inputs, param)\n\ne\n</code></pre> <pre><code>&lt;ExceptionInfo ValueError(\"Uncompatible types: type(inputs)=&lt;class 'keras.engine.keras_tensor.KerasTensor'&gt;, type(param)=&lt;class 'dict'&gt;\") tblen=2&gt;\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nactual = _prepare_mono_input_n_param({\"a\": a}, -1)\nassert actual == ([a], [-1], [\"a\"])\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nb = Input(name=\"b\", shape=(1,))\n\nactual = _prepare_mono_input_n_param({\"a\": a, \"b\": b}, {\"a\": -1, \"b\": 1})\nassert actual == ([a, b], [-1, 1], [\"a\", \"b\"])\n</code></pre> <pre><code>with pytest.raises(ValueError) as e:\n    actual = _prepare_mono_input_n_param(\n        {\"a\": Input(name=\"a\", shape=(1,)), \"b\": Input(name=\"b\", shape=(1,))}, {\"a\": -1}\n    )\ne\n</code></pre> <pre><code>&lt;ExceptionInfo ValueError(\"{'a'} != {'a', 'b'}\") tblen=2&gt;\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nb = Input(name=\"b\", shape=(1,))\n\nactual = _prepare_mono_input_n_param([a, b], [1, -1])\nassert actual == ([a, b], [1, -1], [\"0\", \"1\"])\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nb = Input(name=\"b\", shape=(1,))\n\nactual = _prepare_mono_input_n_param([a, b], -1)\nassert actual == ([a, b], [-1, -1], [\"0\", \"1\"])\n</code></pre> <pre><code>monotonicity_indicator = [-1, 0, 1]\nis_convex = [True] * 3\nis_concave = [False] * 3\nnames = list(\"abc\")\nhas_convex, has_concave = _check_convexity_params(\n    monotonicity_indicator, is_convex, is_concave, names\n)\nassert (has_convex, has_concave) == (True, False)\n</code></pre>"},{"location":"MonoDenseLayer/#type-1-architecture","title":"Type-1 architecture","text":""},{"location":"MonoDenseLayer/#create_type_1","title":"create_type_1","text":"<pre><code> create_type_1 (inputs:Union[tensorflow.python.types.core.Tensor,tensorflo\n                w.python.types.core.TensorProtocol,int,float,bool,str,byte\n                s,complex,tuple,list,numpy.ndarray,numpy.generic,Dict[str,\n                Union[tensorflow.python.types.core.Tensor,tensorflow.pytho\n                n.types.core.TensorProtocol,int,float,bool,str,bytes,compl\n                ex,tuple,list,numpy.ndarray,numpy.generic]],List[Union[ten\n                sorflow.python.types.core.Tensor,tensorflow.python.types.c\n                ore.TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                list,numpy.ndarray,numpy.generic]]], units:int,\n                final_units:int, activation:Union[str,Callable[[Union[tens\n                orflow.python.types.core.Tensor,tensorflow.python.types.co\n                re.TensorProtocol,int,float,bool,str,bytes,complex,tuple,l\n                ist,numpy.ndarray,numpy.generic]],Union[tensorflow.python.\n                types.core.Tensor,tensorflow.python.types.core.TensorProto\n                col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndar\n                ray,numpy.generic]]], n_layers:int, final_activation:Union\n                [str,Callable[[Union[tensorflow.python.types.core.Tensor,t\n                ensorflow.python.types.core.TensorProtocol,int,float,bool,\n                str,bytes,complex,tuple,list,numpy.ndarray,numpy.generic]]\n                ,Union[tensorflow.python.types.core.Tensor,tensorflow.pyth\n                on.types.core.TensorProtocol,int,float,bool,str,bytes,comp\n                lex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=Non\n                e, monotonicity_indicator:Union[int,Dict[str,int],List[int\n                ]]=1,\n                is_convex:Union[bool,Dict[str,bool],List[bool]]=False,\n                is_concave:Union[bool,Dict[str,bool],List[bool]]=False,\n                dropout:Optional[float]=None)\n</code></pre> <p>Builds Type-1 monotonic network</p> <p>Args: inputs: input tensor or a dictionary of tensors units: number of units in hidden layers final_units: number of units in the output layer activation: the base activation function n_layers: total number of layers (hidden layers plus the output layer) final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used. monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator. is_convex: set to True if a particular input feature is convex is_concave: set to True if a particular inputs feature is concave dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <p>Returns: Output tensor</p> <pre><code>n_layers = 4\n\ninputs = {name: Input(name=name, shape=(1, )) for name in list(\"abcd\")}\noutputs = create_type_1(\n    inputs=inputs,\n    units=64,\n    final_units=10,\n    activation=\"elu\",\n    n_layers=n_layers,\n    final_activation=\"softmax\",\n    monotonicity_indicator=dict(a=1, b=0, c=-1, d=0),\n    is_convex=True,\n    dropout=0.1,\n)\n\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.summary()\n\nmono_layers = [layer for layer in model.layers if isinstance(layer, MonoDense)]\nassert len(mono_layers) == n_layers\n\n# check monotonicity indicator\nnp.testing.assert_array_equal(\n    mono_layers[0].monotonicity_indicator, np.array([1, 0, -1, 0]).reshape((-1, 1))\n)\nfor i in range(1, n_layers):\n    assert mono_layers[i].monotonicity_indicator == 1\n\n# check convexity and concavity\nfor i in range(n_layers):\n    assert mono_layers[i].is_convex\n    assert not mono_layers[i].is_concave\n</code></pre> <pre><code>Model: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n a (InputLayer)                 [(None, 1)]          0           []\n\n b (InputLayer)                 [(None, 1)]          0           []\n\n c (InputLayer)                 [(None, 1)]          0           []\n\n d (InputLayer)                 [(None, 1)]          0           []\n\n concatenate (Concatenate)      (None, 4)            0           ['a[0][0]',                      \n                                                                  'b[0][0]',                      \n                                                                  'c[0][0]',                      \n                                                                  'd[0][0]']\n\n mono_dense_0_convex (MonoDense  (None, 64)          320         ['concatenate[0][0]']            \n )\n\n dropout_3 (Dropout)            (None, 64)           0           ['mono_dense_0_convex[0][0]']\n\n mono_dense_1_increasing_convex  (None, 64)          4160        ['dropout_3[0][0]']              \n  (MonoDense)\n\n dropout_4 (Dropout)            (None, 64)           0           ['mono_dense_1_increasing_convex[\n                                                                 0][0]']\n\n mono_dense_2_increasing_convex  (None, 64)          4160        ['dropout_4[0][0]']              \n  (MonoDense)\n\n dropout_5 (Dropout)            (None, 64)           0           ['mono_dense_2_increasing_convex[\n                                                                 0][0]']\n\n mono_dense_3_increasing_convex  (None, 10)          650         ['dropout_5[0][0]']              \n  (MonoDense)\n\n tf.nn.softmax (TFOpLambda)     (None, 10)           0           ['mono_dense_3_increasing_convex[\n                                                                 0][0]']\n\n==================================================================================================\nTotal params: 9,290\nTrainable params: 9,290\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre>"},{"location":"MonoDenseLayer/#type-2-architecture","title":"Type-2 architecture","text":""},{"location":"MonoDenseLayer/#create_type_2","title":"create_type_2","text":"<pre><code> create_type_2 (inputs:Union[tensorflow.python.types.core.Tensor,tensorflo\n                w.python.types.core.TensorProtocol,int,float,bool,str,byte\n                s,complex,tuple,list,numpy.ndarray,numpy.generic,Dict[str,\n                Union[tensorflow.python.types.core.Tensor,tensorflow.pytho\n                n.types.core.TensorProtocol,int,float,bool,str,bytes,compl\n                ex,tuple,list,numpy.ndarray,numpy.generic]],List[Union[ten\n                sorflow.python.types.core.Tensor,tensorflow.python.types.c\n                ore.TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                list,numpy.ndarray,numpy.generic]]],\n                mono_units:Optional[int]=None,\n                non_mono_units:Optional[List[int]]=None, units:int,\n                final_units:int, activation:Union[str,Callable[[Union[tens\n                orflow.python.types.core.Tensor,tensorflow.python.types.co\n                re.TensorProtocol,int,float,bool,str,bytes,complex,tuple,l\n                ist,numpy.ndarray,numpy.generic]],Union[tensorflow.python.\n                types.core.Tensor,tensorflow.python.types.core.TensorProto\n                col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndar\n                ray,numpy.generic]]], n_layers:int, final_activation:Union\n                [str,Callable[[Union[tensorflow.python.types.core.Tensor,t\n                ensorflow.python.types.core.TensorProtocol,int,float,bool,\n                str,bytes,complex,tuple,list,numpy.ndarray,numpy.generic]]\n                ,Union[tensorflow.python.types.core.Tensor,tensorflow.pyth\n                on.types.core.TensorProtocol,int,float,bool,str,bytes,comp\n                lex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=Non\n                e, monotonicity_indicator:Union[int,Dict[str,int]]=1,\n                is_convex:Union[bool,Dict[str,bool],List[bool]]=False,\n                is_concave:Union[bool,Dict[str,bool],List[bool]]=False,\n                dropout:Optional[float]=None)\n</code></pre> <p>Builds Type-2 monotonic network</p> <p>Args: inputs: input tensor or a dictionary of tensors mono_units: used to preprocess monotonic features before entering the common mono block non_mono_units: fully-connected network used to preprocess non-monotonic features before entering the common mono block units: number of units in hidden layers final_units: number of units in the output layer activation: the base activation function n_layers: total number of layers (hidden layers plus the output layer) final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used. monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator. is_convex: set to True if a particular input feature is convex is_concave: set to True if a particular inputs feature is concave dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <p>Returns: Output tensor</p> <pre><code>for dropout in [False, True]:\n    print(\"*\" * 120)\n    print()\n    print(f\"{dropout=}\")\n    print()\n    inputs = {name: Input(name=name, shape=(1, )) for name in list(\"abcd\")}\n    outputs = create_type_2(\n        inputs,\n        units=32,\n        final_units=10,\n        activation=\"elu\",\n        n_layers=4,\n        dropout=dropout,\n        monotonicity_indicator=dict(a=1, b=0, c=-1, d=0),\n        is_convex=dict(a=True, b=False, c=False, d=False),\n        is_concave=False,\n    )\n    model = Model(inputs=inputs, outputs=outputs)\n    model.summary()\n</code></pre> <pre><code>************************************************************************************************************************\n\ndropout=False\n\nModel: \"model_3\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n a (InputLayer)                 [(None, 1)]          0           []\n\n c (InputLayer)                 [(None, 1)]          0           []\n\n b (InputLayer)                 [(None, 1)]          0           []\n\n d (InputLayer)                 [(None, 1)]          0           []\n\n mono_dense_a_increasing_convex  (None, 8)           16          ['a[0][0]']                      \n  (MonoDense)\n\n mono_dense_c_decreasing (MonoD  (None, 8)           16          ['c[0][0]']                      \n ense)\n\n concat_non_mono (Concatenate)  (None, 2)            0           ['b[0][0]',                      \n                                                                  'd[0][0]']\n\n concat_mono (Concatenate)      (None, 16)           0           ['mono_dense_a_increasing_convex[\n                                                                 0][0]',                          \n                                                                  'mono_dense_c_decreasing[0][0]']\n\n non_mono_dense_0 (Dense)       (None, 32)           96          ['concat_non_mono[0][0]']\n\n concat_preprocess (Concatenate  (None, 48)          0           ['concat_mono[0][0]',            \n )                                                                'non_mono_dense_0[0][0]']\n\n mono_dense_0_convex (MonoDense  (None, 32)          1568        ['concat_preprocess[0][0]']      \n )\n\n mono_dense_1_increasing_convex  (None, 32)          1056        ['mono_dense_0_convex[0][0]']    \n  (MonoDense)\n\n mono_dense_2_increasing_convex  (None, 32)          1056        ['mono_dense_1_increasing_convex[\n  (MonoDense)                                                    0][0]']\n\n mono_dense_3_increasing_convex  (None, 10)          330         ['mono_dense_2_increasing_convex[\n  (MonoDense)                                                    0][0]']\n\n==================================================================================================\nTotal params: 4,138\nTrainable params: 4,138\nNon-trainable params: 0\n__________________________________________________________________________________________________\n************************************************************************************************************************\n\ndropout=True\n\nModel: \"model_4\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n a (InputLayer)                 [(None, 1)]          0           []\n\n c (InputLayer)                 [(None, 1)]          0           []\n\n mono_dense_a_increasing_convex  (None, 8)           16          ['a[0][0]']                      \n  (MonoDense)\n\n mono_dense_c_decreasing (MonoD  (None, 8)           16          ['c[0][0]']                      \n ense)\n\n b (InputLayer)                 [(None, 1)]          0           []\n\n d (InputLayer)                 [(None, 1)]          0           []\n\n concat_mono (Concatenate)      (None, 16)           0           ['mono_dense_a_increasing_convex[\n                                                                 0][0]',                          \n                                                                  'mono_dense_c_decreasing[0][0]']\n\n concat_non_mono (Concatenate)  (None, 2)            0           ['b[0][0]',                      \n                                                                  'd[0][0]']\n\n dropout_6 (Dropout)            (None, 16)           0           ['concat_mono[0][0]']\n\n non_mono_dense_0 (Dense)       (None, 32)           96          ['concat_non_mono[0][0]']\n\n concat_preprocess (Concatenate  (None, 48)          0           ['dropout_6[0][0]',              \n )                                                                'non_mono_dense_0[0][0]']\n\n mono_dense_0_convex (MonoDense  (None, 32)          1568        ['concat_preprocess[0][0]']      \n )\n\n dropout_8 (Dropout)            (None, 32)           0           ['mono_dense_0_convex[0][0]']\n\n mono_dense_1_increasing_convex  (None, 32)          1056        ['dropout_8[0][0]']              \n  (MonoDense)\n\n dropout_9 (Dropout)            (None, 32)           0           ['mono_dense_1_increasing_convex[\n                                                                 0][0]']\n\n mono_dense_2_increasing_convex  (None, 32)          1056        ['dropout_9[0][0]']              \n  (MonoDense)\n\n dropout_10 (Dropout)           (None, 32)           0           ['mono_dense_2_increasing_convex[\n                                                                 0][0]']\n\n mono_dense_3_increasing_convex  (None, 10)          330         ['dropout_10[0][0]']             \n  (MonoDense)\n\n==================================================================================================\nTotal params: 4,138\nTrainable params: 4,138\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Monotonic Dense Layer</li> <li>Experiments<ul> <li>Experiments</li> <li>Auto MPG</li> <li>Heart disease</li> </ul> </li> <li>API<ul> <li>mono_dense_keras<ul> <li>MonoDense</li> <li>create_type_1</li> <li>create_type_2</li> <li>experiments<ul> <li>DownloadProgressBar</li> <li>TestHyperModel</li> <li>build_mono_model_f</li> <li>count_model_params</li> <li>create_model_stats</li> <li>create_tuner_stats</li> <li>df2ds</li> <li>download_data</li> <li>download_url</li> <li>find_hyperparameters</li> <li>get_build_model_with_hp_f</li> <li>get_data_path</li> <li>get_train_n_test_data</li> <li>peek</li> <li>sanitize_col_names</li> </ul> </li> <li>helpers<ul> <li>export</li> </ul> </li> <li>replace_kernel_using_monotonicity_indicator</li> </ul> </li> </ul> </li> <li>Releases</li> </ul>"},{"location":"TopLevel/","title":"TopLevel","text":""},{"location":"TopLevel/#dummy","title":"dummy","text":"<pre><code> dummy ()\n</code></pre>"},{"location":"changelog_not_found/","title":"Releases","text":""},{"location":"changelog_not_found/#changelogmd-file-not-found","title":"CHANGELOG.md file not found.","text":"<p>To generate the changelog file, please run the following command from the project root directory. </p> <pre><code>nbdev_changelog\n</code></pre> <p>If you do not want this page to be rendered as part of the documentation, please remove the following line from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- [Releases]{changelog}\n</code></pre>"},{"location":"cli_commands_not_found/","title":"No CLI commands found in console_scripts in settings.ini file.","text":"<p>For documenting CLI commands, please add command line executables in <code>console_scripts</code> in <code>settings.ini</code> file. </p> <p>If you do not want this page to be rendered as part of the documentation, please remove the following lines from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- CLI\n{cli}\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/","title":"MonoDense","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense","title":"<code>mono_dense_keras.MonoDense</code>","text":"<p>         Bases: <code>Dense</code></p> <p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> <p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is  -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>@export\nclass MonoDense(Dense):\n\"\"\"Monotonic counterpart of the regular Dense Layer of tf.keras\n\n    This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.\n\n    In the code, the variable `monotonicity_indicator` corresponds to **t** in the figure and the variable `activation_selector` corresponds to **s**.\n\n    Parameters `convexity_indicator` and `epsilon` are used to calculate `activation_selector` as follows:\n    - if `convexity_indicator` is  -1 or 1, then `activation_selector` will have all elements 0 or 1, respecively.\n    - if `convexity_indicator` is `None`, then `epsilon` must have a value between 0 and 1 and corresponds to the percentage of elements of `activation_selector` set to 1.\n\n    ![mono-dense-layer-diagram.png](images/mono-dense-layer-diagram.png)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        units: int,\n        *,\n        activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n        monotonicity_indicator: ArrayLike = 1,\n        is_convex: bool = False,\n        is_concave: bool = False,\n        activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0),\n        **kwargs: Dict[str, Any],\n    ):\n\"\"\"Constructs a new MonoDense instance.\n\n        Params:\n            units: Positive integer, dimensionality of the output space.\n            activation: Activation function to use, it is assumed to be convex monotonically\n                increasing function such as \"relu\" or \"elu\"\n            monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n                monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n                -1 for monotonically decreasing and 0 for non-monotonic.\n            is_convex: convex if set to True\n            is_concave: concave if set to True\n            activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n                Ignored if is_convex or is_concave is set to True\n            **kwargs: passed as kwargs to the constructor of `Dense`\n\n        Raise:\n            ValueError:\n                - if both **is_concave** and **is_convex** are set to **True**, or\n                - if any component of activation_weights is negative or there is not exactly three components\n        \"\"\"\n        if is_convex and is_concave:\n            raise ValueError(\n                \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n            )\n\n        if len(activation_weights) != 3:\n            raise ValueError(\n                f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n            )\n\n        if (np.array(activation_weights) &lt; 0).any():\n            raise ValueError(\n                f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n            )\n\n        super(MonoDense, self).__init__(units=units, activation=None, **kwargs)\n\n        self.units = units\n        self.org_activation = activation\n        self.activation_weights = activation_weights\n        self.monotonicity_indicator = monotonicity_indicator\n        self.is_convex = is_convex\n        self.is_concave = is_concave\n\n        (\n            self.convex_activation,\n            self.concave_activation,\n            self.saturated_activation,\n        ) = get_activation_functions(self.org_activation)\n\n    def build(\n        self, input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]\n    ) -&gt; None:\n\"\"\"Build\n\n        Args:\n            input_shape: input tensor\n            args: positional arguments passed to Dense.build()\n            kwargs: keyword arguments passed to Dense.build()\n        \"\"\"\n        super(MonoDense, self).build(input_shape, *args, **kwargs)\n        self.monotonicity_indicator = get_monotonicity_indicator(\n            monotonicity_indicator=self.monotonicity_indicator,\n            input_shape=input_shape,\n            units=self.units,\n        )\n\n    def call(self, inputs: TensorLike) -&gt; TensorLike:\n\"\"\"Call\n\n        Args:\n            inputs: input tensor of shape (batch_size, ..., x_length)\n\n        Returns:\n            N-D tensor with shape: `(batch_size, ..., units)`.\n\n        \"\"\"\n        # calculate W'*x+y after we replace the kernal according to monotonicity vector\n        with replace_kernel_using_monotonicity_indicator(\n            self, monotonicity_indicator=self.monotonicity_indicator\n        ):\n            h = super(MonoDense, self).call(inputs)\n\n        y = apply_activations(\n            h,\n            units=self.units,\n            convex_activation=self.convex_activation,\n            concave_activation=self.concave_activation,\n            saturated_activation=self.saturated_activation,\n            is_convex=self.is_convex,\n            is_concave=self.is_concave,\n            activation_weights=self.activation_weights,\n        )\n\n        return y\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense-attributes","title":"Attributes","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.activation_weights","title":"<code>activation_weights = activation_weights</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.is_concave","title":"<code>is_concave = is_concave</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.is_convex","title":"<code>is_convex = is_convex</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.monotonicity_indicator","title":"<code>monotonicity_indicator = monotonicity_indicator</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.org_activation","title":"<code>org_activation = activation</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.units","title":"<code>units = units</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense-functions","title":"Functions","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.__init__","title":"<code>__init__(units: int, *, activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None, monotonicity_indicator: ArrayLike = 1, is_convex: bool = False, is_concave: bool = False, activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0), **kwargs: Dict[str, Any])</code>","text":"<p>Constructs a new MonoDense instance.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Positive integer, dimensionality of the output space.</p> required <code>activation</code> <code>Optional[Union[str, Callable[[TensorLike], TensorLike]]]</code> <p>Activation function to use, it is assumed to be convex monotonically increasing function such as \"relu\" or \"elu\"</p> <code>None</code> <code>monotonicity_indicator</code> <code>ArrayLike</code> <p>Vector to indicate which of the inputs are monotonically increasing or monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing, -1 for monotonically decreasing and 0 for non-monotonic.</p> <code>1</code> <code>is_convex</code> <code>bool</code> <p>convex if set to True</p> <code>False</code> <code>is_concave</code> <code>bool</code> <p>concave if set to True</p> <code>False</code> <code>activation_weights</code> <code>Tuple[float, float, float]</code> <p>relative weights for each type of activation, the default is (1.0, 1.0, 1.0). Ignored if is_convex or is_concave is set to True</p> <code>(7.0, 7.0, 2.0)</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>passed as kwargs to the constructor of <code>Dense</code></p> <code>{}</code> Raise <p>ValueError:     - if both is_concave and is_convex are set to True, or     - if any component of activation_weights is negative or there is not exactly three components</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def __init__(\n    self,\n    units: int,\n    *,\n    activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    monotonicity_indicator: ArrayLike = 1,\n    is_convex: bool = False,\n    is_concave: bool = False,\n    activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0),\n    **kwargs: Dict[str, Any],\n):\n\"\"\"Constructs a new MonoDense instance.\n\n    Params:\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use, it is assumed to be convex monotonically\n            increasing function such as \"relu\" or \"elu\"\n        monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n            monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n            -1 for monotonically decreasing and 0 for non-monotonic.\n        is_convex: convex if set to True\n        is_concave: concave if set to True\n        activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n            Ignored if is_convex or is_concave is set to True\n        **kwargs: passed as kwargs to the constructor of `Dense`\n\n    Raise:\n        ValueError:\n            - if both **is_concave** and **is_convex** are set to **True**, or\n            - if any component of activation_weights is negative or there is not exactly three components\n    \"\"\"\n    if is_convex and is_concave:\n        raise ValueError(\n            \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n        )\n\n    if len(activation_weights) != 3:\n        raise ValueError(\n            f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n        )\n\n    if (np.array(activation_weights) &lt; 0).any():\n        raise ValueError(\n            f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n        )\n\n    super(MonoDense, self).__init__(units=units, activation=None, **kwargs)\n\n    self.units = units\n    self.org_activation = activation\n    self.activation_weights = activation_weights\n    self.monotonicity_indicator = monotonicity_indicator\n    self.is_convex = is_convex\n    self.is_concave = is_concave\n\n    (\n        self.convex_activation,\n        self.concave_activation,\n        self.saturated_activation,\n    ) = get_activation_functions(self.org_activation)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_loss","title":"<code>keras.engine.base_layer.Layer.add_loss(losses, **kwargs)</code>","text":"<p>Add loss tensor(s), potentially dependent on layer inputs.</p> <p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p> <p>This method can be used inside a subclassed layer or model's <code>call</code> function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p> <p>Example:</p> <pre><code>class MyLayer(tf.keras.layers.Layer):\n  def call(self, inputs):\n    self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n    return inputs\n</code></pre> <p>The same code works in distributed training: the input to <code>add_loss()</code> is treated like a regularization loss and averaged across replicas by the training loop (both built-in <code>Model.fit()</code> and compliant custom training loops).</p> <p>The <code>add_loss</code> method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's <code>Input</code>s. These losses become part of the model's topology and are tracked in <code>get_config</code>.</p> <p>Example:</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\n# Activity regularization.\nmodel.add_loss(tf.abs(tf.reduce_mean(x)))\n</code></pre> <p>If this is not the case for your loss (if, for example, your loss references a <code>Variable</code> of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized.</p> <p>Example:</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nd = tf.keras.layers.Dense(10)\nx = d(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\n# Weight regularization.\nmodel.add_loss(lambda: tf.reduce_mean(d.kernel))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>losses</code> <p>Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor.</p> required <code>**kwargs</code> <p>Used for backwards compatibility only.</p> <code>{}</code> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def add_loss(self, losses, **kwargs):\n\"\"\"Add loss tensor(s), potentially dependent on layer inputs.\n\n    Some losses (for instance, activity regularization losses) may be\n    dependent on the inputs passed when calling a layer. Hence, when reusing\n    the same layer on different inputs `a` and `b`, some entries in\n    `layer.losses` may be dependent on `a` and some on `b`. This method\n    automatically keeps track of dependencies.\n\n    This method can be used inside a subclassed layer or model's `call`\n    function, in which case `losses` should be a Tensor or list of Tensors.\n\n    Example:\n\n    ```python\n    class MyLayer(tf.keras.layers.Layer):\n      def call(self, inputs):\n        self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n        return inputs\n    ```\n\n    The same code works in distributed training: the input to `add_loss()`\n    is treated like a regularization loss and averaged across replicas\n    by the training loop (both built-in `Model.fit()` and compliant custom\n    training loops).\n\n    The `add_loss` method can also be called directly on a Functional Model\n    during construction. In this case, any loss Tensors passed to this Model\n    must be symbolic and be able to be traced back to the model's `Input`s.\n    These losses become part of the model's topology and are tracked in\n    `get_config`.\n\n    Example:\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    # Activity regularization.\n    model.add_loss(tf.abs(tf.reduce_mean(x)))\n    ```\n\n    If this is not the case for your loss (if, for example, your loss\n    references a `Variable` of one of the model's layers), you can wrap your\n    loss in a zero-argument lambda. These losses are not tracked as part of\n    the model's topology since they can't be serialized.\n\n    Example:\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    d = tf.keras.layers.Dense(10)\n    x = d(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    # Weight regularization.\n    model.add_loss(lambda: tf.reduce_mean(d.kernel))\n    ```\n\n    Args:\n      losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n        losses may also be zero-argument callables which create a loss\n        tensor.\n      **kwargs: Used for backwards compatibility only.\n    \"\"\"\n    kwargs.pop(\"inputs\", None)\n    if kwargs:\n        raise TypeError(f\"Unknown keyword arguments: {kwargs.keys()}\")\n\n    def _tag_callable(loss):\n\"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\"\n        if callable(loss):\n            # We run the loss without autocasting, as regularizers are often\n            # numerically unstable in float16.\n            with autocast_variable.enable_auto_cast_variables(None):\n                loss = loss()\n        if loss is None:\n            # Will be filtered out when computing the .losses property\n            return None\n        if not tf.is_tensor(loss):\n            loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n        loss._unconditional_loss = True\n        return loss\n\n    losses = tf.nest.flatten(losses)\n\n    callable_losses = []\n    eager_losses = []\n    symbolic_losses = []\n    for loss in losses:\n        if callable(loss):\n            callable_losses.append(functools.partial(_tag_callable, loss))\n            continue\n        if loss is None:\n            continue\n        if not tf.is_tensor(loss) and not isinstance(\n            loss, keras_tensor.KerasTensor\n        ):\n            loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n        # TF Functions should take the eager path.\n        if (\n            tf_utils.is_symbolic_tensor(loss)\n            or isinstance(loss, keras_tensor.KerasTensor)\n        ) and not base_layer_utils.is_in_tf_function():\n            symbolic_losses.append(loss)\n        elif tf.is_tensor(loss):\n            eager_losses.append(loss)\n\n    self._callable_losses.extend(callable_losses)\n\n    in_call_context = base_layer_utils.call_context().in_call\n    if eager_losses and not in_call_context:\n        raise ValueError(\n            \"Expected a symbolic Tensors or a callable for the loss value. \"\n            \"Please wrap your loss computation in a zero argument `lambda`.\"\n        )\n\n    self._eager_losses.extend(eager_losses)\n\n    for symbolic_loss in symbolic_losses:\n        if getattr(self, \"_is_graph_network\", False):\n            self._graph_network_add_loss(symbolic_loss)\n        else:\n            # Possible a loss was added in a Layer's `build`.\n            self._losses.append(symbolic_loss)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_metric","title":"<code>keras.engine.base_layer.Layer.add_metric(value, name = None, **kwargs)</code>","text":"<p>Adds metric tensor to the layer.</p> <p>This method can be used inside the <code>call()</code> method of a subclassed layer or model.</p> <pre><code>class MyMetricLayer(tf.keras.layers.Layer):\n  def __init__(self):\n    super(MyMetricLayer, self).__init__(name='my_metric_layer')\n    self.mean = tf.keras.metrics.Mean(name='metric_1')\n\n  def call(self, inputs):\n    self.add_metric(self.mean(inputs))\n    self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n    return inputs\n</code></pre> <p>This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's <code>Input</code>s. These metrics become part of the model's topology and are tracked when you save the model via <code>save()</code>.</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.add_metric(math_ops.reduce_sum(x), name='metric_1')\n</code></pre> <p>Note: Calling <code>add_metric()</code> with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs.</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>Metric tensor.</p> required <code>name</code> <p>String metric name.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for backward compatibility. Accepted values: <code>aggregation</code> - When the <code>value</code> tensor provided is not the result of calling a <code>keras.Metric</code> instance, it will be aggregated by default using a <code>keras.Metric.Mean</code>.</p> <code>{}</code> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def add_metric(self, value, name=None, **kwargs):\n\"\"\"Adds metric tensor to the layer.\n\n    This method can be used inside the `call()` method of a subclassed layer\n    or model.\n\n    ```python\n    class MyMetricLayer(tf.keras.layers.Layer):\n      def __init__(self):\n        super(MyMetricLayer, self).__init__(name='my_metric_layer')\n        self.mean = tf.keras.metrics.Mean(name='metric_1')\n\n      def call(self, inputs):\n        self.add_metric(self.mean(inputs))\n        self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n        return inputs\n    ```\n\n    This method can also be called directly on a Functional Model during\n    construction. In this case, any tensor passed to this Model must\n    be symbolic and be able to be traced back to the model's `Input`s. These\n    metrics become part of the model's topology and are tracked when you\n    save the model via `save()`.\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n    ```\n\n    Note: Calling `add_metric()` with the result of a metric object on a\n    Functional Model, as shown in the example below, is not supported. This\n    is because we cannot trace the metric result tensor back to the model's\n    inputs.\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n    ```\n\n    Args:\n      value: Metric tensor.\n      name: String metric name.\n      **kwargs: Additional keyword arguments for backward compatibility.\n        Accepted values:\n        `aggregation` - When the `value` tensor provided is not the result\n        of calling a `keras.Metric` instance, it will be aggregated by\n        default using a `keras.Metric.Mean`.\n    \"\"\"\n    kwargs_keys = list(kwargs.keys())\n    if len(kwargs_keys) &gt; 1 or (\n        len(kwargs_keys) == 1 and kwargs_keys[0] != \"aggregation\"\n    ):\n        raise TypeError(\n            f\"Unknown keyword arguments: {kwargs.keys()}. \"\n            \"Expected `aggregation`.\"\n        )\n\n    from_metric_obj = hasattr(value, \"_metric_obj\")\n    is_symbolic = isinstance(value, keras_tensor.KerasTensor)\n    in_call_context = base_layer_utils.call_context().in_call\n\n    if name is None and not from_metric_obj:\n        # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we\n        # use metric name to lookup a metric. Without a name, a new Mean\n        # metric wrapper will be created on every model/layer call. So, we\n        # raise an error when no name is provided. We will do the same for\n        # symbolic mode for consistency although a name will be generated if\n        # no name is provided.\n\n        # We will not raise this error in the foll use case for the sake of\n        # consistency as name in provided in the metric constructor.\n        # mean = metrics.Mean(name='my_metric')\n        # model.add_metric(mean(outputs))\n        raise ValueError(\n            \"Please provide a name for your metric like \"\n            \"`self.add_metric(tf.reduce_sum(inputs), \"\n            \"name='mean_activation')`\"\n        )\n    elif from_metric_obj:\n        name = value._metric_obj.name\n\n    if not in_call_context and not is_symbolic:\n        raise ValueError(\n            \"Expected a symbolic Tensor for the metric value, received: \"\n            + str(value)\n        )\n\n    # If a metric was added in a Layer's `call` or `build`.\n    if in_call_context or not getattr(self, \"_is_graph_network\", False):\n        # TF Function path should take the eager path.\n\n        # If the given metric is available in `metrics` list we just update\n        # state on it, otherwise we create a new metric instance and\n        # add it to the `metrics` list.\n        metric_obj = getattr(value, \"_metric_obj\", None)\n        # Tensors that come from a Metric object already updated the Metric\n        # state.\n        should_update_state = not metric_obj\n        name = metric_obj.name if metric_obj else name\n\n        with self._metrics_lock:\n            match = self._get_existing_metric(name)\n            if match:\n                metric_obj = match\n            elif metric_obj:\n                self._metrics.append(metric_obj)\n            else:\n                # Build the metric object with the value's dtype if it\n                # defines one\n                metric_obj = metrics_mod.Mean(\n                    name=name, dtype=getattr(value, \"dtype\", None)\n                )\n                self._metrics.append(metric_obj)\n\n        if should_update_state:\n            metric_obj(value)\n    else:\n        if from_metric_obj:\n            raise ValueError(\n                \"Using the result of calling a `Metric` object \"\n                \"when calling `add_metric` on a Functional \"\n                \"Model is not supported. Please pass the \"\n                \"Tensor to monitor directly.\"\n            )\n\n        # Insert layers into the Keras Graph Network.\n        aggregation = None if from_metric_obj else \"mean\"\n        self._graph_network_add_metric(value, aggregation, name)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_update","title":"<code>keras.engine.base_layer.Layer.add_update(updates)</code>","text":"<p>Add update op(s), potentially dependent on layer inputs.</p> <p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p> <p>This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution).</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <p>Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting <code>trainable=False</code> on this Layer, when executing in Eager mode.</p> required Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef add_update(self, updates):\n\"\"\"Add update op(s), potentially dependent on layer inputs.\n\n    Weight updates (for instance, the updates of the moving mean and\n    variance in a BatchNormalization layer) may be dependent on the inputs\n    passed when calling a layer. Hence, when reusing the same layer on\n    different inputs `a` and `b`, some entries in `layer.updates` may be\n    dependent on `a` and some on `b`. This method automatically keeps track\n    of dependencies.\n\n    This call is ignored when eager execution is enabled (in that case,\n    variable updates are run on the fly and thus do not need to be tracked\n    for later execution).\n\n    Args:\n      updates: Update op, or list/tuple of update ops, or zero-arg callable\n        that returns an update op. A zero-arg callable should be passed in\n        order to disable running the updates by setting `trainable=False`\n        on this Layer, when executing in Eager mode.\n    \"\"\"\n    call_context = base_layer_utils.call_context()\n    # No need to run updates during Functional API construction.\n    if call_context.in_keras_graph:\n        return\n\n    # Callable updates are disabled by setting `trainable=False`.\n    if not call_context.frozen:\n        for update in tf.nest.flatten(updates):\n            if callable(update):\n                update()\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_variable","title":"<code>keras.engine.base_layer.Layer.add_variable(*args, **kwargs)</code>","text":"<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef add_variable(self, *args, **kwargs):\n\"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\n    warnings.warn(\n        \"`layer.add_variable` is deprecated and \"\n        \"will be removed in a future version. \"\n        \"Please use the `layer.add_weight()` method instead.\",\n        stacklevel=2,\n    )\n    return self.add_weight(*args, **kwargs)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_weight","title":"<code>keras.engine.base_layer.Layer.add_weight(name = None, shape = None, dtype = None, initializer = None, regularizer = None, trainable = None, constraint = None, use_resource = None, synchronization = tf.VariableSynchronization.AUTO, aggregation = tf.VariableAggregation.NONE, **kwargs)</code>","text":"<p>Adds a new variable to the layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Variable name.</p> <code>None</code> <code>shape</code> <p>Variable shape. Defaults to scalar if unspecified.</p> <code>None</code> <code>dtype</code> <p>The type of the variable. Defaults to <code>self.dtype</code>.</p> <code>None</code> <code>initializer</code> <p>Initializer instance (callable).</p> <code>None</code> <code>regularizer</code> <p>Regularizer instance (callable).</p> <code>None</code> <code>trainable</code> <p>Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code> is set to <code>ON_READ</code>.</p> <code>None</code> <code>constraint</code> <p>Constraint instance (callable).</p> <code>None</code> <code>use_resource</code> <p>Whether to use a <code>ResourceVariable</code> or not. See this guide  for more information.</p> <code>None</code> <code>synchronization</code> <p>Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class <code>tf.VariableSynchronization</code>. By default the synchronization is set to <code>AUTO</code> and the current <code>DistributionStrategy</code> chooses when to synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>, <code>trainable</code> must not be set to <code>True</code>.</p> <code>tf.VariableSynchronization.AUTO</code> <code>aggregation</code> <p>Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class <code>tf.VariableAggregation</code>.</p> <code>tf.VariableAggregation.NONE</code> <code>**kwargs</code> <p>Additional keyword arguments. Accepted values are <code>getter</code>, <code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The variable created.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as <code>ON_READ</code>.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.for_subclass_implementers\ndef add_weight(\n    self,\n    name=None,\n    shape=None,\n    dtype=None,\n    initializer=None,\n    regularizer=None,\n    trainable=None,\n    constraint=None,\n    use_resource=None,\n    synchronization=tf.VariableSynchronization.AUTO,\n    aggregation=tf.VariableAggregation.NONE,\n    **kwargs,\n):\n\"\"\"Adds a new variable to the layer.\n\n    Args:\n      name: Variable name.\n      shape: Variable shape. Defaults to scalar if unspecified.\n      dtype: The type of the variable. Defaults to `self.dtype`.\n      initializer: Initializer instance (callable).\n      regularizer: Regularizer instance (callable).\n      trainable: Boolean, whether the variable should be part of the layer's\n        \"trainable_variables\" (e.g. variables, biases)\n        or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n        Note that `trainable` cannot be `True` if `synchronization`\n        is set to `ON_READ`.\n      constraint: Constraint instance (callable).\n      use_resource: Whether to use a `ResourceVariable` or not.\n        See [this guide](\n        https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n         for more information.\n      synchronization: Indicates when a distributed a variable will be\n        aggregated. Accepted values are constants defined in the class\n        `tf.VariableSynchronization`. By default the synchronization is set\n        to `AUTO` and the current `DistributionStrategy` chooses when to\n        synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n        must not be set to `True`.\n      aggregation: Indicates how a distributed variable will be aggregated.\n        Accepted values are constants defined in the class\n        `tf.VariableAggregation`.\n      **kwargs: Additional keyword arguments. Accepted values are `getter`,\n        `collections`, `experimental_autocast` and `caching_device`.\n\n    Returns:\n      The variable created.\n\n    Raises:\n      ValueError: When giving unsupported dtype and no initializer or when\n        trainable has been set to True with synchronization set as\n        `ON_READ`.\n    \"\"\"\n    if shape is None:\n        shape = ()\n    kwargs.pop(\"partitioner\", None)  # Ignored.\n    # Validate optional keyword arguments.\n    for kwarg in kwargs:\n        if kwarg not in [\n            \"collections\",\n            \"experimental_autocast\",\n            \"caching_device\",\n            \"getter\",\n            \"layout\",\n        ]:\n            raise TypeError(\"Unknown keyword argument:\", kwarg)\n    collections_arg = kwargs.pop(\"collections\", None)\n    # 'experimental_autocast' can be set to False by the caller to indicate\n    # an AutoCastVariable should never be created.\n    autocast = kwargs.pop(\"experimental_autocast\", True)\n    # See the docstring for tf.Variable about the details for\n    # caching_device.\n    caching_device = kwargs.pop(\"caching_device\", None)\n\n    layout = kwargs.pop(\"layout\", None)\n    # Specially handling of auto layout fetch, based on the variable name\n    # and attribute name. For built-in keras layers, usually the variable\n    # name, eg 'kernel', will match with a 'kernel_layout' attribute name on\n    # the instance. We will try to do this auto fetch if layout is not\n    # explicitly specified. This is mainly a quick workaround for not\n    # applying too many interface change to built-in layers, until DTensor\n    # is a public API.  Also see dtensor.utils.allow_initializer_layout for\n    # more details.\n    # TODO(scottzhu): Remove this once dtensor is public to end user.\n    if not layout and name:\n        layout = getattr(self, name + \"_layout\", None)\n\n    if dtype is None:\n        dtype = self.dtype or backend.floatx()\n    dtype = tf.as_dtype(dtype)\n    if self._dtype_policy.variable_dtype is None:\n        # The policy is \"_infer\", so we infer the policy from the variable\n        # dtype.\n        self._set_dtype_policy(policy.Policy(dtype.base_dtype.name))\n    initializer = initializers.get(initializer)\n    regularizer = regularizers.get(regularizer)\n    constraint = constraints.get(constraint)\n\n    if synchronization == tf.VariableSynchronization.ON_READ:\n        if trainable:\n            raise ValueError(\n                \"Synchronization value can be set to \"\n                \"VariableSynchronization.ON_READ only for non-trainable \"\n                \"variables. You have specified trainable=True and \"\n                \"synchronization=VariableSynchronization.ON_READ.\"\n            )\n        else:\n            # Set trainable to be false when variable is to be synced on\n            # read.\n            trainable = False\n    elif trainable is None:\n        trainable = True\n\n    # Initialize variable when no initializer provided\n    if initializer is None:\n        # If dtype is DT_FLOAT, provide a uniform unit scaling initializer\n        if dtype.is_floating:\n            initializer = initializers.get(\"glorot_uniform\")\n        # If dtype is DT_INT/DT_UINT, provide a default value `zero`\n        # If dtype is DT_BOOL, provide a default value `FALSE`\n        elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n            initializer = initializers.get(\"zeros\")\n        # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX\n        # here?\n        elif \"getter\" not in kwargs:\n            # When `getter` is specified, it's possibly fine for\n            # `initializer` to be None since it's up to the custom `getter`\n            # to raise error in case it indeed needs `initializer`.\n            raise ValueError(\n                f\"An initializer for variable {name} of type \"\n                f\"{dtype.base_dtype} is required for layer \"\n                f\"{self.name}. Received: {initializer}.\"\n            )\n\n    getter = kwargs.pop(\"getter\", base_layer_utils.make_variable)\n    if (\n        autocast\n        and self._dtype_policy.compute_dtype\n        != self._dtype_policy.variable_dtype\n        and dtype.is_floating\n    ):\n        old_getter = getter\n\n        # Wrap variable constructor to return an AutoCastVariable.\n        def getter(*args, **kwargs):\n            variable = old_getter(*args, **kwargs)\n            return autocast_variable.create_autocast_variable(variable)\n\n        # Also the caching_device does not work with the mixed precision\n        # API, disable it if it is specified.\n        # TODO(b/142020079): Re-enable it once the bug is fixed.\n        if caching_device is not None:\n            tf_logging.warning(\n                \"`caching_device` does not work with mixed precision API. \"\n                \"Ignoring user specified `caching_device`.\"\n            )\n            caching_device = None\n    if layout:\n        getter = functools.partial(getter, layout=layout)\n\n    variable = self._add_variable_with_custom_getter(\n        name=name,\n        shape=shape,\n        # TODO(allenl): a `make_variable` equivalent should be added as a\n        # `Trackable` method.\n        getter=getter,\n        # Manage errors in Layer rather than Trackable.\n        overwrite=True,\n        initializer=initializer,\n        dtype=dtype,\n        constraint=constraint,\n        trainable=trainable,\n        use_resource=use_resource,\n        collections=collections_arg,\n        synchronization=synchronization,\n        aggregation=aggregation,\n        caching_device=caching_device,\n    )\n    if regularizer is not None:\n        # TODO(fchollet): in the future, this should be handled at the\n        # level of variable creation, and weight regularization losses\n        # should be variable attributes.\n        name_in_scope = variable.name[: variable.name.find(\":\")]\n        self._handle_weight_regularization(\n            name_in_scope, variable, regularizer\n        )\n    if base_layer_utils.is_split_variable(variable):\n        for v in variable:\n            backend.track_variable(v)\n            if trainable:\n                self._trainable_weights.append(v)\n            else:\n                self._non_trainable_weights.append(v)\n    else:\n        backend.track_variable(variable)\n        if trainable:\n            self._trainable_weights.append(variable)\n        else:\n            self._non_trainable_weights.append(variable)\n    return variable\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.build","title":"<code>mono_dense_keras._components.mono_dense_layer.MonoDense.build(input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; None</code>","text":"<p>Build</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple</code> <p>input tensor</p> required <code>args</code> <code>List[Any]</code> <p>positional arguments passed to Dense.build()</p> <code>()</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>keyword arguments passed to Dense.build()</p> <code>{}</code> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def build(\n    self, input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]\n) -&gt; None:\n\"\"\"Build\n\n    Args:\n        input_shape: input tensor\n        args: positional arguments passed to Dense.build()\n        kwargs: keyword arguments passed to Dense.build()\n    \"\"\"\n    super(MonoDense, self).build(input_shape, *args, **kwargs)\n    self.monotonicity_indicator = get_monotonicity_indicator(\n        monotonicity_indicator=self.monotonicity_indicator,\n        input_shape=input_shape,\n        units=self.units,\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.call","title":"<code>mono_dense_keras._components.mono_dense_layer.MonoDense.call(inputs: TensorLike) -&gt; TensorLike</code>","text":"<p>Call</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>TensorLike</code> <p>input tensor of shape (batch_size, ..., x_length)</p> required <p>Returns:</p> Type Description <code>TensorLike</code> <p>N-D tensor with shape: <code>(batch_size, ..., units)</code>.</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def call(self, inputs: TensorLike) -&gt; TensorLike:\n\"\"\"Call\n\n    Args:\n        inputs: input tensor of shape (batch_size, ..., x_length)\n\n    Returns:\n        N-D tensor with shape: `(batch_size, ..., units)`.\n\n    \"\"\"\n    # calculate W'*x+y after we replace the kernal according to monotonicity vector\n    with replace_kernel_using_monotonicity_indicator(\n        self, monotonicity_indicator=self.monotonicity_indicator\n    ):\n        h = super(MonoDense, self).call(inputs)\n\n    y = apply_activations(\n        h,\n        units=self.units,\n        convex_activation=self.convex_activation,\n        concave_activation=self.concave_activation,\n        saturated_activation=self.saturated_activation,\n        is_convex=self.is_convex,\n        is_concave=self.is_concave,\n        activation_weights=self.activation_weights,\n    )\n\n    return y\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.compute_mask","title":"<code>keras.engine.base_layer.Layer.compute_mask(inputs, mask = None)</code>","text":"<p>Computes an output mask tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Tensor or list of tensors.</p> required <code>mask</code> <p>Tensor or list of tensors.</p> <code>None</code> <p>Returns:</p> Type Description <p>None or a tensor (or list of tensors, one per output tensor of the layer).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@generic_utils.default\ndef compute_mask(self, inputs, mask=None):\n\"\"\"Computes an output mask tensor.\n\n    Args:\n        inputs: Tensor or list of tensors.\n        mask: Tensor or list of tensors.\n\n    Returns:\n        None or a tensor (or list of tensors,\n            one per output tensor of the layer).\n    \"\"\"\n    if not self._supports_masking:\n        if any(m is not None for m in tf.nest.flatten(mask)):\n            raise TypeError(\n                \"Layer \" + self.name + \" does not support masking, \"\n                \"but was passed an input_mask: \" + str(mask)\n            )\n        # masking not explicitly supported: return None as mask.\n        return None\n    # if masking is explicitly supported, by default\n    # carry over the input mask\n    return mask\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.compute_output_signature","title":"<code>keras.engine.base_layer.Layer.compute_output_signature(input_signature)</code>","text":"<p>Compute the output tensor signature of the layer based on the inputs.</p> <p>Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use <code>compute_output_shape</code>, and will assume that the output dtype matches the input dtype.</p> <p>Parameters:</p> Name Type Description Default <code>input_signature</code> <p>Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer.</p> required <p>Returns:</p> Type Description <p>Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_signature contains a non-TensorSpec object.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.for_subclass_implementers\ndef compute_output_signature(self, input_signature):\n\"\"\"Compute the output tensor signature of the layer based on the inputs.\n\n    Unlike a TensorShape object, a TensorSpec object contains both shape\n    and dtype information for a tensor. This method allows layers to provide\n    output dtype information if it is different from the input dtype.\n    For any layer that doesn't implement this function,\n    the framework will fall back to use `compute_output_shape`, and will\n    assume that the output dtype matches the input dtype.\n\n    Args:\n      input_signature: Single TensorSpec or nested structure of TensorSpec\n        objects, describing a candidate input for the layer.\n\n    Returns:\n      Single TensorSpec or nested structure of TensorSpec objects,\n        describing how the layer would transform the provided input.\n\n    Raises:\n      TypeError: If input_signature contains a non-TensorSpec object.\n    \"\"\"\n\n    def check_type_return_shape(s):\n        if not isinstance(s, tf.TensorSpec):\n            raise TypeError(\n                \"Only TensorSpec signature types are supported. \"\n                f\"Received: {s}.\"\n            )\n        return s.shape\n\n    input_shape = tf.nest.map_structure(\n        check_type_return_shape, input_signature\n    )\n    output_shape = self.compute_output_shape(input_shape)\n    dtype = self._compute_dtype\n    if dtype is None:\n        input_dtypes = [s.dtype for s in tf.nest.flatten(input_signature)]\n        # Default behavior when self.dtype is None, is to use the first\n        # input's dtype.\n        dtype = input_dtypes[0]\n    return tf.nest.map_structure(\n        lambda s: tf.TensorSpec(dtype=dtype, shape=s), output_shape\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.count_params","title":"<code>keras.engine.base_layer.Layer.count_params()</code>","text":"<p>Count the total number of scalars composing the weights.</p> <p>Returns:</p> Type Description <p>An integer count.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the layer isn't yet built (in which case its weights aren't yet defined).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def count_params(self):\n\"\"\"Count the total number of scalars composing the weights.\n\n    Returns:\n        An integer count.\n\n    Raises:\n        ValueError: if the layer isn't yet built\n          (in which case its weights aren't yet defined).\n    \"\"\"\n    if not self.built:\n        if getattr(self, \"_is_graph_network\", False):\n            with tf_utils.maybe_init_scope(self):\n                self._maybe_build(self.inputs)\n        else:\n            raise ValueError(\n                \"You tried to call `count_params` \"\n                f\"on layer {self.name}\"\n                \", but the layer isn't built. \"\n                \"You can build it manually via: \"\n                f\"`{self.name}.build(batch_input_shape)`.\"\n            )\n    return layer_utils.count_params(self.weights)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.finalize_state","title":"<code>keras.engine.base_layer.Layer.finalize_state()</code>","text":"<p>Finalizes the layers state after updating layer weights.</p> <p>This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update.</p> <p>This function will be called after weights of a layer have been restored from a loaded model.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef finalize_state(self):\n\"\"\"Finalizes the layers state after updating layer weights.\n\n    This function can be subclassed in a layer and will be called after\n    updating a layer weights. It can be overridden to finalize any\n    additional layer state after a weight update.\n\n    This function will be called after weights of a layer have been restored\n    from a loaded model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_at","title":"<code>keras.engine.base_layer.Layer.get_input_at(node_index)</code>","text":"<p>Retrieves the input tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first input node of the layer.</p> required <p>Returns:</p> Type Description <p>A tensor (or list of tensors if the layer has multiple inputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_at(self, node_index):\n\"\"\"Retrieves the input tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first input node of the layer.\n\n    Returns:\n        A tensor (or list of tensors if the layer has multiple inputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"input_tensors\", \"input\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_mask_at","title":"<code>keras.engine.base_layer.Layer.get_input_mask_at(node_index)</code>","text":"<p>Retrieves the input mask tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A mask tensor</p> <p>(or list of tensors if the layer has multiple inputs).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_mask_at(self, node_index):\n\"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A mask tensor\n        (or list of tensors if the layer has multiple inputs).\n    \"\"\"\n    inputs = self.get_input_at(node_index)\n    if isinstance(inputs, list):\n        return [getattr(x, \"_keras_mask\", None) for x in inputs]\n    else:\n        return getattr(inputs, \"_keras_mask\", None)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_shape_at","title":"<code>keras.engine.base_layer.Layer.get_input_shape_at(node_index)</code>","text":"<p>Retrieves the input shape(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A shape tuple</p> <p>(or list of shape tuples if the layer has multiple inputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_shape_at(self, node_index):\n\"\"\"Retrieves the input shape(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A shape tuple\n        (or list of shape tuples if the layer has multiple inputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"input_shapes\", \"input shape\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_at","title":"<code>keras.engine.base_layer.Layer.get_output_at(node_index)</code>","text":"<p>Retrieves the output tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first output node of the layer.</p> required <p>Returns:</p> Type Description <p>A tensor (or list of tensors if the layer has multiple outputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_at(self, node_index):\n\"\"\"Retrieves the output tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first output node of the layer.\n\n    Returns:\n        A tensor (or list of tensors if the layer has multiple outputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"output_tensors\", \"output\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_mask_at","title":"<code>keras.engine.base_layer.Layer.get_output_mask_at(node_index)</code>","text":"<p>Retrieves the output mask tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A mask tensor</p> <p>(or list of tensors if the layer has multiple outputs).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_mask_at(self, node_index):\n\"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A mask tensor\n        (or list of tensors if the layer has multiple outputs).\n    \"\"\"\n    output = self.get_output_at(node_index)\n    if isinstance(output, list):\n        return [getattr(x, \"_keras_mask\", None) for x in output]\n    else:\n        return getattr(output, \"_keras_mask\", None)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_shape_at","title":"<code>keras.engine.base_layer.Layer.get_output_shape_at(node_index)</code>","text":"<p>Retrieves the output shape(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A shape tuple</p> <p>(or list of shape tuples if the layer has multiple outputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_shape_at(self, node_index):\n\"\"\"Retrieves the output shape(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A shape tuple\n        (or list of shape tuples if the layer has multiple outputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"output_shapes\", \"output shape\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_weights","title":"<code>keras.engine.base_layer.Layer.get_weights()</code>","text":"<p>Returns the current weights of the layer, as NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers.</p> <p>For example, a <code>Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code>Dense</code> layer:</p> <p>layer_a = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.],        [2.],        [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)]</p> <p>Returns:</p> Type Description <p>Weights values as a list of NumPy arrays.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def get_weights(self):\n\"\"\"Returns the current weights of the layer, as NumPy arrays.\n\n    The weights of a layer represent the state of the layer. This function\n    returns both trainable and non-trainable weight values associated with\n    this layer as a list of NumPy arrays, which can in turn be used to load\n    state into similarly parameterized layers.\n\n    For example, a `Dense` layer returns a list of two values: the kernel\n    matrix and the bias vector. These can be used to set the weights of\n    another `Dense` layer:\n\n    &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(1.))\n    &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n    &gt;&gt;&gt; layer_a.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(2.))\n    &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[2.],\n           [2.],\n           [2.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n\n    Returns:\n        Weights values as a list of NumPy arrays.\n    \"\"\"\n    weights = self.weights\n    output_weights = []\n    for weight in weights:\n        if isinstance(weight, base_layer_utils.TrackableWeightHandler):\n            output_weights.extend(weight.get_tensors())\n        else:\n            output_weights.append(weight)\n    return backend.batch_get_value(output_weights)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.set_weights","title":"<code>keras.engine.base_layer.Layer.set_weights(weights)</code>","text":"<p>Sets the weights of the layer, from NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer.</p> <p>For example, a <code>Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code>Dense</code> layer:</p> <p>layer_a = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.],        [2.],        [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)]</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <p>a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of <code>get_weights</code>).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided weights list does not match the layer's specifications.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def set_weights(self, weights):\n\"\"\"Sets the weights of the layer, from NumPy arrays.\n\n    The weights of a layer represent the state of the layer. This function\n    sets the weight values from numpy arrays. The weight values should be\n    passed in the order they are created by the layer. Note that the layer's\n    weights must be instantiated before calling this function, by calling\n    the layer.\n\n    For example, a `Dense` layer returns a list of two values: the kernel\n    matrix and the bias vector. These can be used to set the weights of\n    another `Dense` layer:\n\n    &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(1.))\n    &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n    &gt;&gt;&gt; layer_a.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(2.))\n    &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[2.],\n           [2.],\n           [2.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n\n    Args:\n      weights: a list of NumPy arrays. The number\n        of arrays and their shape must match\n        number of the dimensions of the weights\n        of the layer (i.e. it should match the\n        output of `get_weights`).\n\n    Raises:\n      ValueError: If the provided weights list does not match the\n        layer's specifications.\n    \"\"\"\n    params = self.weights\n\n    expected_num_weights = 0\n    for param in params:\n        if isinstance(param, base_layer_utils.TrackableWeightHandler):\n            expected_num_weights += param.num_tensors\n        else:\n            expected_num_weights += 1\n\n    if expected_num_weights != len(weights):\n        raise ValueError(\n            'You called `set_weights(weights)` on layer \"%s\" '\n            \"with a weight list of length %s, but the layer was \"\n            \"expecting %s weights. Provided weights: %s...\"\n            % (\n                self.name,\n                len(weights),\n                expected_num_weights,\n                str(weights)[:50],\n            )\n        )\n\n    weight_index = 0\n    weight_value_tuples = []\n    for param in params:\n        if isinstance(param, base_layer_utils.TrackableWeightHandler):\n            num_tensors = param.num_tensors\n            tensors = weights[weight_index : weight_index + num_tensors]\n            param.set_weights(tensors)\n            weight_index += num_tensors\n        else:\n            weight = weights[weight_index]\n            weight_shape = weight.shape if hasattr(weight, \"shape\") else ()\n            ref_shape = param.shape\n            if not ref_shape.is_compatible_with(weight_shape):\n                raise ValueError(\n                    f\"Layer {self.name} weight shape {ref_shape} \"\n                    \"is not compatible with provided weight \"\n                    f\"shape {weight_shape}.\"\n                )\n            weight_value_tuples.append((param, weight))\n            weight_index += 1\n\n    backend.batch_set_value(weight_value_tuples)\n\n    # Perform any layer defined finalization of the layer state.\n    for layer in self._flatten_layers():\n        layer.finalize_state()\n</code></pre>"},{"location":"api/mono_dense_keras/create_type_1/","title":"create_type_1","text":""},{"location":"api/mono_dense_keras/create_type_1/#mono_dense_keras.create_type_1","title":"<code>mono_dense_keras.create_type_1(inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]], *, units: int, final_units: int, activation: Union[str, Callable[[TensorLike], TensorLike]], n_layers: int, final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None, monotonicity_indicator: Union[int, Dict[str, int], List[int]] = 1, is_convex: Union[bool, Dict[str, bool], List[bool]] = False, is_concave: Union[bool, Dict[str, bool], List[bool]] = False, dropout: Optional[float] = None) -&gt; TensorLike</code>","text":"<p>Builds Type-1 monotonic network</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[TensorLike, Dict[str, TensorLike], List[TensorLike]]</code> <p>input tensor or a dictionary of tensors</p> required <code>units</code> <code>int</code> <p>number of units in hidden layers</p> required <code>final_units</code> <code>int</code> <p>number of units in the output layer</p> required <code>activation</code> <code>Union[str, Callable[[TensorLike], TensorLike]]</code> <p>the base activation function</p> required <code>n_layers</code> <code>int</code> <p>total number of layers (hidden layers plus the output layer)</p> required <code>final_activation</code> <code>Optional[Union[str, Callable[[TensorLike], TensorLike]]]</code> <p>the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used.</p> <code>None</code> <code>monotonicity_indicator</code> <code>Union[int, Dict[str, int], List[int]]</code> <p>if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator.</p> <code>1</code> <code>is_convex</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular input feature is convex</p> <code>False</code> <code>is_concave</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular inputs feature is concave</p> <code>False</code> <code>dropout</code> <code>Optional[float]</code> <p>dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorLike</code> <p>Output tensor</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>@export\ndef create_type_1(\n    inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]],\n    *,\n    units: int,\n    final_units: int,\n    activation: Union[str, Callable[[TensorLike], TensorLike]],\n    n_layers: int,\n    final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    monotonicity_indicator: Union[int, Dict[str, int], List[int]] = 1,\n    is_convex: Union[bool, Dict[str, bool], List[bool]] = False,\n    is_concave: Union[bool, Dict[str, bool], List[bool]] = False,\n    dropout: Optional[float] = None,\n) -&gt; TensorLike:\n\"\"\"Builds Type-1 monotonic network\n\n    Args:\n        inputs: input tensor or a dictionary of tensors\n        units: number of units in hidden layers\n        final_units: number of units in the output layer\n        activation: the base activation function\n        n_layers: total number of layers (hidden layers plus the output layer)\n        final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear).\n            If set to None (default value), then the linear activation is used.\n        monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity\n            indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int,\n            then all input features are set to the same monotinicity indicator.\n        is_convex: set to True if a particular input feature is convex\n        is_concave: set to True if a particular inputs feature is concave\n        dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.\n\n    Returns:\n        Output tensor\n\n    \"\"\"\n    _, is_convex, _ = _prepare_mono_input_n_param(inputs, is_convex)\n    _, is_concave, _ = _prepare_mono_input_n_param(inputs, is_concave)\n    x, monotonicity_indicator, names = _prepare_mono_input_n_param(\n        inputs, monotonicity_indicator\n    )\n    has_convex, has_concave = _check_convexity_params(\n        monotonicity_indicator, is_convex, is_concave, names\n    )\n\n    y = tf.keras.layers.Concatenate()(x)\n\n    y = _create_mono_block(\n        units=[units] * (n_layers - 1) + [final_units],\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator,\n        is_convex=has_convex,\n        is_concave=has_concave and not has_convex,\n        dropout=dropout,\n    )(y)\n\n    if final_activation is not None:\n        final_activation = tf.keras.activations.get(final_activation)\n        y = final_activation(y)\n\n    return y\n</code></pre>"},{"location":"api/mono_dense_keras/create_type_2/","title":"create_type_2","text":""},{"location":"api/mono_dense_keras/create_type_2/#mono_dense_keras.create_type_2","title":"<code>mono_dense_keras.create_type_2(inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]], *, mono_units: Optional[int] = None, non_mono_units: Optional[List[int]] = None, units: int, final_units: int, activation: Union[str, Callable[[TensorLike], TensorLike]], n_layers: int, final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None, monotonicity_indicator: Union[int, Dict[str, int]] = 1, is_convex: Union[bool, Dict[str, bool], List[bool]] = False, is_concave: Union[bool, Dict[str, bool], List[bool]] = False, dropout: Optional[float] = None) -&gt; TensorLike</code>","text":"<p>Builds Type-2 monotonic network</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[TensorLike, Dict[str, TensorLike], List[TensorLike]]</code> <p>input tensor or a dictionary of tensors</p> required <code>mono_units</code> <code>Optional[int]</code> <p>used to preprocess monotonic features before entering the common mono block</p> <code>None</code> <code>non_mono_units</code> <code>Optional[List[int]]</code> <p>fully-connected network used to preprocess non-monotonic features before entering the common mono block</p> <code>None</code> <code>units</code> <code>int</code> <p>number of units in hidden layers</p> required <code>final_units</code> <code>int</code> <p>number of units in the output layer</p> required <code>activation</code> <code>Union[str, Callable[[TensorLike], TensorLike]]</code> <p>the base activation function</p> required <code>n_layers</code> <code>int</code> <p>total number of layers (hidden layers plus the output layer)</p> required <code>final_activation</code> <code>Optional[Union[str, Callable[[TensorLike], TensorLike]]]</code> <p>the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used.</p> <code>None</code> <code>monotonicity_indicator</code> <code>Union[int, Dict[str, int]]</code> <p>if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator.</p> <code>1</code> <code>is_convex</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular input feature is convex</p> <code>False</code> <code>is_concave</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular inputs feature is concave</p> <code>False</code> <code>dropout</code> <code>Optional[float]</code> <p>dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorLike</code> <p>Output tensor</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>@export\ndef create_type_2(\n    inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]],\n    *,\n    mono_units: Optional[int] = None,\n    non_mono_units: Optional[List[int]] = None,\n    units: int,\n    final_units: int,\n    activation: Union[str, Callable[[TensorLike], TensorLike]],\n    n_layers: int,\n    final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    monotonicity_indicator: Union[int, Dict[str, int]] = 1,\n    is_convex: Union[bool, Dict[str, bool], List[bool]] = False,\n    is_concave: Union[bool, Dict[str, bool], List[bool]] = False,\n    dropout: Optional[float] = None,\n) -&gt; TensorLike:\n\"\"\"Builds Type-2 monotonic network\n\n    Args:\n        inputs: input tensor or a dictionary of tensors\n        mono_units: used to preprocess monotonic features before entering the common mono block\n        non_mono_units: fully-connected network used to preprocess non-monotonic features before entering the common mono block\n        units: number of units in hidden layers\n        final_units: number of units in the output layer\n        activation: the base activation function\n        n_layers: total number of layers (hidden layers plus the output layer)\n        final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear).\n            If set to None (default value), then the linear activation is used.\n        monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity\n            indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int,\n            then all input features are set to the same monotinicity indicator.\n        is_convex: set to True if a particular input feature is convex\n        is_concave: set to True if a particular inputs feature is concave\n        dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.\n\n    Returns:\n        Output tensor\n\n    \"\"\"\n    _, is_convex, _ = _prepare_mono_input_n_param(inputs, is_convex)\n    _, is_concave, _ = _prepare_mono_input_n_param(inputs, is_concave)\n    x, monotonicity_indicator, names = _prepare_mono_input_n_param(\n        inputs, monotonicity_indicator\n    )\n    has_convex, has_concave = _check_convexity_params(\n        monotonicity_indicator, is_convex, is_concave, names\n    )\n\n    if mono_units is None:\n        mono_units = max(units // 4, 1)\n\n    if non_mono_units is None:\n        non_mono_units = [units]\n\n    y_mono = [\n        (\n            MonoDense(\n                units=mono_units,\n                activation=activation,\n                monotonicity_indicator=monotonicity_indicator[i],\n                is_convex=is_convex[i],\n                is_concave=is_concave[i],\n                name=f\"mono_dense_{names[i]}\"\n                + (\"_increasing\" if monotonicity_indicator[i] == 1 else \"_decreasing\")\n                + (\"_convex\" if is_convex[i] else \"\")\n                + (\"_concave\" if is_concave[i] else \"\"),\n            )\n        )(x[i])\n        for i in range(len(inputs))\n        if monotonicity_indicator[i] != 0\n    ]\n\n    y_mono_len = len(y_mono)\n\n    y_mono = Concatenate(name=\"concat_mono\")(y_mono)\n    if dropout and dropout &gt; 0.0:\n        y_mono = Dropout(dropout)(y_mono)\n\n    y_non_mono = [x[i] for i in range(len(inputs)) if monotonicity_indicator[i] == 0]\n    if len(non_mono_units) == 0:\n        y_non_mono_len = len(y_non_mono)\n    else:\n        y_non_mono_len = non_mono_units[-1]\n    y_non_mono = Concatenate(name=\"concat_non_mono\")(y_non_mono)\n\n    for i in range(len(non_mono_units)):\n        y_non_mono = Dense(\n            units=non_mono_units[i], activation=activation, name=f\"non_mono_dense_{i}\"\n        )(y_non_mono)\n        if dropout and dropout &gt; 0.0:\n            y = Dropout(dropout)(y_non_mono)\n\n    y = Concatenate(name=\"concat_preprocess\")([y_mono, y_non_mono])\n    monotonicity_indicator_block = [1] * (mono_units * y_mono_len) + [\n        0\n    ] * y_non_mono_len\n\n    y = _create_mono_block(\n        units=[units] * (n_layers - 1) + [final_units],\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator_block,\n        is_convex=has_convex,\n        is_concave=has_concave and not has_convex,\n        dropout=dropout,\n    )(y)\n\n    if final_activation is not None:\n        final_activation = tf.keras.activations.get(final_activation)\n        y = final_activation(y)\n\n    return y\n</code></pre>"},{"location":"experiments/AutoMPG/","title":"Auto MPG","text":"<p>The Auto MPG Dataset is a regression dataset [1] with 7 features - Cylinders, Displacement, Horsepower,Weight, Acceleration, Model Year, Origin. And the dependant variable is monotonically decreasing with respect to features weigh, displacement, and horsepower. The <code>monotonicity_indicator</code> corrsponding to these features are set to -1, since the relationship is a monotonically decreasing one with respect to the dependant variable.</p> <p>This is a part of comparison with methods and datasets from COMET [2] (Reference #20 in our paper).</p> <p>References:</p> <ol> <li> <p>Quinlan,R. (1993). Combining Instance-Based and Model-Based     Learning. In Proceedings on the Tenth International Conference of     Machine Learning, 236-243, University of Massachusetts, Amherst.     Morgan Kaufmann.</p> <p>https://archive.ics.uci.edu/ml/datasets/auto+mpg</p> </li> <li> <p>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</p> </li> </ol> <pre><code>Github repo: https://github.com/AishwaryaSivaraman/COMET\n</code></pre> <pre><code>from mono_dense_keras.experiments import get_train_n_test_data, find_hyperparameters, create_tuner_stats\n</code></pre> <pre><code>from os import environ\n</code></pre> <p>These are a few examples of the dataset:</p> 0 1 2 3 4 Cylinders 1.482807 1.482807 1.482807 1.482807 1.482807 Displacement 1.073028 1.482902 1.044432 1.025368 2.235927 Horsepower 0.650564 1.548993 1.163952 0.907258 2.396084 Weight 0.606625 0.828131 0.523413 0.542165 1.587581 Acceleration -1.275546 -1.452517 -1.275546 -1.806460 -1.983431 Model_Year -1.631803 -1.631803 -1.631803 -1.631803 -1.631803 Origin -0.701669 -0.701669 -0.701669 -0.701669 -0.701669 ground_truth 18.000000 15.000000 16.000000 17.000000 15.000000 <pre><code>tuner = find_hyperparameters(\n    \"auto\",\n    monotonicity_indicator={\n        \"Cylinders\": 0,\n        \"Displacement\": -1,\n        \"Horsepower\": -1,\n        \"Weight\": -1,\n        \"Acceleration\": 0,\n        \"Model_Year\": 0,\n        \"Origin\": 0,\n    },\n    max_trials=200,\n    final_activation=None,\n    loss=\"mse\",\n    metrics=\"mse\",\n    objective=\"val_mse\",\n)\n</code></pre> <pre><code>Trial 116 Complete [00h 00m 20s]\nval_mse: 10.398303349812826\n\nBest val_mse So Far: 8.577057838439941\nTotal elapsed time: 00h 07m 41s\n\nSearch: Running Trial #117\n\nValue             |Best Value So Far |Hyperparameter\n21                |19                |units\n1                 |4                 |n_layers\nelu               |elu               |activation\n0.3               |0.011234          |learning_rate\n0.16568           |0.1               |weight_decay\n0.5               |0.18594           |dropout\n1                 |1                 |decay_rate\n\nEpoch 1/50\n34/40 [========================&gt;.....] - ETA: 0s - loss: 72.2397 - mse: 72.2397\n</code></pre> 0 1 2 3 4 units 26 12 19 32 8 n_layers 2 4 4 4 4 activation elu elu elu elu elu learning_rate 0.086301 0.067798 0.011234 0.019907 0.040900 weight_decay 0.147297 0.228063 0.100000 0.159397 0.100000 dropout 0.162063 0.000000 0.185937 0.199401 0.000000 decay_rate 0.927282 0.955851 1.000000 1.000000 0.500000 val_mse_mean 8.599928 8.634456 8.674133 8.766920 8.768065 val_mse_std 0.115006 0.098686 0.144630 0.157671 0.152409 val_mse_min 8.454746 8.513648 8.518104 8.514189 8.603386 val_mse_max 8.747616 8.730823 8.877400 8.888693 8.928326 params 1363 667 1507 4177 325"},{"location":"experiments/Heart/","title":"Heart disease","text":"<p>Heart Disease [1] is a classification dataset used for predicting the presence of heart disease with 13 features (age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal) and monotonically increasing with respect to features- trestbps and cholestrol (chol). The <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li>John H. Gennari, Pat Langley, and Douglas H. Fisher. Models of     incremental concept formation. Artif. Intell., 40(1-3):11\u201361, 1989.</li> </ol> <p>https://archive.ics.uci.edu/ml/datasets/heart+disease</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>These are a few examples of the dataset:</p> 0 1 2 3 4 age 0.972778 1.415074 1.415074 -1.902148 -1.459852 sex 0.649445 0.649445 0.649445 0.649445 -1.533413 cp -2.020077 0.884034 0.884034 -0.084003 -1.052040 trestbps 0.721008 1.543527 -0.649858 -0.101512 -0.101512 chol -0.251855 0.740555 -0.326754 0.066465 -0.794872 fbs 2.426901 -0.410346 -0.410346 -0.410346 -0.410346 restecg 1.070838 1.070838 1.070838 -0.953715 1.070838 thalach -0.025055 -1.831151 -0.928103 1.566030 0.920995 exang -0.721010 1.381212 1.381212 -0.721010 -0.721010 oldpeak 0.986440 0.330395 1.232457 1.970508 0.248389 slope 2.334348 0.687374 0.687374 2.334348 -0.959601 ca -0.770198 2.425024 1.359950 -0.770198 -0.770198 thal -2.070238 -0.514345 1.041548 -0.514345 -0.514345 ground_truth 0.000000 1.000000 0.000000 0.000000 0.000000 <pre><code>tuner = find_hyperparameters(\n    \"heart\",\n    monotonicity_indicator = {\n        \"age\": 0,\n        \"sex\": 0,\n        \"cp\": 0,\n        \"trestbps\": 1,\n        \"chol\": 1,\n        \"fbs\": 0,\n        \"restecg\": 0,\n        \"thalach\": 0,\n        \"exang\": 0,\n        \"oldpeak\": 0,\n        \"slope\": 0,\n        \"ca\": 0,\n        \"thal\": 0,\n    },\n    max_trials=100,\n    final_activation=\"sigmoid\",\n    loss = \"binary_crossentropy\",\n    metrics = \"accuracy\",\n    objective=\"val_accuracy\",\n)\n</code></pre> <pre><code>Trial 100 Complete [00h 00m 24s]\nval_accuracy: 0.8633879621823629\n\nBest val_accuracy So Far: 0.8797814249992371\nTotal elapsed time: 00h 36m 01s\nINFO:tensorflow:Oracle triggered exit\n</code></pre> 0 1 2 3 4 units 16 16 17 26 32 n_layers 3 3 2 2 2 activation elu elu elu elu elu learning_rate 0.001000 0.001000 0.001112 0.001000 0.001000 weight_decay 0.205112 0.205682 0.169679 0.144370 0.114788 dropout 0.500000 0.500000 0.500000 0.500000 0.446662 decay_rate 1.000000 1.000000 1.000000 0.994300 0.925633 val_accuracy_mean 0.881967 0.881967 0.881967 0.878689 0.875410 val_accuracy_std 0.007331 0.007331 0.013716 0.008979 0.008979 val_accuracy_min 0.868852 0.868852 0.868852 0.868852 0.868852 val_accuracy_max 0.885246 0.885246 0.901639 0.885246 0.885246 params 897 897 680 1377 2017"}]}