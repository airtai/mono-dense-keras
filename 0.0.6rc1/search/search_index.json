{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Monotonic Dense Layer","text":"<p>This Python library implements Monotonic Dense Layer as described in Davor Runje, Sharath M. Shankaranarayana, \u201cConstrained Monotonic Neural Networks\u201d, https://https://arxiv.org/abs/2205.11775.</p> <p>If you use this library, please cite:</p> <pre><code>@misc{https://doi.org/10.48550/arxiv.2205.11775,\n  doi = {10.48550/ARXIV.2205.11775},\n  url = {https://arxiv.org/abs/2205.11775},\n  author = {Davor Runje and Sharath M. Shankaranarayana},\n  title = {Constrained Monotonic Neural Networks},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}\n</code></pre>"},{"location":"#install","title":"Install","text":"<pre><code>pip install mono-dense-keras\n</code></pre>"},{"location":"#how-to-use","title":"How to use","text":"<p>First, we\u2019ll create a simple dataset for testing using numpy. Inputs values \\(x_1\\), \\(x_2\\) and \\(x_3\\) will be sampled from the normal distribution, while the output value \\(y\\) will be calculated according to the following formula before adding noise to it:</p> <p>\\(y = x_1^3 + \\sin\\left(\\frac{x_2}{2 \\pi}\\right) + e^{-x_3}\\)</p> <pre><code>import numpy as np\n\nrng = np.random.default_rng(42)\n\ndef generate_data(no_samples: int, noise: float):\n    x = rng.normal(size=(no_samples, 3))\n    y = x[:, 0] ** 3\n    y += np.sin(x[:, 1] / (2*np.pi))\n    y += np.exp(-x[:, 2])\n    y += noise * rng.normal(size=no_samples)\n    return x, y\n\nx_train, y_train = generate_data(10_000, noise=0.1)\nx_val, y_val = generate_data(10_000, noise=0.)\n</code></pre> <p>Now, we\u2019ll use the <code>MonoDense</code> layer instead of <code>Dense</code> layer. By default, the <code>MonoDense</code> layer assumes the output of the layer is monotonically increasing with all inputs. This assumtion is always true for all layers except possibly the first one. For the first layer, we use <code>monotonicity_indicator</code> to specify which input parameters are monotonic and to specify are they increasingly or decreasingly monotonic: - set 1 for increasingly monotonic parameter,</p> <ul> <li> <p>set -1 for decreasingly monotonic parameter, and</p> </li> <li> <p>set 0 otherwise.</p> </li> </ul> <p>In our case, the <code>monotonicity_indicator</code> is <code>[1, 0, -1]</code> because \\(y\\) is: - monotonically increasing w.r.t. \\(x_1\\) \\(\\left(\\frac{\\partial y}{x_1} = 3 {x_1}^2 \\geq 0\\right)\\), and</p> <ul> <li>monotonically decreasing w.r.t. \\(x_3\\) \\(\\left(\\frac{\\partial y}{x_3} = - e^{-x_2} \\leq 0\\right)\\).</li> </ul> <pre><code>from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Input, Dense\nfrom mono_dense_keras import MonoDense\n\n# build a simple model with 3 hidden layer, but this using MonotonicDense layer\nmodel = Sequential()\n\nmodel.add(Input(shape=(3,)))\nmonotonicity_indicator = [1, 0, -1]\nmodel.add(MonoDense(128, activation=\"elu\", monotonicity_indicator=monotonicity_indicator))\nmodel.add(MonoDense(128, activation=\"elu\"))\nmodel.add(MonoDense(1))\n\nmodel.summary()\n</code></pre> <pre><code>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n mono_dense_2 (MonoDense)    (None, 128)               512\n\n mono_dense_3 (MonoDense)    (None, 128)               16512\n\n mono_dense_4 (MonoDense)    (None, 1)                 129\n\n=================================================================\nTotal params: 17,153\nTrainable params: 17,153\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <pre><code>from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ndef train_model(model, initial_learning_rate):\n    # train the model\n    lr_schedule = ExponentialDecay(\n        initial_learning_rate=initial_learning_rate,\n        decay_steps=10_000 // 32,\n        decay_rate=0.9,\n    )\n    optimizer = Adam(learning_rate=lr_schedule)\n    model.compile(optimizer=\"adam\", loss=\"mse\")\n\n    model.fit(x=x_train, y=y_train, batch_size=32, validation_data=(x_val, y_val), epochs=10)\n\ntrain_model(model, initial_learning_rate=1.)\n</code></pre> <pre><code>Epoch 1/10\n313/313 [==============================] - 2s 5ms/step - loss: 0.2590 - val_loss: 0.4990\nEpoch 2/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2875 - val_loss: 0.1390\nEpoch 3/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2241 - val_loss: 0.0790\nEpoch 4/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2297 - val_loss: 0.1043\nEpoch 5/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2502 - val_loss: 0.1089\nEpoch 6/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2231 - val_loss: 0.0590\nEpoch 7/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1715 - val_loss: 0.5466\nEpoch 8/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1890 - val_loss: 0.0863\nEpoch 9/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.1655 - val_loss: 0.1200\nEpoch 10/10\n313/313 [==============================] - 1s 4ms/step - loss: 0.2332 - val_loss: 0.1196\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The full text of the license is available at:</p> <p>https://github.com/airtai/mono-dense-keras/blob/main/LICENSE</p> <p>You are free to: - Share \u2014 copy and redistribute the material in any medium or format</p> <ul> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p> <p>Under the following terms: - Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> <ul> <li> <p>NonCommercial \u2014 You may not use the material for commercial purposes.</p> </li> <li> <p>ShareAlike \u2014 If you remix, transform, or build upon the material, you   must distribute your contributions under the same license as the   original.</p> </li> <li> <p>No additional restrictions \u2014 You may not apply legal terms or   technological measures that legally restrict others from doing   anything the license permits.</p> </li> </ul>"},{"location":"Experiments/","title":"Experiments","text":""},{"location":"Experiments/#imports","title":"Imports","text":"<pre><code>from keras_tuner import RandomSearch\n</code></pre> <pre><code>environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n</code></pre>"},{"location":"Experiments/#monotonic-dense-layer","title":"Monotonic Dense Layer","text":""},{"location":"Experiments/#monotonic-dense-layer_1","title":"Monotonic Dense Layer","text":"<p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p> <pre><code>units = 18\nactivation = \"relu\"\nbatch_size = 9\nx_len = 11\n\ntf.keras.utils.set_random_seed(42)\n\n\ndef display_kernel(kernel: Union[tf.Variable, np.typing.NDArray[float]]) -&gt; None:\n    cm = sns.color_palette(\"coolwarm_r\", as_cmap=True)\n\n    df = pd.DataFrame(kernel)\n\n    display(\n        df.style.format(\"{:.2f}\").background_gradient(cmap=cm, vmin=-1e-8, vmax=1e-8)\n    )\n\n\nx = np.random.default_rng(42).normal(size=(batch_size, x_len))\n\nfor monotonicity_indicator in [\n    [1] * 4 + [0] * 4 + [-1] * 3,\n    1,\n    np.ones((x_len,)),\n    -1,\n    -np.ones((x_len,)),\n]:\n    print(\"*\" * 120)\n    mono_layer = MonoDense(\n        units=units,\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator,\n        activation_weights=(7, 7, 4),\n    )\n    print(\"input:\")\n    display_kernel(x)\n\n    y = mono_layer(x)\n    print(f\"monotonicity_indicator = {monotonicity_indicator}\")\n    display_kernel(mono_layer.monotonicity_indicator)\n\n    print(\"kernel:\")\n    with replace_kernel_using_monotonicity_indicator(\n        mono_layer, mono_layer.monotonicity_indicator\n    ):\n        display_kernel(mono_layer.kernel)\n\n    print(\"output:\")\n    display_kernel(y)\nprint(\"ok\")\n</code></pre> <pre><code>************************************************************************************************************************\ninput:\nmonotonicity_indicator = [1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = 1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = -1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\nkernel:\noutput:\nok\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 0.00 5 0.00 6 0.00 7 0.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.33 0.15 0.13 0.41 0.38 0.14 0.43 0.30 0.02 0.12 0.38 0.05 0.42 0.03 0.00 0.24 0.44 0.28 1 0.01 0.39 0.42 0.32 0.38 0.22 0.33 0.34 0.03 0.06 0.06 0.27 0.26 0.45 0.35 0.05 0.21 0.34 2 0.21 0.29 0.16 0.14 0.42 0.06 0.15 0.10 0.41 0.08 0.03 0.22 0.34 0.20 0.11 0.01 0.43 0.35 3 0.27 0.33 0.06 0.17 0.42 0.42 0.24 0.30 0.11 0.20 0.17 0.25 0.17 0.07 0.32 0.30 0.17 0.36 4 0.32 -0.25 0.12 -0.37 0.41 0.20 0.06 -0.28 -0.27 0.43 -0.41 -0.17 -0.24 -0.31 0.33 0.31 0.11 0.03 5 0.04 0.19 -0.02 -0.34 0.36 -0.12 0.28 0.32 -0.11 -0.40 0.41 0.30 0.06 -0.28 -0.27 0.23 -0.41 -0.12 6 0.35 -0.04 -0.28 0.16 -0.03 0.35 -0.03 -0.16 0.39 -0.36 -0.31 -0.18 0.02 -0.38 -0.40 0.39 0.35 -0.19 7 0.33 -0.34 0.11 -0.29 0.25 -0.21 0.11 0.08 -0.19 -0.39 0.01 0.10 0.39 -0.25 -0.37 -0.27 0.04 0.34 8 -0.27 -0.09 -0.02 -0.45 -0.16 -0.12 -0.09 -0.43 -0.36 -0.09 -0.23 -0.42 -0.28 -0.24 -0.30 -0.31 -0.07 -0.07 9 -0.38 -0.34 -0.44 -0.42 -0.32 -0.06 -0.27 -0.28 -0.22 -0.05 -0.08 -0.07 -0.21 -0.39 -0.01 -0.26 -0.24 -0.42 10 -0.09 -0.45 -0.41 -0.36 -0.19 -0.09 -0.00 -0.34 -0.17 -0.18 -0.05 -0.39 -0.06 -0.20 -0.40 -0.33 -0.18 -0.01 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.01 0.40 0.00 1.38 0.00 0.10 0.00 -0.00 -0.00 -0.13 -0.00 -0.26 -0.00 -0.00 -0.55 -0.52 0.79 0.64 1 0.45 1.02 0.96 0.71 1.22 0.00 0.86 -0.00 -0.00 -0.09 -0.00 -0.00 -0.00 -0.00 0.26 -0.17 0.54 1.00 2 0.30 0.00 0.33 0.00 0.41 0.00 0.42 -0.53 -0.89 -0.29 -0.23 -0.84 -0.16 -0.93 -0.90 0.08 0.37 0.08 3 0.21 0.26 0.33 0.42 0.00 0.00 0.00 -0.16 -0.00 -0.61 -0.53 -0.07 -0.00 -0.00 -0.55 -0.66 0.83 0.78 4 1.38 0.49 0.70 0.82 1.47 0.54 0.63 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.73 0.97 0.94 0.91 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.86 -0.25 -0.00 -1.57 -1.19 -0.61 -0.23 0.13 -1.00 0.50 -0.06 6 0.00 0.00 0.00 0.17 0.00 0.00 0.00 -0.15 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.06 -1.00 0.00 0.12 7 0.00 0.96 0.35 0.93 0.00 0.32 0.17 -0.00 -0.00 -0.00 -0.00 -0.00 -0.17 -0.00 0.67 0.06 0.12 0.17 8 0.00 1.33 0.92 1.63 0.52 0.00 0.66 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.23 0.18 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.44 0.02 0.24 0.22 0.29 0.35 0.18 0.03 0.39 0.17 0.25 0.02 0.10 0.13 0.00 0.42 0.21 0.31 1 0.35 0.06 0.26 0.42 0.05 0.41 0.16 0.33 0.03 0.26 0.11 0.03 0.23 0.04 0.37 0.27 0.32 0.40 2 0.37 0.30 0.36 0.14 0.21 0.40 0.01 0.28 0.16 0.44 0.43 0.23 0.27 0.22 0.23 0.25 0.43 0.05 3 0.32 0.25 0.05 0.45 0.08 0.18 0.26 0.24 0.34 0.07 0.07 0.14 0.04 0.19 0.29 0.23 0.43 0.09 4 0.36 0.05 0.20 0.41 0.38 0.29 0.01 0.44 0.17 0.04 0.31 0.34 0.29 0.16 0.25 0.18 0.01 0.28 5 0.34 0.31 0.38 0.34 0.08 0.40 0.15 0.16 0.14 0.25 0.15 0.20 0.10 0.06 0.44 0.19 0.42 0.21 6 0.01 0.38 0.43 0.18 0.00 0.43 0.45 0.28 0.25 0.18 0.03 0.26 0.22 0.26 0.08 0.23 0.45 0.42 7 0.04 0.12 0.28 0.17 0.11 0.00 0.15 0.24 0.05 0.05 0.27 0.32 0.33 0.11 0.09 0.40 0.19 0.06 8 0.30 0.17 0.21 0.42 0.21 0.29 0.19 0.38 0.03 0.34 0.32 0.30 0.34 0.15 0.28 0.11 0.44 0.19 9 0.10 0.10 0.35 0.32 0.24 0.28 0.30 0.28 0.10 0.12 0.30 0.41 0.15 0.00 0.10 0.40 0.18 0.24 10 0.00 0.22 0.21 0.09 0.10 0.13 0.18 0.37 0.24 0.29 0.25 0.23 0.32 0.14 0.27 0.34 0.25 0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.01 0.00 0.00 0.00 0.00 0.00 -0.93 -0.00 -0.07 -0.58 -0.88 -0.58 -0.00 -0.87 -0.49 -0.05 -1.00 1 0.73 0.10 0.22 0.18 0.18 0.16 0.00 -0.23 -0.00 -0.00 -0.00 -0.09 -0.00 -0.00 0.16 0.47 0.53 -0.27 2 1.15 0.36 0.82 1.20 0.80 1.06 0.61 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.53 0.61 1.00 0.94 3 0.00 0.45 0.28 0.00 0.00 0.11 0.14 -0.00 -0.21 -0.00 -0.00 -0.00 -0.00 -0.00 0.15 0.08 0.72 -0.08 4 0.34 0.19 0.36 0.05 0.15 0.30 0.00 -0.00 -0.00 -0.08 -0.00 -0.00 -0.00 -0.00 0.06 0.38 0.04 0.14 5 0.00 0.00 0.26 0.00 0.67 0.05 0.00 -0.00 -0.16 -0.00 -0.00 -0.00 -0.00 -0.00 -0.08 0.30 -0.17 -0.17 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.76 -0.68 -0.28 -0.11 -0.37 -0.42 -0.40 -0.88 -0.41 -0.67 -1.00 7 0.01 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.17 -0.04 -0.57 -0.82 -0.50 -0.22 -0.07 -0.62 -0.13 -0.18 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.32 -0.35 -0.39 -0.77 -1.63 -1.12 -0.60 -0.47 -0.99 -1.00 -1.00 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 1.00 5 1.00 6 1.00 7 1.00 8 1.00 9 1.00 10 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.31 0.02 0.11 0.29 0.10 0.33 0.37 0.06 0.39 0.35 0.15 0.13 0.15 0.45 0.07 0.19 0.03 0.06 1 0.12 0.02 0.06 0.41 0.32 0.24 0.34 0.28 0.22 0.06 0.33 0.27 0.25 0.23 0.43 0.09 0.45 0.27 2 0.19 0.11 0.19 0.25 0.07 0.42 0.32 0.35 0.15 0.05 0.00 0.24 0.22 0.39 0.44 0.11 0.19 0.10 3 0.15 0.37 0.21 0.41 0.25 0.04 0.37 0.04 0.05 0.22 0.31 0.35 0.35 0.08 0.38 0.01 0.25 0.29 4 0.17 0.45 0.24 0.32 0.01 0.00 0.19 0.34 0.17 0.19 0.18 0.34 0.02 0.24 0.03 0.41 0.26 0.00 5 0.29 0.10 0.07 0.34 0.04 0.30 0.39 0.27 0.39 0.16 0.33 0.45 0.06 0.19 0.23 0.04 0.36 0.04 6 0.13 0.15 0.22 0.40 0.14 0.30 0.11 0.45 0.14 0.17 0.26 0.16 0.36 0.10 0.17 0.32 0.14 0.08 7 0.25 0.25 0.24 0.45 0.17 0.45 0.30 0.35 0.41 0.40 0.11 0.26 0.32 0.08 0.22 0.34 0.05 0.09 8 0.16 0.27 0.10 0.23 0.08 0.21 0.19 0.16 0.06 0.04 0.17 0.05 0.39 0.11 0.26 0.25 0.13 0.05 9 0.17 0.17 0.00 0.13 0.12 0.03 0.39 0.11 0.01 0.29 0.43 0.20 0.21 0.43 0.39 0.18 0.19 0.27 10 0.26 0.23 0.43 0.04 0.25 0.36 0.21 0.36 0.37 0.36 0.08 0.14 0.25 0.24 0.30 0.33 0.04 0.07 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.00 0.08 0.00 0.00 0.00 0.00 -0.82 -0.58 -0.32 -1.07 -1.09 -0.00 -0.63 -0.21 -0.74 -1.00 -0.15 1 0.36 0.00 0.00 0.51 0.11 0.72 0.76 -0.12 -0.00 -0.00 -0.05 -0.00 -0.00 -0.00 0.56 -0.34 0.13 0.22 2 0.72 0.68 0.32 1.10 0.10 0.84 0.68 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.20 0.97 0.33 -0.07 3 0.00 0.00 0.36 0.35 0.36 0.82 0.00 -0.00 -0.00 -0.19 -0.29 -0.13 -0.00 -0.20 0.67 0.20 -0.00 0.14 4 0.18 0.14 0.26 0.68 0.09 0.38 0.36 -0.00 -0.00 -0.00 -0.00 -0.00 -0.07 -0.00 0.14 0.15 0.33 0.10 5 0.01 0.55 0.50 0.00 0.00 0.21 0.00 -0.00 -0.27 -0.00 -0.44 -0.25 -0.00 -0.00 0.44 0.83 -0.24 -0.01 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.89 -0.85 -0.48 -0.77 -0.90 -0.21 -0.30 -0.09 -0.69 -0.83 -0.03 7 0.00 0.00 0.00 0.00 0.01 0.00 0.00 -0.79 -0.59 -0.65 -0.21 -0.55 -0.19 -0.37 -0.17 -0.71 -0.10 0.03 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.24 -0.48 -0.95 -1.13 -0.71 -1.40 -0.30 -0.76 -1.00 -0.47 -0.39 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.29 -0.12 -0.00 -0.17 -0.33 -0.17 -0.33 -0.36 -0.28 -0.16 -0.24 -0.22 -0.10 -0.13 -0.02 -0.38 -0.23 -0.02 1 -0.36 -0.13 -0.05 -0.07 -0.41 -0.30 -0.38 -0.06 -0.40 -0.42 -0.44 -0.03 -0.27 -0.03 -0.32 -0.31 -0.35 -0.40 2 -0.30 -0.07 -0.40 -0.06 -0.10 -0.21 -0.16 -0.22 -0.06 -0.36 -0.40 -0.42 -0.23 -0.22 -0.20 -0.33 -0.45 -0.06 3 -0.05 -0.08 -0.07 -0.30 -0.44 -0.23 -0.40 -0.25 -0.13 -0.31 -0.11 -0.13 -0.13 -0.34 -0.15 -0.05 -0.36 -0.13 4 -0.45 -0.34 -0.41 -0.39 -0.15 -0.10 -0.40 -0.32 -0.19 -0.13 -0.29 -0.39 -0.43 -0.29 -0.13 -0.05 -0.39 -0.01 5 -0.09 -0.38 -0.00 -0.12 -0.07 -0.42 -0.01 -0.12 -0.26 -0.28 -0.16 -0.06 -0.08 -0.43 -0.23 -0.28 -0.28 -0.07 6 -0.34 -0.38 -0.15 -0.44 -0.41 -0.19 -0.25 -0.41 -0.34 -0.22 -0.43 -0.36 -0.25 -0.28 -0.06 -0.12 -0.15 -0.16 7 -0.17 -0.39 -0.40 -0.26 -0.40 -0.20 -0.10 -0.14 -0.42 -0.21 -0.18 -0.25 -0.15 -0.21 -0.13 -0.41 -0.14 -0.14 8 -0.38 -0.03 -0.10 -0.21 -0.13 -0.04 -0.19 -0.00 -0.09 -0.38 -0.01 -0.27 -0.24 -0.24 -0.13 -0.18 -0.37 -0.21 9 -0.43 -0.08 -0.20 -0.29 -0.10 -0.27 -0.08 -0.43 -0.22 -0.37 -0.27 -0.24 -0.15 -0.22 -0.01 -0.45 -0.35 -0.31 10 -0.38 -0.44 -0.20 -0.31 -0.42 -0.23 -0.03 -0.31 -0.11 -0.35 -0.01 -0.00 -0.00 -0.39 -0.45 -0.14 -0.03 -0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 1.05 0.88 0.59 0.61 0.00 0.70 0.64 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.24 0.74 1.00 0.55 1 0.27 0.26 0.00 0.41 0.00 0.00 0.00 -0.00 -0.23 -0.33 -0.21 -0.20 -0.00 -0.02 -0.04 -0.82 -0.52 -0.02 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.36 -0.77 -0.71 -0.39 -1.00 -0.82 -0.67 -0.11 -0.74 -0.97 -0.31 3 0.00 0.00 0.00 0.00 0.00 0.01 0.00 -0.00 -0.15 -0.50 -0.38 -0.33 -0.20 -0.00 -0.39 -0.20 -0.12 -0.36 4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.46 -0.00 -0.84 -0.48 -0.36 -0.13 -0.08 -0.28 -0.33 0.13 5 0.00 0.02 0.00 0.00 0.12 0.33 0.00 -0.41 -0.00 -0.44 -0.33 -0.90 -0.56 -0.04 -0.24 -0.27 -0.48 -0.16 6 0.74 1.20 0.11 0.90 0.84 0.65 0.87 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.60 0.01 0.53 0.12 7 0.47 0.89 0.91 0.62 0.26 0.37 0.01 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.07 0.61 0.29 0.01 8 1.30 1.17 0.98 1.61 1.09 0.59 0.65 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.09 0.93 0.94 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 1 -1.00 2 -1.00 3 -1.00 4 -1.00 5 -1.00 6 -1.00 7 -1.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.45 -0.28 -0.30 -0.41 -0.17 -0.39 -0.22 -0.45 -0.28 -0.40 -0.18 -0.20 -0.16 -0.18 -0.10 -0.13 -0.14 -0.35 1 -0.09 -0.27 -0.09 -0.14 -0.02 -0.36 -0.21 -0.05 -0.05 -0.01 -0.02 -0.45 -0.03 -0.09 -0.01 -0.05 -0.39 -0.05 2 -0.17 -0.15 -0.37 -0.35 -0.32 -0.03 -0.24 -0.31 -0.35 -0.41 -0.00 -0.37 -0.18 -0.26 -0.09 -0.44 -0.09 -0.17 3 -0.42 -0.17 -0.11 -0.31 -0.32 -0.11 -0.20 -0.10 -0.34 -0.15 -0.24 -0.22 -0.22 -0.08 -0.40 -0.02 -0.23 -0.38 4 -0.13 -0.17 -0.06 -0.13 -0.32 -0.42 -0.28 -0.44 -0.03 -0.26 -0.38 -0.45 -0.08 -0.06 -0.04 -0.33 -0.27 -0.38 5 -0.32 -0.38 -0.19 -0.19 -0.33 -0.01 -0.15 -0.08 -0.31 -0.27 -0.07 -0.11 -0.21 -0.22 -0.18 -0.27 -0.19 -0.15 6 -0.30 -0.16 -0.09 -0.25 -0.23 -0.44 -0.25 -0.16 -0.05 -0.13 -0.20 -0.09 -0.14 -0.18 -0.15 -0.22 -0.37 -0.38 7 -0.20 -0.14 -0.12 -0.10 -0.42 -0.42 -0.14 -0.04 -0.44 -0.11 -0.10 -0.17 -0.06 -0.29 -0.22 -0.24 -0.01 -0.45 8 -0.31 -0.11 -0.16 -0.21 -0.16 -0.39 -0.12 -0.36 -0.36 -0.29 -0.24 -0.24 -0.20 -0.18 -0.33 -0.39 -0.20 -0.02 9 -0.41 -0.14 -0.12 -0.21 -0.01 -0.37 -0.03 -0.22 -0.38 -0.22 -0.09 -0.22 -0.19 -0.17 -0.13 -0.32 -0.30 -0.21 10 -0.31 -0.05 -0.02 -0.36 -0.04 -0.15 -0.03 -0.12 -0.36 -0.21 -0.40 -0.03 -0.04 -0.03 -0.23 -0.01 -0.02 -0.41 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.20 0.84 0.11 0.00 0.55 1.24 0.55 -0.00 -0.02 -0.00 -0.00 -0.00 -0.00 -0.00 -0.20 0.98 1.00 0.30 1 0.00 0.00 0.00 0.00 0.00 0.19 0.00 -0.14 -0.87 -0.50 -0.00 -0.34 -0.28 -0.53 -0.24 -0.34 0.23 -0.09 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.34 -0.82 -1.02 -0.75 -0.74 -0.56 -0.68 -0.71 -1.00 -0.65 -0.56 3 0.23 0.18 0.00 0.00 0.00 0.00 0.00 -0.00 -0.27 -0.00 -0.00 -0.21 -0.00 -0.28 -0.21 -0.24 0.02 0.00 4 0.09 0.00 0.00 0.00 0.00 0.00 0.00 -0.08 -0.00 -0.14 -0.00 -0.50 -0.01 -0.25 0.23 -0.20 -0.14 -0.66 5 0.18 0.49 0.00 0.00 0.03 0.00 0.00 -0.79 -0.36 -0.49 -0.39 -0.69 -0.00 -0.09 0.08 -0.84 0.10 -0.25 6 0.64 0.76 0.08 0.50 0.62 0.79 0.68 -0.00 -0.06 -0.00 -0.00 -0.00 -0.00 -0.00 0.28 0.24 0.86 0.87 7 0.32 0.24 0.23 0.18 0.76 0.62 0.28 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.13 0.73 0.09 0.87 8 1.23 0.50 0.27 0.51 1.08 2.00 0.60 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 1.00 1.00 1.00 <pre><code>x = Input(shape=(5, 7, 8))\n\nlayer = MonoDense(\n    units=12,\n    activation=activation,\n    monotonicity_indicator=[1] * 3 + [-1] * 3 + [0] * 2,\n    is_convex=False,\n    is_concave=False,\n)\n\ny = layer(x)\n\nmodel = Model(inputs=x, outputs=y)\n\nmodel.summary()\n\ndisplay_kernel(layer.monotonicity_indicator)\n</code></pre> <pre><code>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 5, 7, 8)]         0\n\n mono_dense_5 (MonoDense)    (None, 5, 7, 12)          108\n\n=================================================================\nTotal params: 108\nTrainable params: 108\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> 0 0 1.00 1 1.00 2 1.00 3 -1.00 4 -1.00 5 -1.00 6 0.00 7 0.00"},{"location":"Experiments/#experiments_1","title":"Experiments","text":"<p>For our experiments, we employ the datasets used by the authors of Certified Monotonic Network [1] and COMET [2]. We use the exact train-test split provided by the authors. Their respective repositories are linked below in the references. We directly load the saved train-test data split which have been saved after running the codes from respective papers\u2019 authors.</p> <p>References:</p> <ol> <li>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</li> </ol> <p>Github repo: https://github.com/gnobitab/CertifiedMonotonicNetwork</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>Github repo: https://github.com/AishwaryaSivaraman/COMET</p> <p>source</p>"},{"location":"Experiments/#download_data","title":"download_data","text":"<pre><code> download_data (dataset_name:str,\n                data_path:Union[pathlib.Path,str,NoneType]='data',\n                force_download:bool=False)\n</code></pre> <p>source</p>"},{"location":"Experiments/#get_data_path","title":"get_data_path","text":"<pre><code> get_data_path (data_path:Union[pathlib.Path,str,NoneType]=None)\n</code></pre> <pre><code>download_data(\"auto\", force_download=True)\n\n!ls -l data\n\nassert (Path(\"data\") / \"train_auto.csv\").exists()\n</code></pre> <pre><code>train_auto.csv: 49.2kB [00:01, 39.3kB/s]                            \ntest_auto.csv: 16.4kB [00:00, 22.7kB/s]\n\ntotal 257812\n-rw-rw-r-- 1 davor davor    11161 May 30 20:35 test_auto.csv\n-rw-rw-r-- 1 davor davor 11340054 May 25 04:48 test_blog.csv\n-rw-rw-r-- 1 davor davor   101210 May 25 04:48 test_compas.csv\n-rw-rw-r-- 1 davor davor    15798 May 25 04:48 test_heart.csv\n-rw-rw-r-- 1 davor davor 13339777 May 25 04:48 test_loan.csv\n-rw-rw-r-- 1 davor davor    44626 May 30 20:35 train_auto.csv\n-rw-rw-r-- 1 davor davor 79478767 May 25 04:48 train_blog.csv\n-rw-rw-r-- 1 davor davor   405660 May 25 04:48 train_compas.csv\n-rw-rw-r-- 1 davor davor    62282 May 25 04:48 train_heart.csv\n-rw-rw-r-- 1 davor davor 79588030 May 25 04:48 train_loan.csv\n-rw-rw-r-- 1 davor davor 79588030 May 29 13:57 {prefix}_{name}.csv\n</code></pre> <p>source</p>"},{"location":"Experiments/#sanitize_col_names","title":"sanitize_col_names","text":"<pre><code> sanitize_col_names (df:pandas.core.frame.DataFrame)\n</code></pre> <pre><code>sanitize_col_names(pd.DataFrame({\"a b\": [1, 2, 3]}))\n</code></pre>   |     | a_b | |-----|-----| | 0   | 1   | | 1   | 2   | | 2   | 3   |   <p>source</p>"},{"location":"Experiments/#get_train_n_test_data","title":"get_train_n_test_data","text":"<pre><code> get_train_n_test_data (dataset_name:str,\n                        data_path:Union[pathlib.Path,str,NoneType]='./data\n                        ')\n</code></pre> <pre><code>train_df, test_df = get_train_n_test_data(\"auto\")\ndisplay(train_df)\ndisplay(test_df)\n</code></pre>   |     | Cylinders | Displacement | Horsepower | Weight    | Acceleration | Model_Year | Origin    | ground_truth | |-----|-----------|--------------|------------|-----------|--------------|------------|-----------|--------------| | 0   | 1.482807  | 1.073028     | 0.650564   | 0.606625  | -1.275546    | -1.631803  | -0.701669 | 18.0         | | 1   | 1.482807  | 1.482902     | 1.548993   | 0.828131  | -1.452517    | -1.631803  | -0.701669 | 15.0         | | 2   | 1.482807  | 1.044432     | 1.163952   | 0.523413  | -1.275546    | -1.631803  | -0.701669 | 16.0         | | 3   | 1.482807  | 1.025368     | 0.907258   | 0.542165  | -1.806460    | -1.631803  | -0.701669 | 17.0         | | 4   | 1.482807  | 2.235927     | 2.396084   | 1.587581  | -1.983431    | -1.631803  | -0.701669 | 15.0         | | ... | ...       | ...          | ...        | ...       | ...          | ...        | ...       | ...          | | 309 | 0.310007  | 0.358131     | 0.188515   | -0.177437 | -0.319901    | 1.720778   | -0.701669 | 22.0         | | 310 | -0.862792 | -0.566468    | -0.530229  | -0.722413 | -0.921604    | 1.720778   | -0.701669 | 36.0         | | 311 | -0.862792 | -0.928683    | -1.351650  | -1.003691 | 3.184131     | 1.720778   | 0.557325  | 44.0         | | 312 | -0.862792 | -0.566468    | -0.530229  | -0.810312 | -1.417123    | 1.720778   | -0.701669 | 32.0         | | 313 | -0.862792 | -0.709448    | -0.658576  | -0.423555 | 1.060475     | 1.720778   | -0.701669 | 28.0         |  <p>314 rows \u00d7 8 columns</p>   |     | Cylinders | Displacement | Horsepower | Weight    | Acceleration | Model_Year | Origin    | ground_truth | |-----|-----------|--------------|------------|-----------|--------------|------------|-----------|--------------| | 0   | -0.862792 | -1.043066    | -1.017947  | -1.027131 | 1.272841     | 1.162014   | 1.816319  | 40.8         | | 1   | 1.482807  | 1.177880     | 1.163952   | 0.526929  | -1.629489    | -1.631803  | -0.701669 | 18.0         | | 2   | 1.482807  | 1.482902     | 1.934034   | 0.794143  | -1.629489    | -0.793657  | -0.701669 | 11.0         | | 3   | 0.310007  | 0.529707     | -0.119518  | 0.346443  | -0.213718    | -1.352421  | -0.701669 | 19.0         | | 4   | -0.862792 | -1.004939    | -0.863931  | -1.243949 | -0.567661    | 0.882633   | 0.557325  | 31.9         | | ... | ...       | ...          | ...        | ...       | ...          | ...        | ...       | ...          | | 73  | -0.862792 | -0.699916    | 0.188515   | -0.062582 | -0.390690    | -1.073039  | 0.557325  | 18.0         | | 74  | -0.862792 | -0.518809    | -0.838261  | -0.686081 | 1.379024     | -0.793657  | -0.701669 | 21.0         | | 75  | 0.310007  | -0.251914    | 0.701903   | -0.089538 | -1.487912    | 1.162014   | 1.816319  | 32.7         | | 76  | 1.482807  | 1.492434     | 1.138283   | 1.580549  | -0.390690    | 0.323869   | -0.701669 | 16.0         | | 77  | 0.310007  | -0.375829    | 0.060168   | -0.602870 | -0.567661    | -0.793657  | -0.701669 | 21.0         |  <p>78 rows \u00d7 8 columns</p> <p>source</p>"},{"location":"Experiments/#peek","title":"peek","text":"<pre><code> peek (ds:tensorflow.python.data.ops.dataset_ops.DatasetV2)\n</code></pre> <p>source</p>"},{"location":"Experiments/#df2ds","title":"df2ds","text":"<pre><code> df2ds (df:pandas.core.frame.DataFrame)\n</code></pre> <pre><code>x, y = peek(df2ds(train_df).batch(8))\ndisplay(x)\ndisplay(y)\n\nexpected = {\n    \"Acceleration\",\n    \"Cylinders\",\n    \"Displacement\",\n    \"Horsepower\",\n    \"Model_Year\",\n    \"Origin\",\n    \"Weight\",\n}\nassert set(x.keys()) == expected\nfor k in expected:\n    assert x[k].shape == (8,)\nassert y.shape == (8,)\n</code></pre> <pre><code>{'Cylinders': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([1.4828068, 1.4828068, 1.4828068, 1.4828068, 1.4828068, 1.4828068,\n        1.4828068, 1.4828068], dtype=float32)&gt;,\n 'Displacement': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([1.0730283, 1.4829025, 1.0444324, 1.0253685, 2.235927 , 2.474226 ,\n        2.3407786, 1.8641808], dtype=float32)&gt;,\n 'Horsepower': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([0.65056413, 1.5489933 , 1.1639522 , 0.9072582 , 2.3960838 ,\n        2.9608107 , 2.8324637 , 2.1907284 ], dtype=float32)&gt;,\n 'Weight': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([0.6066247, 0.828131 , 0.5234134, 0.5421652, 1.5875812, 1.602817 ,\n        1.5535934, 1.0121336], dtype=float32)&gt;,\n 'Acceleration': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([-1.2755462, -1.4525175, -1.2755462, -1.8064601, -1.9834315,\n        -2.3373742, -2.5143454, -2.5143454], dtype=float32)&gt;,\n 'Model_Year': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([-1.6318026, -1.6318026, -1.6318026, -1.6318026, -1.6318026,\n        -1.6318026, -1.6318026, -1.6318026], dtype=float32)&gt;,\n 'Origin': &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([-0.7016686, -0.7016686, -0.7016686, -0.7016686, -0.7016686,\n        -0.7016686, -0.7016686, -0.7016686], dtype=float32)&gt;}\n\n&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([18., 15., 16., 17., 15., 14., 14., 15.], dtype=float32)&gt;\n</code></pre> <p>source</p>"},{"location":"Experiments/#build_mono_model_f","title":"build_mono_model_f","text":"<pre><code> build_mono_model_f (monotonicity_indicator:Dict[str,int], final_activatio\n                     n:Union[str,Callable[[Union[tensorflow.python.types.c\n                     ore.Tensor,tensorflow.python.types.core.TensorProtoco\n                     l,int,float,bool,str,bytes,complex,tuple,list,numpy.n\n                     darray,numpy.generic],Union[tensorflow.python.types.c\n                     ore.Tensor,tensorflow.python.types.core.TensorProtoco\n                     l,int,float,bool,str,bytes,complex,tuple,list,numpy.n\n                     darray,numpy.generic]],Union[tensorflow.python.types.\n                     core.Tensor,tensorflow.python.types.core.TensorProtoc\n                     ol,int,float,bool,str,bytes,complex,tuple,list,numpy.\n                     ndarray,numpy.generic]]], loss:Union[str,Callable[[Un\n                     ion[tensorflow.python.types.core.Tensor,tensorflow.py\n                     thon.types.core.TensorProtocol,int,float,bool,str,byt\n                     es,complex,tuple,list,numpy.ndarray,numpy.generic],Un\n                     ion[tensorflow.python.types.core.Tensor,tensorflow.py\n                     thon.types.core.TensorProtocol,int,float,bool,str,byt\n                     es,complex,tuple,list,numpy.ndarray,numpy.generic]],U\n                     nion[tensorflow.python.types.core.Tensor,tensorflow.p\n                     ython.types.core.TensorProtocol,int,float,bool,str,by\n                     tes,complex,tuple,list,numpy.ndarray,numpy.generic]]]\n                     , metrics:Union[str,Callable[[Union[tensorflow.python\n                     .types.core.Tensor,tensorflow.python.types.core.Tenso\n                     rProtocol,int,float,bool,str,bytes,complex,tuple,list\n                     ,numpy.ndarray,numpy.generic],Union[tensorflow.python\n                     .types.core.Tensor,tensorflow.python.types.core.Tenso\n                     rProtocol,int,float,bool,str,bytes,complex,tuple,list\n                     ,numpy.ndarray,numpy.generic]],Union[tensorflow.pytho\n                     n.types.core.Tensor,tensorflow.python.types.core.Tens\n                     orProtocol,int,float,bool,str,bytes,complex,tuple,lis\n                     t,numpy.ndarray,numpy.generic]]], train_ds:tensorflow\n                     .python.data.ops.dataset_ops.DatasetV2, batch_size,\n                     units:int, n_layers:int, activation:Union[str,Callabl\n                     e[[Union[tensorflow.python.types.core.Tensor,tensorfl\n                     ow.python.types.core.TensorProtocol,int,float,bool,st\n                     r,bytes,complex,tuple,list,numpy.ndarray,numpy.generi\n                     c],Union[tensorflow.python.types.core.Tensor,tensorfl\n                     ow.python.types.core.TensorProtocol,int,float,bool,st\n                     r,bytes,complex,tuple,list,numpy.ndarray,numpy.generi\n                     c]],Union[tensorflow.python.types.core.Tensor,tensorf\n                     low.python.types.core.TensorProtocol,int,float,bool,s\n                     tr,bytes,complex,tuple,list,numpy.ndarray,numpy.gener\n                     ic]]], learning_rate:float, weight_decay:float,\n                     dropout:float, decay_rate:float)\n</code></pre> <pre><code>train_df, test_df = get_train_n_test_data(\"auto\")\ntrain_ds = df2ds(train_df)\ntest_ds = df2ds(test_df)\n\nbuild_model_f = lambda: build_mono_model_f(\n    monotonicity_indicator = {\n        \"Cylinders\": 0,\n        \"Displacement\": -1,\n        \"Horsepower\": -1,\n        \"Weight\": -1,\n        \"Acceleration\": 0,\n        \"Model_Year\": 0,\n        \"Origin\": 0,\n    },\n    final_activation = None,\n    loss = \"mse\",\n    metrics = \"mse\",\n    train_ds=train_ds,\n    batch_size=8,\n    units = 16,\n    n_layers = 3,\n    activation = \"elu\",\n    learning_rate = 0.01,\n    weight_decay = 0.001,\n    dropout = 0.25,\n    decay_rate = 0.95,\n)\nmodel = build_model_f()\nmodel.summary()\nmodel.fit(train_ds.batch(8), validation_data=test_ds.batch(256), epochs=1)\n</code></pre> <pre><code>Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n Acceleration (InputLayer)      [(None, 1)]          0           []\n\n Cylinders (InputLayer)         [(None, 1)]          0           []\n\n Displacement (InputLayer)      [(None, 1)]          0           []\n\n Horsepower (InputLayer)        [(None, 1)]          0           []\n\n Model_Year (InputLayer)        [(None, 1)]          0           []\n\n Origin (InputLayer)            [(None, 1)]          0           []\n\n Weight (InputLayer)            [(None, 1)]          0           []\n\n dense_Acceleration (Dense)     (None, 4)            8           ['Acceleration[0][0]']\n\n dense_Cylinders (Dense)        (None, 4)            8           ['Cylinders[0][0]']\n\n mono_dense_Displacement_decrea  (None, 4)           8           ['Displacement[0][0]']           \n sing (MonoDense)\n\n mono_dense_Horsepower_decreasi  (None, 4)           8           ['Horsepower[0][0]']             \n ng (MonoDense)\n\n dense_Model_Year (Dense)       (None, 4)            8           ['Model_Year[0][0]']\n\n dense_Origin (Dense)           (None, 4)            8           ['Origin[0][0]']\n\n mono_dense_Weight_decreasing (  (None, 4)           8           ['Weight[0][0]']                 \n MonoDense)\n\n preprocessed_features (Concate  (None, 28)          0           ['dense_Acceleration[0][0]',     \n nate)                                                            'dense_Cylinders[0][0]',        \n                                                                  'mono_dense_Displacement_decreas\n                                                                 ing[0][0]',                      \n                                                                  'mono_dense_Horsepower_decreasin\n                                                                 g[0][0]',                        \n                                                                  'dense_Model_Year[0][0]',       \n                                                                  'dense_Origin[0][0]',           \n                                                                  'mono_dense_Weight_decreasing[0]\n                                                                 [0]']\n\n mono_dense_0 (MonoDense)       (None, 16)           464         ['preprocessed_features[0][0]']\n\n dropout (Dropout)              (None, 16)           0           ['mono_dense_0[0][0]']\n\n mono_dense_1_increasing (MonoD  (None, 16)          272         ['dropout[0][0]']                \n ense)\n\n dropout_1 (Dropout)            (None, 16)           0           ['mono_dense_1_increasing[0][0]']\n\n mono_dense_2_increasing (MonoD  (None, 1)           17          ['dropout_1[0][0]']              \n ense)\n\n==================================================================================================\nTotal params: 809\nTrainable params: 809\nNon-trainable params: 0\n__________________________________________________________________________________________________\n40/40 [==============================] - 3s 12ms/step - loss: 152.4450 - mse: 152.4450 - val_loss: 159.3336 - val_mse: 159.3336\n\n&lt;keras.callbacks.History at 0x7f2ef83a3e80&gt;\n</code></pre> <p>source</p>"},{"location":"Experiments/#testhypermodel","title":"TestHyperModel","text":"<pre><code> TestHyperModel (**kwargs:Any)\n</code></pre> <p>Defines a search space of models.</p> <p>A search space is a collection of models. The <code>build</code> function will build one of the models from the space using the given <code>HyperParameters</code> object.</p> <p>Users should subclass the <code>HyperModel</code> class to define their search spaces by overriding <code>build()</code>, which creates and returns the Keras model. Optionally, you may also override <code>fit()</code> to customize the training process of the model.</p> <p>Examples:</p> <p>In <code>build()</code>, you can create the model using the hyperparameters.</p> <pre><code>class MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        model = keras.Sequential()\n        model.add(keras.layers.Dense(\n            hp.Choice('units', [8, 16, 32]),\n            activation='relu'))\n        model.add(keras.layers.Dense(1, activation='relu'))\n        model.compile(loss='mse')\n        return model\n</code></pre> <p>When overriding <code>HyperModel.fit()</code>, if you use <code>model.fit()</code> to train your model, which returns the training history, you can return it directly. You may use <code>hp</code> to specify any hyperparameters to tune.</p> <pre><code>class MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        ...\n\n    def fit(self, hp, model, *args, **kwargs):\n        return model.fit(\n            *args,\n            epochs=hp.Int(\"epochs\", 5, 20),\n            **kwargs)\n</code></pre> <p>If you have a customized training process, you can return the objective value as a float.</p> <p>If you want to keep track of more metrics, you can return a dictionary of the metrics to track.</p> <pre><code>class MyHyperModel(kt.HyperModel):\n    def build(self, hp):\n        ...\n\n    def fit(self, hp, model, *args, **kwargs):\n        ...\n        return {\n            \"loss\": loss,\n            \"val_loss\": val_loss,\n            \"val_accuracy\": val_accuracy\n        }\n</code></pre> <p>Args: name: Optional string, the name of this HyperModel. tunable: Boolean, whether the hyperparameters defined in this hypermodel should be added to search space. If <code>False</code>, either the search space for these parameters must be defined in advance, or the default values will be used. Defaults to True.</p> <p>source</p>"},{"location":"Experiments/#get_build_model_with_hp_f","title":"get_build_model_with_hp_f","text":"<pre><code> get_build_model_with_hp_f\n                            (build_model_f:Callable[[],keras.engine.traini\n                            ng.Model], **kwargs:Any)\n</code></pre> <pre><code>with TemporaryDirectory() as d:\n    tuner = RandomSearch(\n        hypermodel=TestHyperModel(\n            monotonicity_indicator={\n                \"Cylinders\": 0,\n                \"Displacement\": -1,\n                \"Horsepower\": -1,\n                \"Weight\": -1,\n                \"Acceleration\": 0,\n                \"Model_Year\": 0,\n                \"Origin\": 0,\n            },\n            final_activation=None,\n            loss=\"mse\",\n            metrics=\"mse\",\n            train_ds=train_ds,\n            batch_size=8,\n        ),\n        directory=d,\n        project_name=\"testing\",\n        max_trials=2,\n        objective=\"val_loss\",\n    )\n    tuner.search(\n        train_ds.shuffle(len(train_ds)).batch(8).prefetch(2),\n        validation_data=test_ds.batch(256),\n        epochs=2,\n    )\n</code></pre> <pre><code>Trial 2 Complete [00h 00m 04s]\nval_loss: 10.756942749023438\n\nBest val_loss So Far: 10.756942749023438\nTotal elapsed time: 00h 00m 09s\nINFO:tensorflow:Oracle triggered exit\n</code></pre> <p>source</p>"},{"location":"Experiments/#find_hyperparameters","title":"find_hyperparameters","text":"<pre><code> find_hyperparameters (dataset_name:str,\n                       monotonicity_indicator:Dict[str,int], final_activat\n                       ion:Union[str,Callable[[Union[tensorflow.python.typ\n                       es.core.Tensor,tensorflow.python.types.core.TensorP\n                       rotocol,int,float,bool,str,bytes,complex,tuple,list\n                       ,numpy.ndarray,numpy.generic],Union[tensorflow.pyth\n                       on.types.core.Tensor,tensorflow.python.types.core.T\n                       ensorProtocol,int,float,bool,str,bytes,complex,tupl\n                       e,list,numpy.ndarray,numpy.generic]],Union[tensorfl\n                       ow.python.types.core.Tensor,tensorflow.python.types\n                       .core.TensorProtocol,int,float,bool,str,bytes,compl\n                       ex,tuple,list,numpy.ndarray,numpy.generic]]], loss:\n                       Union[str,Callable[[Union[tensorflow.python.types.c\n                       ore.Tensor,tensorflow.python.types.core.TensorProto\n                       col,int,float,bool,str,bytes,complex,tuple,list,num\n                       py.ndarray,numpy.generic],Union[tensorflow.python.t\n                       ypes.core.Tensor,tensorflow.python.types.core.Tenso\n                       rProtocol,int,float,bool,str,bytes,complex,tuple,li\n                       st,numpy.ndarray,numpy.generic]],Union[tensorflow.p\n                       ython.types.core.Tensor,tensorflow.python.types.cor\n                       e.TensorProtocol,int,float,bool,str,bytes,complex,t\n                       uple,list,numpy.ndarray,numpy.generic]]], metrics:U\n                       nion[str,Callable[[Union[tensorflow.python.types.co\n                       re.Tensor,tensorflow.python.types.core.TensorProtoc\n                       ol,int,float,bool,str,bytes,complex,tuple,list,nump\n                       y.ndarray,numpy.generic],Union[tensorflow.python.ty\n                       pes.core.Tensor,tensorflow.python.types.core.Tensor\n                       Protocol,int,float,bool,str,bytes,complex,tuple,lis\n                       t,numpy.ndarray,numpy.generic]],Union[tensorflow.py\n                       thon.types.core.Tensor,tensorflow.python.types.core\n                       .TensorProtocol,int,float,bool,str,bytes,complex,tu\n                       ple,list,numpy.ndarray,numpy.generic]]],\n                       max_trials:int=100, max_epochs:int=50,\n                       batch_size:int=8, objective:Union[str,keras_tuner.e\n                       ngine.objective.Objective], direction:str,\n                       dir_root:Union[pathlib.Path,str]='tuner',\n                       seed:int=42, executions_per_trial:int=3,\n                       max_consecutive_failed_trials:int=5,\n                       patience:int=10)\n</code></pre> <pre><code>tuner = find_hyperparameters(\n    \"auto\",\n    monotonicity_indicator={\n        \"Cylinders\": 0,\n        \"Displacement\": -1,\n        \"Horsepower\": -1,\n        \"Weight\": -1,\n        \"Acceleration\": 0,\n        \"Model_Year\": 0,\n        \"Origin\": 0,\n    },\n    max_trials=2,\n    final_activation=None,\n    loss=\"mse\",\n    metrics=\"mse\",\n    objective=\"val_mse\",\n    direction=\"min\",\n)\n</code></pre> <pre><code>INFO:tensorflow:Reloading Tuner from tuner/auto/tuner0.json\nINFO:tensorflow:Oracle triggered exit\n</code></pre> <p>source</p>"},{"location":"Experiments/#create_tuner_stats","title":"create_tuner_stats","text":"<pre><code> create_tuner_stats (tuner:keras_tuner.engine.tuner.Tuner,\n                     num_models:int=10,\n                     stats:Optional[pandas.core.frame.DataFrame]=None,\n                     max_epochs:int=50, batch_size:int=8, patience:int=10,\n                     verbose:int=0)\n</code></pre> <p>source</p>"},{"location":"Experiments/#create_model_stats","title":"create_model_stats","text":"<pre><code> create_model_stats (tuner:keras_tuner.engine.tuner.Tuner,\n                     hp:Dict[str,Any],\n                     stats:Optional[pandas.core.frame.DataFrame]=None,\n                     max_epochs:int, num_runs:int, top_runs:int,\n                     batch_size:int, patience:int, verbose:int, train_ds:t\n                     ensorflow.python.data.ops.dataset_ops.DatasetV2, test\n                     _ds:tensorflow.python.data.ops.dataset_ops.DatasetV2)\n</code></pre> <p>source</p>"},{"location":"Experiments/#count_model_params","title":"count_model_params","text":"<pre><code> count_model_params (model:keras.engine.training.Model)\n</code></pre> <pre><code>stats = create_tuner_stats(tuner, verbose=0)\n</code></pre>   |     | units | n_layers | activation | learning_rate | weight_decay | dropout  | decay_rate | val_mse_mean | val_mse_std | val_mse_min | val_mse_max | params | |-----|-------|----------|------------|---------------|--------------|----------|------------|--------------|-------------|-------------|-------------|--------| | 0   | 23    | 1        | elu        | 0.004715      | 0.265345     | 0.175923 | 0.816107   | 21.378424    | 1.74334     | 18.393272   | 22.992588   | 106    |     |     | units | n_layers | activation | learning_rate | weight_decay | dropout  | decay_rate | val_mse_mean | val_mse_std | val_mse_min | val_mse_max | params | |-----|-------|----------|------------|---------------|--------------|----------|------------|--------------|-------------|-------------|-------------|--------| | 1   | 9     | 2        | elu        | 0.265157      | 0.196993     | 0.456821 | 0.560699   | 12.739007    | 1.868053    | 10.745924   | 15.127256   | 173    | | 0   | 23    | 1        | elu        | 0.004715      | 0.265345     | 0.175923 | 0.816107   | 21.378424    | 1.743340    | 18.393272   | 22.992588   | 106    |"},{"location":"Experiments/#rm-rf-tuner","title":"!rm -rf tuner","text":"<p>assert False</p>"},{"location":"Experiments/#ls-tmptunerauto_tuner2023-02-28t130231787216","title":"!ls /tmp/tuner/auto_tuner/2023-02-28T13:02:31.787216","text":"<p>def load_latest_tuner( build_model_f: Callable[\u2026, Model], tuner_name: str = \u201cBayesianOptimization\u201d, *, max_trials: Optional[int] = None, max_epochs: Optional[int] = None, train_ds: tf.data.Dataset, test_ds: tf.data.Dataset, objective: Union[str, Objective], dir_root: Union[Path, str], project_name: str, factor: int = 2, seed: int = 42, executions_per_trial: int = 1, hyperband_iterations: int = 1, max_consecutive_failed_trials: int = 5, ) -&gt; Tuner: directory = Path(dir_root) print(f\u201dLoading tuner saved at: {directory}\u201c)</p> <pre><code>if tuner_name == \"BayesianOptimization\":\n    tuner = BayesianOptimization(\n        build_model_f,\n        objective=objective,\n        max_trials=max_trials,\n        seed=seed,\n        directory=directory,\n        project_name=project_name,\n        executions_per_trial=executions_per_trial,\n        max_consecutive_failed_trials=max_consecutive_failed_trials,\n    )\n    kwargs = dict(epochs=max_epochs)\nelif tuner_name == \"Hyperband\":\n    tuner = Hyperband(\n        build_model_f,\n        objective=objective,\n        max_epochs=max_epochs,\n        factor=factor,\n        seed=seed,\n        directory=directory,\n        project_name=project_name,\n        executions_per_trial=executions_per_trial,\n        hyperband_iterations=hyperband_iterations,\n        max_consecutive_failed_trials=max_consecutive_failed_trials,\n    )\n    kwargs = dict()\nelse:\n    raise ValueError(f\"tuner_name={tuner_name}\")\n\nreturn tuner\n</code></pre> <p>def hyperparameter_search(epochs, num_runs=10, num_models=10, tuner_search_kwargs): tuner = find_hyperparameters(tuner_search_kwargs) tuner = load_latest_tuner(**tuner_search_kwargs)</p> <pre><code>stats = create_tuner_stats(\n    tuner,\n    epochs=epochs,\n    num_runs=num_runs,\n    num_models=num_models,\n    train_ds=tuner_search_kwargs[\"train_ds\"],\n    test_ds=tuner_search_kwargs[\"test_ds\"],\n)\n\nreturn stats, tuner\n</code></pre>"},{"location":"Experiments/#comparison-with-methods-and-datasets-from-comet-1-reference-20-in-our-paper","title":"Comparison with methods and datasets from COMET [1] (Reference #20 in our paper)","text":"<p>References:</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>Github repo: https://github.com/AishwaryaSivaraman/COMET</p>"},{"location":"Experiments/#experiment-for-auto-mpg-dataset","title":"Experiment for Auto MPG dataset","text":"<p>The Auto MPG Dataset is a regression dataset [1] with 7 features - Cylinders, Displacement, Horsepower,Weight, Acceleration, Model Year, Origin. And the dependant variable is monotonically decreasing with respect to features weigh, displacement, and horsepower. The <code>monotonicity_indicator</code> corrsponding to these features are set to -1, since the relationship is a monotonically decreasing one with respect to the dependant variable.</p> <p>References:</p> <ol> <li>Quinlan,R. (1993). Combining Instance-Based and Model-Based     Learning. In Proceedings on the Tenth International Conference of     Machine Learning, 236-243, University of Massachusetts, Amherst.     Morgan Kaufmann.</li> </ol> <p>https://archive.ics.uci.edu/ml/datasets/auto+mpg</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>auto_train_df, auto_test_df = get_train_n_test_data( data_path=data_path, dataset_name=\u201cauto\u201d ) display(auto_train_df)</p> <p>auto_train_ds = ( df2ds(auto_train_df).repeat(10).shuffle(10 * auto_train_df.shape[0]).batch(16) ) auto_test_ds = df2ds(auto_test_df).batch(16)</p> <p>def build_auto_model_f( **kwargs, ) -&gt; Model: monotonicity_indicator = { \u201cCylinders\u201d: 0, \u201cDisplacement\u201d: -1, \u201cHorsepower\u201d: -1, \u201cWeight\u201d: -1, \u201cAcceleration\u201d: 0, \u201cModel_Year\u201d: 0, \u201cOrigin\u201d: 0, }</p> <pre><code>metrics = \"mse\"\nloss = \"mse\"\n\nreturn build_mono_model_f(\n    monotonicity_indicator=monotonicity_indicator,\n    metrics=metrics,\n    loss=loss,\n    train_ds=auto_train_ds,\n    **kwargs,\n)\n</code></pre> <p>auto_model = build_auto_model_f( units=16, n_layers=3, activation=\u201crelu\u201d, dropout=0.1, weight_decay=0.1, learning_rate=0.1, decay_rate=0.8, ) auto_model.summary() auto_model.fit( auto_train_ds, validation_data=auto_test_ds, epochs=1, )</p> <p>def build_auto_model(hp) -&gt; Model: return build_auto_model_f( units=hp.Int(\u201cunits\u201d, min_value=8, max_value=32, step=1), n_layers=hp.Int(\u201cn_layers\u201d, min_value=1, max_value=4), activation=hp.Choice(\u201cactivation\u201d, values=[\u201celu\u201d]), learning_rate=hp.Float( \u201clearning_rate\u201d, min_value=1e-2, max_value=0.3, sampling=\u201clog\u201d ), weight_decay=hp.Float( \u201cweight_decay\u201d, min_value=1e-2, max_value=0.3, sampling=\u201clog\u201d ), dropout=hp.Float(\u201cdropout\u201d, min_value=0.0, max_value=0.25, sampling=\u201clinear\u201d), decay_rate=hp.Float( \u201cdecay_rate\u201d, min_value=0.1, max_value=1.0, sampling=\u201creverse_log\u201d ), )</p> <p>def get_auto_tuner_search_kwargs(build_auto_model, *, max_trials, executions_per_trial): auto_tuner_search_kwargs = dict( build_model_f=build_auto_model, tuner_name=\u201cBayesianOptimization\u201d, train_ds=auto_train_ds, test_ds=auto_test_ds, objective=Objective(\u201cval_mse\u201d, direction=\u201cmin\u201d), max_epochs=5, executions_per_trial=executions_per_trial, dir_root=\u201c/tmp/tuner/auto_tuner\u201d, project_name=\u201cauto_tuner\u201d, max_trials=max_trials, ) return auto_tuner_search_kwargs</p> <p>if should_find_hyperparam[\u201cauto\u201d]: auto_tuner = find_hyperparameters( **get_auto_tuner_search_kwargs( build_auto_model, max_trials=100, executions_per_trial=5 ) )genesis8</p> <p>if should_find_hyperparam[\u201cauto\u201d]: auto_stats = create_tuner_stats( auto_tuner, epochs=5, num_runs=10, num_models=10, train_ds=auto_train_ds, test_ds=auto_test_ds, )</p> <p>def final_build_auto_model(hp) -&gt; Model: return build_auto_model_f( units=hp.Fixed(\u201cunits\u201d, 16), n_layers=hp.Fixed(\u201cn_layers\u201d, 2), activation=hp.Fixed(\u201cactivation\u201d, \u201celu\u201d), learning_rate=hp.Fixed(\u201clearning_rate\u201d, 0.124332), weight_decay=hp.Fixed(\u201cweight_decay\u201d, 0.026992), dropout=hp.Fixed(\u201cdropout\u201d, 0.032543), decay_rate=hp.Fixed(\u201cdecay_rate\u201d, 0.303206), )</p> <p>final_auto_tuner = find_hyperparameters( **get_auto_tuner_search_kwargs( final_build_auto_model, max_trials=1, executions_per_trial=1 ) ) final_auto_stats = create_tuner_stats( final_auto_tuner, epochs=5, num_runs=10, num_models=1, train_ds=auto_train_ds, test_ds=auto_test_ds, )</p>"},{"location":"Experiments/#experiment-for-heart-disease-dataset-1","title":"Experiment for Heart Disease Dataset [1]","text":"<p>Heart Disease [1] is a classification dataset used for predicting the presence of heart disease with 13 features (age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal) and monotonically increasing with respect to features- trestbps and cholestrol (chol). The <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li>John H. Gennari, Pat Langley, and Douglas H. Fisher. Models of     incremental concept formation. Artif. Intell., 40(1-3):11\u201361, 1989.</li> </ol> <p>https://archive.ics.uci.edu/ml/datasets/heart+disease</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>heart_train_df, heart_test_df = get_train_n_test_data( data_path=data_path, dataset_name=\u201cheart\u201d ) display(heart_train_df)</p> <p>heart_train_ds = ( df2ds(heart_train_df).repeat(10).shuffle(10 * heart_train_df.shape[0]).batch(16) ) heart_test_ds = df2ds(heart_test_df).batch(16)</p>"},{"location":"Experiments/#peekheart_train_ds-lenheart_train_ds","title":"peek(heart_train_ds), len(heart_train_ds)","text":"<p>def build_heart_model_f( **kwargs, ) -&gt; Model: monotonicity_indicator = { \u201cage\u201d: 0, \u201csex\u201d: 0, \u201ccp\u201d: 0, \u201ctrestbps\u201d: 1, \u201cchol\u201d: 1, \u201cfbs\u201d: 0, \u201crestecg\u201d: 0, \u201cthalach\u201d: 0, \u201cexang\u201d: 0, \u201coldpeak\u201d: 0, \u201cslope\u201d: 0, \u201cca\u201d: 0, \u201cthal\u201d: 0, }</p> <pre><code>metrics = \"accuracy\"\nloss = \"binary_crossentropy\"\n\nreturn build_mono_model_f(\n    monotonicity_indicator=monotonicity_indicator,\n    metrics=metrics,\n    loss=loss,\n    final_activation=\"sigmoid\",\n    train_ds=heart_train_ds,\n    **kwargs,\n)\n</code></pre> <p>heart_model = build_heart_model_f( units=16, n_layers=3, activation=\u201crelu\u201d, dropout=0.1, weight_decay=0.1, learning_rate=0.1, decay_rate=0.8, ) heart_model.summary() heart_model.fit( heart_train_ds, validation_data=heart_test_ds, epochs=1, )</p> <p>def build_heart_model(hp) -&gt; Model: return build_heart_model_f( units=hp.Int(\u201cunits\u201d, min_value=12, max_value=24, step=1), n_layers=hp.Int(\u201cn_layers\u201d, min_value=2, max_value=3), activation=hp.Choice(\u201cactivation\u201d, values=[\u201celu\u201d]), learning_rate=hp.Float( \u201clearning_rate\u201d, min_value=1e-2, max_value=0.3, sampling=\u201clog\u201d ), weight_decay=hp.Float( \u201cweight_decay\u201d, min_value=1e-2, max_value=0.3, sampling=\u201clog\u201d ), dropout=hp.Float(\u201cdropout\u201d, min_value=0.0, max_value=0.5, sampling=\u201clinear\u201d), decay_rate=hp.Float( \u201cdecay_rate\u201d, min_value=0.1, max_value=1.0, sampling=\u201creverse_log\u201d ), )</p> <p>def get_heart_tuner_search_kwargs( build_heart_model, *, max_trials, executions_per_trial ): heart_tuner_search_kwargs = dict( build_model_f=build_heart_model, tuner_name=\u201cBayesianOptimization\u201d, train_ds=heart_train_ds, test_ds=heart_test_ds, objective=Objective(\u201cval_accuracy\u201d, direction=\u201cmax\u201d), max_epochs=10, executions_per_trial=executions_per_trial, dir_root=\u201c/tmp/tuner/heart_tuner\u201d, project_name=\u201cheart_tuner\u201d, max_trials=max_trials, ) return heart_tuner_search_kwargs</p> <p>heart_tuner = find_hyperparameters( **get_heart_tuner_search_kwargs( build_heart_model, max_trials=100, executions_per_trial=5 ) )</p> <p>create_tuner_stats( heart_tuner, epochs=5, num_runs=10, num_models=10, train_ds=heart_train_ds, test_ds=heart_test_ds, )</p> <p>def final_build_heart_model(hp) -&gt; Model: return build_heart_model_f( units=hp.Fixed(\u201cunits\u201d, 16), n_layers=hp.Fixed(\u201cn_layers\u201d, 2), activation=hp.Fixed(\u201cactivation\u201d, \u201celu\u201d), learning_rate=hp.Fixed(\u201clearning_rate\u201d, 0.06543), weight_decay=hp.Fixed(\u201cweight_decay\u201d, 0.01), dropout=hp.Fixed(\u201cdropout\u201d, 0.25), decay_rate=hp.Fixed(\u201cdecay_rate\u201d, 0.99983), )</p> <p>final_heart_tuner = find_hyperparameters( **get_heart_tuner_search_kwargs( final_build_heart_model, max_trials=1, executions_per_trial=1 ) ) create_tuner_stats( final_heart_tuner, epochs=5, num_runs=10, num_models=1, train_ds=heart_train_ds, test_ds=heart_test_ds, )</p> <p></p> <p>The figure above shows the table from our paper for reference. As can be seen from our experiments above, our proposed methodology performs comparable to or better than state-of-the-art</p>"},{"location":"Experiments/#comparison-with-methods-and-datasets-from-certified-monotonic-network-1-reference-20-in-our-paper","title":"Comparison with methods and datasets from Certified Monotonic Network [1] (Reference #20 in our paper)","text":"<p>References:</p> <ol> <li>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</li> </ol>"},{"location":"Experiments/#experiment-for-compas-dataset-1","title":"Experiment for Compas Dataset [1]","text":"<p>COMPAS [1] is a dataset containing the criminal records of 6,172 individuals arrested in Florida. The task is to predict whether the individual will commit a crime again in 2 years. The probability predicted by the system will be used as a risk score. As mentioned in [2] 13 attributes for prediction. The risk score should be monotonically increasing w.r.t. four attributes, number of prior adult convictions, number of juvenile felony, number of juvenile misdemeanor, and number of other convictions. The <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li> <p>S. Mattu J. Angwin, J. Larson and L. Kirchner. Machine bias: There\u2019s     software used across the country to predict future criminals. and     it\u2019s biased against blacks. ProPublica, 2016.</p> </li> <li> <p>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</p> </li> </ol> <p>compas_train_df, compas_test_df = get_train_n_test_data( data_path=data_path, dataset_name=\u201ccompas\u201d ) display(compas_train_df)</p>"},{"location":"Experiments/#compas_train_ds-df2dscompas_train_dfrepeat10shuffle10-compas_train_dfshape0batch16","title":"compas_train_ds = df2ds(compas_train_df).repeat(10).shuffle(10 * compas_train_df.shape[0]).batch(16)","text":""},{"location":"Experiments/#compas_test_ds-df2dscompas_test_dfbatch16","title":"compas_test_ds = df2ds(compas_test_df).batch(16)","text":"<p>compas_train_ds = df2ds(compas_train_df).shuffle(compas_train_df.shape[0]).batch(16) compas_test_ds = df2ds(compas_test_df).batch(16)</p> <p>peek(compas_train_ds), len(compas_train_ds)</p> <p>def build_compas_model_f( **kwargs, ) -&gt; Model: monotonicity_indicator = { \u201cpriors_count\u201d: 1, \u201cjuv_fel_count\u201d: 1, \u201cjuv_misd_count\u201d: 1, \u201cjuv_other_count\u201d: 1, \u201cage\u201d: 0, \u201crace_0\u201d: 0, \u201crace_1\u201d: 0, \u201crace_2\u201d: 0, \u201crace_3\u201d: 0, \u201crace_4\u201d: 0, \u201crace_5\u201d: 0, \u201csex_0\u201d: 0, \u201csex_1\u201d: 0, }</p> <pre><code>metrics = \"accuracy\"\nloss = \"binary_crossentropy\"\n\nreturn build_mono_model_f(\n    monotonicity_indicator=monotonicity_indicator,\n    metrics=metrics,\n    loss=loss,\n    final_activation=\"sigmoid\",\n    train_ds=compas_train_ds,\n    **kwargs,\n)\n</code></pre> <p>compas_model = build_compas_model_f( units=16, n_layers=3, activation=\u201crelu\u201d, dropout=0.1, weight_decay=0.1, learning_rate=0.1, decay_rate=0.8, ) compas_model.summary()</p> <p>compas_model.fit( compas_train_ds, validation_data=compas_test_ds, epochs=2, )</p> <p>def build_compas_model(hp) -&gt; Model: return build_compas_model_f( units=hp.Int(\u201cunits\u201d, min_value=8, max_value=32, step=1), n_layers=hp.Int(\u201cn_layers\u201d, min_value=1, max_value=3), activation=hp.Choice(\u201cactivation\u201d, values=[\u201celu\u201d]), learning_rate=hp.Float( \u201clearning_rate\u201d, min_value=1e-2, max_value=0.3, sampling=\u201clog\u201d ), weight_decay=hp.Float( \u201cweight_decay\u201d, min_value=1e-2, max_value=0.3, sampling=\u201clog\u201d ), dropout=hp.Float(\u201cdropout\u201d, min_value=0.0, max_value=0.5, sampling=\u201clinear\u201d), decay_rate=hp.Float( \u201cdecay_rate\u201d, min_value=0.1, max_value=1.0, sampling=\u201creverse_log\u201d ), )</p> <p>def get_compas_tuner_search_kwargs( build_compas_model, *, max_trials, executions_per_trial ): compas_tuner_search_kwargs = dict( build_model_f=build_compas_model, tuner_name=\u201cBayesianOptimization\u201d, train_ds=compas_train_ds, test_ds=compas_test_ds, objective=Objective(\u201cval_accuracy\u201d, direction=\u201cmax\u201d), max_epochs=20, executions_per_trial=executions_per_trial, dir_root=\u201c/tmp/tuner/compas_tuner\u201d, project_name=\u201ccompas_tuner\u201d, max_trials=max_trials, ) return compas_tuner_search_kwargs</p> <p>compas_tuner = find_hyperparameters( **get_compas_tuner_search_kwargs( build_compas_model, max_trials=100, executions_per_trial=5 ) )</p>"},{"location":"Experiments/#experiment-for-blog-dataset-1","title":"Experiment for Blog Dataset [1]","text":"<p>Blog Feedback [1] is a dataset containing 54,270 data points from blog posts. The raw HTML-documents of the blog posts were crawled and processed. The prediction task associated with the data is the prediction of the number of comments in the upcoming 24 hours. The feature of the dataset has 276 dimensions, and 8 attributes among them should be monotonically non-decreasing with the prediction. They are A51, A52, A53, A54, A56, A57, A58, A59. Thus the <code>monotonicity_indicator</code> corrsponding to these features are set to 1. As done in [2], we only use the data points with targets smaller than the 90th percentile.</p> <p>References:</p> <ol> <li>Krisztian Buza. Feedback prediction for blogs. In Data analysis,     machine learning and knowledge discovery, pages 145\u2013152. Springer,     2014</li> <li>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</li> </ol> <p>tf.keras.utils.set_random_seed(42)</p> <p>monotonicity_indicator = np.zeros((276)) monotonicity_indicator[50:54] = 1.0 monotonicity_indicator[55:59] = 1.0</p>"},{"location":"Experiments/#convexity_indicator-none","title":"convexity_indicator = None","text":"<p>train_params = dict( batch_size=256, num_epochs=100, units=4, n_layers=2, activation=\u201celu\u201d, loss=\u201cmean_squared_error\u201d, metrics=tf.keras.metrics.RootMeanSquaredError(), learning_rate=0.01, is_classification=False, )</p> <p>history, monotonic_model = train_dataset( dataset_name=\u201cblog\u201d, monotonicity_indicator=monotonicity_indicator, # convexity_indicator=convexity_indicator, train_params=train_params, )</p>"},{"location":"Experiments/#experiment-for-loan-dataset-1","title":"Experiment for Loan Dataset [1]","text":"<p>Lending club loan data contains complete loan data for all loans issued through 2007-2015 of several banks. Each data point is a 28-dimensional feature including the current loan status, latest payment information, and other additional features. The task is to predict loan defaulters given the feature vector. The possibility of loan default should be nondecreasing w.r.t. number of public record bankruptcies, Debt-to-Income ratio, and non-increasing w.r.t. credit score, length of employment, annual income. Thus the <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li>https://www.kaggle.com/wendykan/lending-club-loan-data (Note:     Currently, the dataset seems to be withdrawn from kaggle)</li> </ol> <p>tf.keras.utils.set_random_seed(42)</p>"},{"location":"Experiments/#monotonicity_indicator-nparray-1-1-1-1-1-0-0-0-0-0-0-0-0-0-0-0-0","title":"monotonicity_indicator = np.array([-1, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,","text":""},{"location":"Experiments/#0-0-0-0-0-0-0-0-0-0-0","title":"0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","text":"<p>monotonicity_indicator = np.array([-1, 1, -1, -1, 1] + [0] * 24)</p> <p>convexity_indicator = None</p> <p>train_params = dict( batch_size=256, num_epochs=20, units=4, n_layers=1, activation=\u201celu\u201d, loss=\u201cbinary_crossentropy\u201d, metrics=\u201caccuracy\u201d, learning_rate=0.008, is_classification=True, )</p> <p>history, monotonic_model = train_dataset( dataset_name=\u201cloan\u201d, monotonicity_indicator=monotonicity_indicator, # convexity_indicator=convexity_indicator, train_params=train_params, )</p> <p>The figure above shows the table from our paper for reference. As can be seen from our experiments above, our proposed methodology performs comparable to or better than state-of-the-art</p> <p></p>"},{"location":"Helpers/","title":"Helpers","text":"<p>source</p>"},{"location":"Helpers/#export","title":"export","text":"<pre><code> export (o:~T)\n</code></pre> <pre><code>@export\ndef f():\n    pass\n\nassert f.__module__ == \"mono_dense_keras\"\n</code></pre>"},{"location":"MonoDenseLayer/","title":"Monotonic dense layer","text":""},{"location":"MonoDenseLayer/#imports","title":"Imports","text":"<pre><code>from pathlib import Path\nfrom os import environ\n\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport pytest\nimport seaborn as sns\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras import Model\n</code></pre> <pre><code>environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n</code></pre>"},{"location":"MonoDenseLayer/#monotonic-dense-layer_1","title":"Monotonic Dense Layer","text":""},{"location":"MonoDenseLayer/#actvation-functions","title":"Actvation Functions","text":"<p>We use \\(\\breve{\\mathcal{A}}\\) to denote the set of all zero-centred, monotonically increasing, convex, lower-bounded functions.</p> <p>Let \\(\\breve{\\rho} \\in \\breve{\\mathcal{A}}\\). Then</p> <p>In the code below, the following names are used for denotation of the above functions:</p> <ul> <li> <p><code>convex_activation</code> denotes \\(\\breve{\\rho}\\),</p> </li> <li> <p><code>concave_activation</code> denotes \\(\\hat{\\rho}\\), and</p> </li> <li> <p><code>saturated_activation</code> denotes \\(\\tilde{\\rho}\\).</p> </li> </ul> <p>source</p>"},{"location":"MonoDenseLayer/#get_activation_functions","title":"get_activation_functions","text":"<pre><code> get_activation_functions (activation:Union[str,Callable[[Union[tensorflow\n                           .python.types.core.Tensor,tensorflow.python.typ\n                           es.core.TensorProtocol,int,float,bool,str,bytes\n                           ,complex,tuple,list,numpy.ndarray,numpy.generic\n                           ]],Union[tensorflow.python.types.core.Tensor,te\n                           nsorflow.python.types.core.TensorProtocol,int,f\n                           loat,bool,str,bytes,complex,tuple,list,numpy.nd\n                           array,numpy.generic]],NoneType]=None)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#get_saturated_activation","title":"get_saturated_activation","text":"<pre><code> get_saturated_activation (convex_activation:Callable[[Union[tensorflow.py\n                           thon.types.core.Tensor,tensorflow.python.types.\n                           core.TensorProtocol,int,float,bool,str,bytes,co\n                           mplex,tuple,list,numpy.ndarray,numpy.generic]],\n                           Union[tensorflow.python.types.core.Tensor,tenso\n                           rflow.python.types.core.TensorProtocol,int,floa\n                           t,bool,str,bytes,complex,tuple,list,numpy.ndarr\n                           ay,numpy.generic]], concave_activation:Callable\n                           [[Union[tensorflow.python.types.core.Tensor,ten\n                           sorflow.python.types.core.TensorProtocol,int,fl\n                           oat,bool,str,bytes,complex,tuple,list,numpy.nda\n                           rray,numpy.generic]],Union[tensorflow.python.ty\n                           pes.core.Tensor,tensorflow.python.types.core.Te\n                           nsorProtocol,int,float,bool,str,bytes,complex,t\n                           uple,list,numpy.ndarray,numpy.generic]],\n                           a:float=1.0, c:float=1.0)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_activations","title":"apply_activations","text":"<pre><code> apply_activations (x:Union[tensorflow.python.types.core.Tensor,tensorflow\n                    .python.types.core.TensorProtocol,int,float,bool,str,b\n                    ytes,complex,tuple,list,numpy.ndarray,numpy.generic],\n                    units:int, convex_activation:Callable[[Union[tensorflo\n                    w.python.types.core.Tensor,tensorflow.python.types.cor\n                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl\n                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.\n                    python.types.core.Tensor,tensorflow.python.types.core.\n                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                    list,numpy.ndarray,numpy.generic]], concave_activation\n                    :Callable[[Union[tensorflow.python.types.core.Tensor,t\n                    ensorflow.python.types.core.TensorProtocol,int,float,b\n                    ool,str,bytes,complex,tuple,list,numpy.ndarray,numpy.g\n                    eneric]],Union[tensorflow.python.types.core.Tensor,ten\n                    sorflow.python.types.core.TensorProtocol,int,float,boo\n                    l,str,bytes,complex,tuple,list,numpy.ndarray,numpy.gen\n                    eric]], saturated_activation:Callable[[Union[tensorflo\n                    w.python.types.core.Tensor,tensorflow.python.types.cor\n                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl\n                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.\n                    python.types.core.Tensor,tensorflow.python.types.core.\n                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                    list,numpy.ndarray,numpy.generic]],\n                    is_convex:bool=False, is_concave:bool=False,\n                    activation_weights:Tuple[float,float,float]=(7.0, 7.0,\n                    2.0))\n</code></pre> <pre><code>def plot_applied_activation(\n    activation: str = \"relu\",\n    *,\n    save_pdf: bool = False,\n    save_path: Union[Path, str] = \"plots\",\n    font_size: int = 20,\n    linestyle=\"--\",\n    alpha=0.7,\n    linewidth=2.0,\n):\n    font = {\"size\": font_size}\n    matplotlib.rc(\"font\", **font)\n    plt.rcParams[\"figure.figsize\"] = (18, 3)\n\n    x = np.arange(-1.5, 1.5, step=3 / 256)\n    h = 3 * np.sin(2 * np.pi * x)\n\n    (\n        convex_activation,\n        concave_activation,\n        saturated_activation,\n    ) = get_activation_functions(activation)\n\n    y = apply_activations(\n        h,\n        convex_activation=convex_activation,\n        concave_activation=concave_activation,\n        saturated_activation=saturated_activation,\n        units=x.shape[0],\n        activation_weights=(1.0, 1.0, 1.0),\n    )\n\n    plot_kwargs = dict(linestyle=linestyle, alpha=alpha, linewidth=linewidth)\n\n    plt.plot(np.arange(x.shape[0]), h, label=\"$h$\", **plot_kwargs)\n    plt.plot(np.arange(x.shape[0]), y, label=r\"${\\rho}(h)$\", **plot_kwargs)\n    title = (\n        \"Applying \"\n        + (activation.__name__ if hasattr(activation, \"__name__\") else activation)\n        + f\"-based activations to {x.shape[0]}-dimensional vector\"\n        + r\" $h$\"\n    )\n    plt.title(title)\n\n    plt.legend()\n\n    if save_pdf:\n        path = Path(save_path) / (title.replace(\" \", \"_\") + \".pdf\")\n        path.parent.mkdir(exist_ok=True, parents=True)\n        plt.savefig(path, format=\"pdf\")\n    #         print(f\"Saved figure to: {path}\")\n\n    plt.show()\n</code></pre> <pre><code>for activation in [\"linear\", \"ReLU\", \"ELU\", \"SELU\"]:\n    plot_applied_activation(activation, save_pdf=True)\n</code></pre>"},{"location":"MonoDenseLayer/#monotonicity-indicator","title":"Monotonicity indicator","text":"<p>source</p>"},{"location":"MonoDenseLayer/#get_monotonicity_indicator","title":"get_monotonicity_indicator","text":"<pre><code> get_monotonicity_indicator (monotonicity_indicator:Union[numpy.__array_li\n                             ke._SupportsArray[numpy.dtype],numpy.__nested\n                             _sequence._NestedSequence[numpy.__array_like.\n                             _SupportsArray[numpy.dtype]],bool,int,float,c\n                             omplex,str,bytes,numpy.__nested_sequence._Nes\n                             tedSequence[Union[bool,int,float,complex,str,\n                             bytes]]], input_shape:Tuple[int,...],\n                             units:int)\n</code></pre> <pre><code>input_shape = (13, 2)\nunits = 3\n\nlayer = Dense(units=units)\nlayer.build(input_shape=input_shape)\n\nfor monotonicity_indicator in [\n    1,\n    [1],\n    [1, 1],\n    np.ones((2,)),\n    np.ones((2, 1)),\n    np.ones((2, 3)),\n]:\n    expected = np.ones((2, 3))\n    actual = get_monotonicity_indicator(\n        monotonicity_indicator, input_shape=(13, 2), units=3\n    )\n\n    # rank is 2\n    assert len(actual.shape) == 2\n    # it is broadcastable to the kernel shape of (input_shape[-1], units)\n    np.testing.assert_array_equal(np.broadcast_to(actual, (2, 3)), expected)\n</code></pre> <pre><code>expected = [[1], [0], [-1]]\nactual = get_monotonicity_indicator([1, 0, -1], input_shape=(13, 3), units=4)\nnp.testing.assert_array_equal(actual, expected)\n</code></pre> <pre><code>with pytest.raises(ValueError) as e:\n    get_monotonicity_indicator([0, 1, -1], input_shape=(13, 2), units=3)\nassert e.value.args == (\n    \"operands could not be broadcast together with remapped shapes [original-&gt;remapped]: (3,1)  and requested shape (2,3)\",\n)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#replace_kernel_using_monotonicity_indicator","title":"replace_kernel_using_monotonicity_indicator","text":"<pre><code> replace_kernel_using_monotonicity_indicator\n                                              (layer:keras.layers.core.den\n                                              se.Dense, monotonicity_indic\n                                              ator:Union[tensorflow.python\n                                              .types.core.Tensor,tensorflo\n                                              w.python.types.core.TensorPr\n                                              otocol,int,float,bool,str,by\n                                              tes,complex,tuple,list,numpy\n                                              .ndarray,numpy.generic])\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_monotonicity_indicator_to_kernel","title":"apply_monotonicity_indicator_to_kernel","text":"<pre><code> apply_monotonicity_indicator_to_kernel\n                                         (kernel:tensorflow.python.ops.var\n                                         iables.Variable, monotonicity_ind\n                                         icator:Union[numpy.__array_like._\n                                         SupportsArray[numpy.dtype],numpy.\n                                         __nested_sequence._NestedSequence\n                                         [numpy.__array_like._SupportsArra\n                                         y[numpy.dtype]],bool,int,float,co\n                                         mplex,str,bytes,numpy.__nested_se\n                                         quence._NestedSequence[Union[bool\n                                         ,int,float,complex,str,bytes]]])\n</code></pre> <pre><code>def display_kernel(kernel: Union[tf.Variable, np.typing.NDArray[float]]) -&gt; None:\n    cm = sns.color_palette(\"coolwarm_r\", as_cmap=True)\n\n    df = pd.DataFrame(kernel)\n\n    display(df.style.format(\"{:.2f}\").background_gradient(cmap=cm, vmin=-1e-8, vmax=1e-8))\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\n\nunits = 18\ninput_len = 7\n\nlayer = tf.keras.layers.Dense(units=units)\n\ninput_shape = (input_len,)\nlayer.build(input_shape=input_shape)\n\nprint(\"Original kernel:\")\ndisplay_kernel(layer.kernel)\n\nprint(\"Kernel after applying monotocity indicator 1 for all values:\")\nmonotonicity_indicator = get_monotonicity_indicator(\n    1, input_shape=input_shape, units=units\n)\nwith replace_kernel_using_monotonicity_indicator(layer, monotonicity_indicator):\n    display_kernel(layer.kernel)\n</code></pre> <pre><code>Original kernel:\nKernel after applying monotocity indicator 1 for all values:\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 -0.14 0.44 -0.41 0.15 0.46 -0.33 0.02 0.13 -0.41 -0.05 0.46 -0.03 0.00 0.26 -0.47 -0.30 1 0.01 -0.42 -0.45 0.34 0.41 -0.23 0.35 -0.36 -0.04 0.06 0.07 -0.29 -0.28 0.48 -0.38 -0.06 -0.23 -0.37 2 0.23 -0.31 0.18 0.15 -0.45 0.06 -0.16 -0.11 0.45 -0.09 0.03 -0.24 -0.37 0.21 0.11 0.01 -0.46 -0.37 3 0.29 0.36 -0.07 -0.18 -0.46 -0.45 0.25 0.32 -0.12 0.22 -0.18 0.27 -0.18 -0.07 0.35 0.32 0.18 0.39 4 0.35 -0.27 0.13 -0.40 0.44 0.21 0.06 -0.31 -0.30 0.46 -0.44 -0.18 -0.26 -0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 -0.02 -0.36 0.39 -0.13 0.30 0.35 -0.12 -0.43 0.44 0.32 0.06 -0.30 -0.29 0.24 -0.44 -0.13 6 0.38 -0.04 -0.30 0.17 -0.03 0.37 -0.03 -0.18 0.42 -0.39 -0.33 -0.19 0.02 -0.41 -0.44 0.42 0.38 -0.21 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 0.14 0.44 0.41 0.15 0.46 0.33 0.02 0.13 0.41 0.05 0.46 0.03 0.00 0.26 0.47 0.30 1 0.01 0.42 0.45 0.34 0.41 0.23 0.35 0.36 0.04 0.06 0.07 0.29 0.28 0.48 0.38 0.06 0.23 0.37 2 0.23 0.31 0.18 0.15 0.45 0.06 0.16 0.11 0.45 0.09 0.03 0.24 0.37 0.21 0.11 0.01 0.46 0.37 3 0.29 0.36 0.07 0.18 0.46 0.45 0.25 0.32 0.12 0.22 0.18 0.27 0.18 0.07 0.35 0.32 0.18 0.39 4 0.35 0.27 0.13 0.40 0.44 0.21 0.06 0.31 0.30 0.46 0.44 0.18 0.26 0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 0.02 0.36 0.39 0.13 0.30 0.35 0.12 0.43 0.44 0.32 0.06 0.30 0.29 0.24 0.44 0.13 6 0.38 0.04 0.30 0.17 0.03 0.37 0.03 0.18 0.42 0.39 0.33 0.19 0.02 0.41 0.44 0.42 0.38 0.21 <pre><code>monotonicity_indicator = [1] * 2 + [-1] * 2 + [0] * (input_shape[0] - 4)\nmonotonicity_indicator = get_monotonicity_indicator(\n    monotonicity_indicator, input_shape=input_shape, units=units\n)\n\nprint(\"Monotocity indicator:\")\ndisplay_kernel(monotonicity_indicator)\n\nprint(\"Kernel after applying the monotocity indicator:\")\nwith replace_kernel_using_monotonicity_indicator(\n    layer, monotonicity_indicator\n):\n    display_kernel(layer.kernel)\n</code></pre> <pre><code>Monotocity indicator:\nKernel after applying the monotocity indicator:\n</code></pre> 0 0 1.00 1 1.00 2 -1.00 3 -1.00 4 0.00 5 0.00 6 0.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.35 0.16 0.14 0.44 0.41 0.15 0.46 0.33 0.02 0.13 0.41 0.05 0.46 0.03 0.00 0.26 0.47 0.30 1 0.01 0.42 0.45 0.34 0.41 0.23 0.35 0.36 0.04 0.06 0.07 0.29 0.28 0.48 0.38 0.06 0.23 0.37 2 -0.23 -0.31 -0.18 -0.15 -0.45 -0.06 -0.16 -0.11 -0.45 -0.09 -0.03 -0.24 -0.37 -0.21 -0.11 -0.01 -0.46 -0.37 3 -0.29 -0.36 -0.07 -0.18 -0.46 -0.45 -0.25 -0.32 -0.12 -0.22 -0.18 -0.27 -0.18 -0.07 -0.35 -0.32 -0.18 -0.39 4 0.35 -0.27 0.13 -0.40 0.44 0.21 0.06 -0.31 -0.30 0.46 -0.44 -0.18 -0.26 -0.34 0.36 0.33 0.12 0.04 5 0.04 0.21 -0.02 -0.36 0.39 -0.13 0.30 0.35 -0.12 -0.43 0.44 0.32 0.06 -0.30 -0.29 0.24 -0.44 -0.13 6 0.38 -0.04 -0.30 0.17 -0.03 0.37 -0.03 -0.18 0.42 -0.39 -0.33 -0.19 0.02 -0.41 -0.44 0.42 0.38 -0.21"},{"location":"MonoDenseLayer/#monotonic-dense-layer_2","title":"Monotonic Dense Layer","text":"<p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p>"},{"location":"MonoDenseLayer/#monodense","title":"MonoDense","text":"<pre><code> MonoDense (units:int, activation:Union[str,Callable[[Union[tensorflow.pyt\n            hon.types.core.Tensor,tensorflow.python.types.core.TensorProto\n            col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndarray,\n            numpy.generic]],Union[tensorflow.python.types.core.Tensor,tens\n            orflow.python.types.core.TensorProtocol,int,float,bool,str,byt\n            es,complex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=\n            None, monotonicity_indicator:Union[numpy.__array_like._Support\n            sArray[numpy.dtype],numpy.__nested_sequence._NestedSequence[nu\n            mpy.__array_like._SupportsArray[numpy.dtype]],bool,int,float,c\n            omplex,str,bytes,numpy.__nested_sequence._NestedSequence[Union\n            [bool,int,float,complex,str,bytes]]]=1, is_convex:bool=False,\n            is_concave:bool=False,\n            activation_weights:Tuple[float,float,float]=(7.0, 7.0, 2.0),\n            **kwargs:Dict[str,Any])\n</code></pre> <p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> <p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p> <pre><code>units = 18\nactivation = \"relu\"\nbatch_size = 9\nx_len = 11\n\nx = np.random.default_rng(42).normal(size=(batch_size, x_len))\n\ntf.keras.utils.set_random_seed(42)\n\nfor monotonicity_indicator in [[1]*4+[0]*4+[-1]*3, 1, np.ones((x_len,)), -1, -np.ones((x_len,))]:\n    print(\"*\"*120)    \n    mono_layer = MonoDense(\n        units=units,\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator,\n        activation_weights=(7, 7, 4),\n    )\n    print(\"input:\")\n    display_kernel(x)\n\n    y = mono_layer(x)\n    print(f\"monotonicity_indicator = {monotonicity_indicator}\")\n    display_kernel(mono_layer.monotonicity_indicator)\n\n    print(\"kernel:\")\n    with replace_kernel_using_monotonicity_indicator(\n        mono_layer, mono_layer.monotonicity_indicator\n    ):\n        display_kernel(mono_layer.kernel)\n\n    print(\"output:\")\n    display_kernel(y)\nprint(\"ok\")\n</code></pre> <pre><code>************************************************************************************************************************\ninput:\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function apply_activations at 0x7f519c188a60&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nmonotonicity_indicator = [1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = 1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = -1\nkernel:\noutput:\n************************************************************************************************************************\ninput:\nmonotonicity_indicator = [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\nkernel:\noutput:\nok\n</code></pre> 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 0.00 5 0.00 6 0.00 7 0.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.33 0.15 0.13 0.41 0.38 0.14 0.43 0.30 0.02 0.12 0.38 0.05 0.42 0.03 0.00 0.24 0.44 0.28 1 0.01 0.39 0.42 0.32 0.38 0.22 0.33 0.34 0.03 0.06 0.06 0.27 0.26 0.45 0.35 0.05 0.21 0.34 2 0.21 0.29 0.16 0.14 0.42 0.06 0.15 0.10 0.41 0.08 0.03 0.22 0.34 0.20 0.11 0.01 0.43 0.35 3 0.27 0.33 0.06 0.17 0.42 0.42 0.24 0.30 0.11 0.20 0.17 0.25 0.17 0.07 0.32 0.30 0.17 0.36 4 0.32 -0.25 0.12 -0.37 0.41 0.20 0.06 -0.28 -0.27 0.43 -0.41 -0.17 -0.24 -0.31 0.33 0.31 0.11 0.03 5 0.04 0.19 -0.02 -0.34 0.36 -0.12 0.28 0.32 -0.11 -0.40 0.41 0.30 0.06 -0.28 -0.27 0.23 -0.41 -0.12 6 0.35 -0.04 -0.28 0.16 -0.03 0.35 -0.03 -0.16 0.39 -0.36 -0.31 -0.18 0.02 -0.38 -0.40 0.39 0.35 -0.19 7 0.33 -0.34 0.11 -0.29 0.25 -0.21 0.11 0.08 -0.19 -0.39 0.01 0.10 0.39 -0.25 -0.37 -0.27 0.04 0.34 8 -0.27 -0.09 -0.02 -0.45 -0.16 -0.12 -0.09 -0.43 -0.36 -0.09 -0.23 -0.42 -0.28 -0.24 -0.30 -0.31 -0.07 -0.07 9 -0.38 -0.34 -0.44 -0.42 -0.32 -0.06 -0.27 -0.28 -0.22 -0.05 -0.08 -0.07 -0.21 -0.39 -0.01 -0.26 -0.24 -0.42 10 -0.09 -0.45 -0.41 -0.36 -0.19 -0.09 -0.00 -0.34 -0.17 -0.18 -0.05 -0.39 -0.06 -0.20 -0.40 -0.33 -0.18 -0.01 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.01 0.40 0.00 1.38 0.00 0.10 0.00 -0.00 -0.00 -0.13 -0.00 -0.26 -0.00 -0.00 -0.55 -0.52 0.79 0.64 1 0.45 1.02 0.96 0.71 1.22 0.00 0.86 -0.00 -0.00 -0.09 -0.00 -0.00 -0.00 -0.00 0.26 -0.17 0.54 1.00 2 0.30 0.00 0.33 0.00 0.41 0.00 0.42 -0.53 -0.89 -0.29 -0.23 -0.84 -0.16 -0.93 -0.90 0.08 0.37 0.08 3 0.21 0.26 0.33 0.42 0.00 0.00 0.00 -0.16 -0.00 -0.61 -0.53 -0.07 -0.00 -0.00 -0.55 -0.66 0.83 0.78 4 1.38 0.49 0.70 0.82 1.47 0.54 0.63 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.73 0.97 0.94 0.91 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.86 -0.25 -0.00 -1.57 -1.19 -0.61 -0.23 0.13 -1.00 0.50 -0.06 6 0.00 0.00 0.00 0.17 0.00 0.00 0.00 -0.15 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.06 -1.00 0.00 0.12 7 0.00 0.96 0.35 0.93 0.00 0.32 0.17 -0.00 -0.00 -0.00 -0.00 -0.00 -0.17 -0.00 0.67 0.06 0.12 0.17 8 0.00 1.33 0.92 1.63 0.52 0.00 0.66 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 0.23 0.18 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.44 0.02 0.24 0.22 0.29 0.35 0.18 0.03 0.39 0.17 0.25 0.02 0.10 0.13 0.00 0.42 0.21 0.31 1 0.35 0.06 0.26 0.42 0.05 0.41 0.16 0.33 0.03 0.26 0.11 0.03 0.23 0.04 0.37 0.27 0.32 0.40 2 0.37 0.30 0.36 0.14 0.21 0.40 0.01 0.28 0.16 0.44 0.43 0.23 0.27 0.22 0.23 0.25 0.43 0.05 3 0.32 0.25 0.05 0.45 0.08 0.18 0.26 0.24 0.34 0.07 0.07 0.14 0.04 0.19 0.29 0.23 0.43 0.09 4 0.36 0.05 0.20 0.41 0.38 0.29 0.01 0.44 0.17 0.04 0.31 0.34 0.29 0.16 0.25 0.18 0.01 0.28 5 0.34 0.31 0.38 0.34 0.08 0.40 0.15 0.16 0.14 0.25 0.15 0.20 0.10 0.06 0.44 0.19 0.42 0.21 6 0.01 0.38 0.43 0.18 0.00 0.43 0.45 0.28 0.25 0.18 0.03 0.26 0.22 0.26 0.08 0.23 0.45 0.42 7 0.04 0.12 0.28 0.17 0.11 0.00 0.15 0.24 0.05 0.05 0.27 0.32 0.33 0.11 0.09 0.40 0.19 0.06 8 0.30 0.17 0.21 0.42 0.21 0.29 0.19 0.38 0.03 0.34 0.32 0.30 0.34 0.15 0.28 0.11 0.44 0.19 9 0.10 0.10 0.35 0.32 0.24 0.28 0.30 0.28 0.10 0.12 0.30 0.41 0.15 0.00 0.10 0.40 0.18 0.24 10 0.00 0.22 0.21 0.09 0.10 0.13 0.18 0.37 0.24 0.29 0.25 0.23 0.32 0.14 0.27 0.34 0.25 0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.01 0.00 0.00 0.00 0.00 0.00 -0.93 -0.00 -0.07 -0.58 -0.88 -0.58 -0.00 -0.87 -0.49 -0.05 -1.00 1 0.73 0.10 0.22 0.18 0.18 0.16 0.00 -0.23 -0.00 -0.00 -0.00 -0.09 -0.00 -0.00 0.16 0.47 0.53 -0.27 2 1.15 0.36 0.82 1.20 0.80 1.06 0.61 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.53 0.61 1.00 0.94 3 0.00 0.45 0.28 0.00 0.00 0.11 0.14 -0.00 -0.21 -0.00 -0.00 -0.00 -0.00 -0.00 0.15 0.08 0.72 -0.08 4 0.34 0.19 0.36 0.05 0.15 0.30 0.00 -0.00 -0.00 -0.08 -0.00 -0.00 -0.00 -0.00 0.06 0.38 0.04 0.14 5 0.00 0.00 0.26 0.00 0.67 0.05 0.00 -0.00 -0.16 -0.00 -0.00 -0.00 -0.00 -0.00 -0.08 0.30 -0.17 -0.17 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.76 -0.68 -0.28 -0.11 -0.37 -0.42 -0.40 -0.88 -0.41 -0.67 -1.00 7 0.01 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.17 -0.04 -0.57 -0.82 -0.50 -0.22 -0.07 -0.62 -0.13 -0.18 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.32 -0.35 -0.39 -0.77 -1.63 -1.12 -0.60 -0.47 -0.99 -1.00 -1.00 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 1.00 1 1.00 2 1.00 3 1.00 4 1.00 5 1.00 6 1.00 7 1.00 8 1.00 9 1.00 10 1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.31 0.02 0.11 0.29 0.10 0.33 0.37 0.06 0.39 0.35 0.15 0.13 0.15 0.45 0.07 0.19 0.03 0.06 1 0.12 0.02 0.06 0.41 0.32 0.24 0.34 0.28 0.22 0.06 0.33 0.27 0.25 0.23 0.43 0.09 0.45 0.27 2 0.19 0.11 0.19 0.25 0.07 0.42 0.32 0.35 0.15 0.05 0.00 0.24 0.22 0.39 0.44 0.11 0.19 0.10 3 0.15 0.37 0.21 0.41 0.25 0.04 0.37 0.04 0.05 0.22 0.31 0.35 0.35 0.08 0.38 0.01 0.25 0.29 4 0.17 0.45 0.24 0.32 0.01 0.00 0.19 0.34 0.17 0.19 0.18 0.34 0.02 0.24 0.03 0.41 0.26 0.00 5 0.29 0.10 0.07 0.34 0.04 0.30 0.39 0.27 0.39 0.16 0.33 0.45 0.06 0.19 0.23 0.04 0.36 0.04 6 0.13 0.15 0.22 0.40 0.14 0.30 0.11 0.45 0.14 0.17 0.26 0.16 0.36 0.10 0.17 0.32 0.14 0.08 7 0.25 0.25 0.24 0.45 0.17 0.45 0.30 0.35 0.41 0.40 0.11 0.26 0.32 0.08 0.22 0.34 0.05 0.09 8 0.16 0.27 0.10 0.23 0.08 0.21 0.19 0.16 0.06 0.04 0.17 0.05 0.39 0.11 0.26 0.25 0.13 0.05 9 0.17 0.17 0.00 0.13 0.12 0.03 0.39 0.11 0.01 0.29 0.43 0.20 0.21 0.43 0.39 0.18 0.19 0.27 10 0.26 0.23 0.43 0.04 0.25 0.36 0.21 0.36 0.37 0.36 0.08 0.14 0.25 0.24 0.30 0.33 0.04 0.07 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.00 0.00 0.08 0.00 0.00 0.00 0.00 -0.82 -0.58 -0.32 -1.07 -1.09 -0.00 -0.63 -0.21 -0.74 -1.00 -0.15 1 0.36 0.00 0.00 0.51 0.11 0.72 0.76 -0.12 -0.00 -0.00 -0.05 -0.00 -0.00 -0.00 0.56 -0.34 0.13 0.22 2 0.72 0.68 0.32 1.10 0.10 0.84 0.68 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.20 0.97 0.33 -0.07 3 0.00 0.00 0.36 0.35 0.36 0.82 0.00 -0.00 -0.00 -0.19 -0.29 -0.13 -0.00 -0.20 0.67 0.20 -0.00 0.14 4 0.18 0.14 0.26 0.68 0.09 0.38 0.36 -0.00 -0.00 -0.00 -0.00 -0.00 -0.07 -0.00 0.14 0.15 0.33 0.10 5 0.01 0.55 0.50 0.00 0.00 0.21 0.00 -0.00 -0.27 -0.00 -0.44 -0.25 -0.00 -0.00 0.44 0.83 -0.24 -0.01 6 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.89 -0.85 -0.48 -0.77 -0.90 -0.21 -0.30 -0.09 -0.69 -0.83 -0.03 7 0.00 0.00 0.00 0.00 0.01 0.00 0.00 -0.79 -0.59 -0.65 -0.21 -0.55 -0.19 -0.37 -0.17 -0.71 -0.10 0.03 8 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.24 -0.48 -0.95 -1.13 -0.71 -1.40 -0.30 -0.76 -1.00 -0.47 -0.39 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.29 -0.12 -0.00 -0.17 -0.33 -0.17 -0.33 -0.36 -0.28 -0.16 -0.24 -0.22 -0.10 -0.13 -0.02 -0.38 -0.23 -0.02 1 -0.36 -0.13 -0.05 -0.07 -0.41 -0.30 -0.38 -0.06 -0.40 -0.42 -0.44 -0.03 -0.27 -0.03 -0.32 -0.31 -0.35 -0.40 2 -0.30 -0.07 -0.40 -0.06 -0.10 -0.21 -0.16 -0.22 -0.06 -0.36 -0.40 -0.42 -0.23 -0.22 -0.20 -0.33 -0.45 -0.06 3 -0.05 -0.08 -0.07 -0.30 -0.44 -0.23 -0.40 -0.25 -0.13 -0.31 -0.11 -0.13 -0.13 -0.34 -0.15 -0.05 -0.36 -0.13 4 -0.45 -0.34 -0.41 -0.39 -0.15 -0.10 -0.40 -0.32 -0.19 -0.13 -0.29 -0.39 -0.43 -0.29 -0.13 -0.05 -0.39 -0.01 5 -0.09 -0.38 -0.00 -0.12 -0.07 -0.42 -0.01 -0.12 -0.26 -0.28 -0.16 -0.06 -0.08 -0.43 -0.23 -0.28 -0.28 -0.07 6 -0.34 -0.38 -0.15 -0.44 -0.41 -0.19 -0.25 -0.41 -0.34 -0.22 -0.43 -0.36 -0.25 -0.28 -0.06 -0.12 -0.15 -0.16 7 -0.17 -0.39 -0.40 -0.26 -0.40 -0.20 -0.10 -0.14 -0.42 -0.21 -0.18 -0.25 -0.15 -0.21 -0.13 -0.41 -0.14 -0.14 8 -0.38 -0.03 -0.10 -0.21 -0.13 -0.04 -0.19 -0.00 -0.09 -0.38 -0.01 -0.27 -0.24 -0.24 -0.13 -0.18 -0.37 -0.21 9 -0.43 -0.08 -0.20 -0.29 -0.10 -0.27 -0.08 -0.43 -0.22 -0.37 -0.27 -0.24 -0.15 -0.22 -0.01 -0.45 -0.35 -0.31 10 -0.38 -0.44 -0.20 -0.31 -0.42 -0.23 -0.03 -0.31 -0.11 -0.35 -0.01 -0.00 -0.00 -0.39 -0.45 -0.14 -0.03 -0.10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 1.05 0.88 0.59 0.61 0.00 0.70 0.64 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.24 0.74 1.00 0.55 1 0.27 0.26 0.00 0.41 0.00 0.00 0.00 -0.00 -0.23 -0.33 -0.21 -0.20 -0.00 -0.02 -0.04 -0.82 -0.52 -0.02 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.36 -0.77 -0.71 -0.39 -1.00 -0.82 -0.67 -0.11 -0.74 -0.97 -0.31 3 0.00 0.00 0.00 0.00 0.00 0.01 0.00 -0.00 -0.15 -0.50 -0.38 -0.33 -0.20 -0.00 -0.39 -0.20 -0.12 -0.36 4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.45 -0.46 -0.00 -0.84 -0.48 -0.36 -0.13 -0.08 -0.28 -0.33 0.13 5 0.00 0.02 0.00 0.00 0.12 0.33 0.00 -0.41 -0.00 -0.44 -0.33 -0.90 -0.56 -0.04 -0.24 -0.27 -0.48 -0.16 6 0.74 1.20 0.11 0.90 0.84 0.65 0.87 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.60 0.01 0.53 0.12 7 0.47 0.89 0.91 0.62 0.26 0.37 0.01 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.07 0.61 0.29 0.01 8 1.30 1.17 0.98 1.61 1.09 0.59 0.65 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.09 0.93 0.94 0.81 0 1 2 3 4 5 6 7 8 9 10 0 0.30 -1.04 0.75 0.94 -1.95 -1.30 0.13 -0.32 -0.02 -0.85 0.88 1 0.78 0.07 1.13 0.47 -0.86 0.37 -0.96 0.88 -0.05 -0.18 -0.68 2 1.22 -0.15 -0.43 -0.35 0.53 0.37 0.41 0.43 2.14 -0.41 -0.51 3 -0.81 0.62 1.13 -0.11 -0.84 -0.82 0.65 0.74 0.54 -0.67 0.23 4 0.12 0.22 0.87 0.22 0.68 0.07 0.29 0.63 -1.46 -0.32 -0.47 5 -0.64 -0.28 1.49 -0.87 0.97 -1.68 -0.33 0.16 0.59 0.71 0.79 6 -0.35 -0.46 0.86 -0.19 -1.28 -1.13 -0.92 0.50 0.14 0.69 -0.43 7 0.16 0.63 -0.31 0.46 -0.66 -0.36 -0.38 -1.20 0.49 -0.47 0.01 8 0.48 0.45 0.67 -0.10 -0.42 -0.08 -1.69 -1.45 -1.32 -1.00 0.40 0 0 -1.00 1 -1.00 2 -1.00 3 -1.00 4 -1.00 5 -1.00 6 -1.00 7 -1.00 8 -1.00 9 -1.00 10 -1.00 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 -0.45 -0.28 -0.30 -0.41 -0.17 -0.39 -0.22 -0.45 -0.28 -0.40 -0.18 -0.20 -0.16 -0.18 -0.10 -0.13 -0.14 -0.35 1 -0.09 -0.27 -0.09 -0.14 -0.02 -0.36 -0.21 -0.05 -0.05 -0.01 -0.02 -0.45 -0.03 -0.09 -0.01 -0.05 -0.39 -0.05 2 -0.17 -0.15 -0.37 -0.35 -0.32 -0.03 -0.24 -0.31 -0.35 -0.41 -0.00 -0.37 -0.18 -0.26 -0.09 -0.44 -0.09 -0.17 3 -0.42 -0.17 -0.11 -0.31 -0.32 -0.11 -0.20 -0.10 -0.34 -0.15 -0.24 -0.22 -0.22 -0.08 -0.40 -0.02 -0.23 -0.38 4 -0.13 -0.17 -0.06 -0.13 -0.32 -0.42 -0.28 -0.44 -0.03 -0.26 -0.38 -0.45 -0.08 -0.06 -0.04 -0.33 -0.27 -0.38 5 -0.32 -0.38 -0.19 -0.19 -0.33 -0.01 -0.15 -0.08 -0.31 -0.27 -0.07 -0.11 -0.21 -0.22 -0.18 -0.27 -0.19 -0.15 6 -0.30 -0.16 -0.09 -0.25 -0.23 -0.44 -0.25 -0.16 -0.05 -0.13 -0.20 -0.09 -0.14 -0.18 -0.15 -0.22 -0.37 -0.38 7 -0.20 -0.14 -0.12 -0.10 -0.42 -0.42 -0.14 -0.04 -0.44 -0.11 -0.10 -0.17 -0.06 -0.29 -0.22 -0.24 -0.01 -0.45 8 -0.31 -0.11 -0.16 -0.21 -0.16 -0.39 -0.12 -0.36 -0.36 -0.29 -0.24 -0.24 -0.20 -0.18 -0.33 -0.39 -0.20 -0.02 9 -0.41 -0.14 -0.12 -0.21 -0.01 -0.37 -0.03 -0.22 -0.38 -0.22 -0.09 -0.22 -0.19 -0.17 -0.13 -0.32 -0.30 -0.21 10 -0.31 -0.05 -0.02 -0.36 -0.04 -0.15 -0.03 -0.12 -0.36 -0.21 -0.40 -0.03 -0.04 -0.03 -0.23 -0.01 -0.02 -0.41 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.20 0.84 0.11 0.00 0.55 1.24 0.55 -0.00 -0.02 -0.00 -0.00 -0.00 -0.00 -0.00 -0.20 0.98 1.00 0.30 1 0.00 0.00 0.00 0.00 0.00 0.19 0.00 -0.14 -0.87 -0.50 -0.00 -0.34 -0.28 -0.53 -0.24 -0.34 0.23 -0.09 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.34 -0.82 -1.02 -0.75 -0.74 -0.56 -0.68 -0.71 -1.00 -0.65 -0.56 3 0.23 0.18 0.00 0.00 0.00 0.00 0.00 -0.00 -0.27 -0.00 -0.00 -0.21 -0.00 -0.28 -0.21 -0.24 0.02 0.00 4 0.09 0.00 0.00 0.00 0.00 0.00 0.00 -0.08 -0.00 -0.14 -0.00 -0.50 -0.01 -0.25 0.23 -0.20 -0.14 -0.66 5 0.18 0.49 0.00 0.00 0.03 0.00 0.00 -0.79 -0.36 -0.49 -0.39 -0.69 -0.00 -0.09 0.08 -0.84 0.10 -0.25 6 0.64 0.76 0.08 0.50 0.62 0.79 0.68 -0.00 -0.06 -0.00 -0.00 -0.00 -0.00 -0.00 0.28 0.24 0.86 0.87 7 0.32 0.24 0.23 0.18 0.76 0.62 0.28 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 0.13 0.73 0.09 0.87 8 1.23 0.50 0.27 0.51 1.08 2.00 0.60 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 -0.00 1.00 1.00 1.00 1.00 <pre><code>x = Input(shape=(5, 7, 8))\n\nlayer = MonoDense(\n    units=12,\n    activation=activation,\n    monotonicity_indicator=[1]*3+[-1]*3+[0]*2,\n    is_convex=False,\n    is_concave=False,\n)\n\ny = layer(x)\n\nmodel = Model(inputs=x, outputs=y)\n\nmodel.summary()\n\ndisplay_kernel(layer.monotonicity_indicator)\n</code></pre> <pre><code>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 5, 7, 8)]         0\n\n mono_dense_5 (MonoDense)    (None, 5, 7, 12)          108\n\n=================================================================\nTotal params: 108\nTrainable params: 108\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> 0 0 1.00 1 1.00 2 1.00 3 -1.00 4 -1.00 5 -1.00 6 0.00 7 0.00"},{"location":"MonoDenseLayer/#mono-blocks","title":"Mono blocks","text":"<pre><code>x = Input(shape=(5, 7, 8))\n\n# monotonicity indicator must be broadcastable to input shape, so we use the vector of length 8\nmonotonicity_indicator = [1] * 3 + [0] * 2 + [-1] * 3\n\n# this mono block has 4 layers with the final one having the shape\nmono_block = _create_mono_block(\n    units=[16] * 3 + [3],\n    monotonicity_indicator=monotonicity_indicator,\n    activation=\"elu\",\n    dropout=0.1,\n)\ny = mono_block(x)\nmodel = Model(inputs=x, outputs=y)\nmodel.summary()\n\nmono_layers = [layer for layer in model.layers if isinstance(layer, MonoDense)]\nassert not (mono_layers[0].monotonicity_indicator == 1).all()\nfor mono_layer in mono_layers[1:]:\n    assert (mono_layer.monotonicity_indicator == 1).all()\n\nfor mono_layer in mono_layers[:-1]:\n    assert mono_layer.org_activation == \"elu\"\nassert mono_layers[-1].org_activation == None\n</code></pre> <pre><code>Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 5, 7, 8)]         0\n\n mono_dense_0 (MonoDense)    (None, 5, 7, 16)          144\n\n dropout (Dropout)           (None, 5, 7, 16)          0\n\n mono_dense_1_increasing (Mo  (None, 5, 7, 16)         272       \n noDense)\n\n dropout_1 (Dropout)         (None, 5, 7, 16)          0\n\n mono_dense_2_increasing (Mo  (None, 5, 7, 16)         272       \n noDense)\n\n dropout_2 (Dropout)         (None, 5, 7, 16)          0\n\n mono_dense_3_increasing (Mo  (None, 5, 7, 3)          51        \n noDense)\n\n=================================================================\nTotal params: 739\nTrainable params: 739\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre> <pre><code>inputs = Input(name=\"a\", shape=(1, ))\nparam = 0\n\nactual = _prepare_mono_input_n_param(inputs, param)\nexpected = [inputs], [0], [\"inputs\"]\nassert actual == expected, actual\n</code></pre> <pre><code>inputs = Input(name=\"a\", shape=(1, ))\nparam = {\"a\": 1}\n\nwith pytest.raises(ValueError) as e:\n    actual = _prepare_mono_input_n_param(inputs, param)\n\ne\n</code></pre> <pre><code>&lt;ExceptionInfo ValueError(\"Uncompatible types: type(inputs)=&lt;class 'keras.engine.keras_tensor.KerasTensor'&gt;, type(param)=&lt;class 'dict'&gt;\") tblen=2&gt;\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nactual = _prepare_mono_input_n_param({\"a\": a}, -1)\nassert actual == ([a], [-1], [\"a\"])\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nb = Input(name=\"b\", shape=(1,))\n\nactual = _prepare_mono_input_n_param({\"a\": a, \"b\": b}, {\"a\": -1, \"b\": 1})\nassert actual == ([a, b], [-1, 1], [\"a\", \"b\"])\n</code></pre> <pre><code>with pytest.raises(ValueError) as e:\n    actual = _prepare_mono_input_n_param(\n        {\"a\": Input(name=\"a\", shape=(1,)), \"b\": Input(name=\"b\", shape=(1,))}, {\"a\": -1}\n    )\ne\n</code></pre> <pre><code>&lt;ExceptionInfo ValueError(\"{'a'} != {'a', 'b'}\") tblen=2&gt;\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nb = Input(name=\"b\", shape=(1,))\n\nactual = _prepare_mono_input_n_param([a, b], [1, -1])\nassert actual == ([a, b], [1, -1], [\"0\", \"1\"])\n</code></pre> <pre><code>a = Input(name=\"a\", shape=(1,))\nb = Input(name=\"b\", shape=(1,))\n\nactual = _prepare_mono_input_n_param([a, b], -1)\nassert actual == ([a, b], [-1, -1], [\"0\", \"1\"])\n</code></pre> <pre><code>monotonicity_indicator = [-1, 0, 1]\nis_convex = [True] * 3\nis_concave = [False] * 3\nnames = list(\"abc\")\nhas_convex, has_concave = _check_convexity_params(\n    monotonicity_indicator, is_convex, is_concave, names\n)\nassert (has_convex, has_concave) == (True, False)\n</code></pre>"},{"location":"MonoDenseLayer/#type-1-architecture","title":"Type-1 architecture","text":""},{"location":"MonoDenseLayer/#create_type_1","title":"create_type_1","text":"<pre><code> create_type_1 (inputs:Union[tensorflow.python.types.core.Tensor,tensorflo\n                w.python.types.core.TensorProtocol,int,float,bool,str,byte\n                s,complex,tuple,list,numpy.ndarray,numpy.generic,Dict[str,\n                Union[tensorflow.python.types.core.Tensor,tensorflow.pytho\n                n.types.core.TensorProtocol,int,float,bool,str,bytes,compl\n                ex,tuple,list,numpy.ndarray,numpy.generic]],List[Union[ten\n                sorflow.python.types.core.Tensor,tensorflow.python.types.c\n                ore.TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                list,numpy.ndarray,numpy.generic]]], units:int,\n                final_units:int, activation:Union[str,Callable[[Union[tens\n                orflow.python.types.core.Tensor,tensorflow.python.types.co\n                re.TensorProtocol,int,float,bool,str,bytes,complex,tuple,l\n                ist,numpy.ndarray,numpy.generic]],Union[tensorflow.python.\n                types.core.Tensor,tensorflow.python.types.core.TensorProto\n                col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndar\n                ray,numpy.generic]]], n_layers:int, final_activation:Union\n                [str,Callable[[Union[tensorflow.python.types.core.Tensor,t\n                ensorflow.python.types.core.TensorProtocol,int,float,bool,\n                str,bytes,complex,tuple,list,numpy.ndarray,numpy.generic]]\n                ,Union[tensorflow.python.types.core.Tensor,tensorflow.pyth\n                on.types.core.TensorProtocol,int,float,bool,str,bytes,comp\n                lex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=Non\n                e, monotonicity_indicator:Union[int,Dict[str,int],List[int\n                ]]=1,\n                is_convex:Union[bool,Dict[str,bool],List[bool]]=False,\n                is_concave:Union[bool,Dict[str,bool],List[bool]]=False,\n                dropout:Optional[float]=None)\n</code></pre> <p>Builds Type-1 monotonic network</p> <p>Args: inputs: input tensor or a dictionary of tensors units: number of units in hidden layers final_units: number of units in the output layer activation: the base activation function n_layers: total number of layers (hidden layers plus the output layer) final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used. monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator. is_convex: set to True if a particular input feature is convex is_concave: set to True if a particular inputs feature is concave dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <p>Returns: Output tensor</p> <pre><code>n_layers = 4\n\ninputs = {name: Input(name=name, shape=(1, )) for name in list(\"abcd\")}\noutputs = create_type_1(\n    inputs=inputs,\n    units=64,\n    final_units=10,\n    activation=\"elu\",\n    n_layers=n_layers,\n    final_activation=\"softmax\",\n    monotonicity_indicator=dict(a=1, b=0, c=-1, d=0),\n    is_convex=True,\n    dropout=0.1,\n)\n\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.summary()\n\nmono_layers = [layer for layer in model.layers if isinstance(layer, MonoDense)]\nassert len(mono_layers) == n_layers\n\n# check monotonicity indicator\nnp.testing.assert_array_equal(\n    mono_layers[0].monotonicity_indicator, np.array([1, 0, -1, 0]).reshape((-1, 1))\n)\nfor i in range(1, n_layers):\n    assert mono_layers[i].monotonicity_indicator == 1\n\n# check convexity and concavity\nfor i in range(n_layers):\n    assert mono_layers[i].is_convex\n    assert not mono_layers[i].is_concave\n</code></pre> <pre><code>Model: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n a (InputLayer)                 [(None, 1)]          0           []\n\n b (InputLayer)                 [(None, 1)]          0           []\n\n c (InputLayer)                 [(None, 1)]          0           []\n\n d (InputLayer)                 [(None, 1)]          0           []\n\n concatenate (Concatenate)      (None, 4)            0           ['a[0][0]',                      \n                                                                  'b[0][0]',                      \n                                                                  'c[0][0]',                      \n                                                                  'd[0][0]']\n\n mono_dense_0_convex (MonoDense  (None, 64)          320         ['concatenate[0][0]']            \n )\n\n dropout_3 (Dropout)            (None, 64)           0           ['mono_dense_0_convex[0][0]']\n\n mono_dense_1_increasing_convex  (None, 64)          4160        ['dropout_3[0][0]']              \n  (MonoDense)\n\n dropout_4 (Dropout)            (None, 64)           0           ['mono_dense_1_increasing_convex[\n                                                                 0][0]']\n\n mono_dense_2_increasing_convex  (None, 64)          4160        ['dropout_4[0][0]']              \n  (MonoDense)\n\n dropout_5 (Dropout)            (None, 64)           0           ['mono_dense_2_increasing_convex[\n                                                                 0][0]']\n\n mono_dense_3_increasing_convex  (None, 10)          650         ['dropout_5[0][0]']              \n  (MonoDense)\n\n tf.nn.softmax (TFOpLambda)     (None, 10)           0           ['mono_dense_3_increasing_convex[\n                                                                 0][0]']\n\n==================================================================================================\nTotal params: 9,290\nTrainable params: 9,290\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre>"},{"location":"MonoDenseLayer/#type-2-architecture","title":"Type-2 architecture","text":"<pre><code>monotonicity_indicator = [1, 0, -1]\ninput_units = 2\nmonotonicity_indicator = sum([[abs(x)]*input_units for x in monotonicity_indicator], [])\nmonotonicity_indicator\n</code></pre> <pre><code>[1, 1, 0, 0, 1, 1]\n</code></pre>"},{"location":"MonoDenseLayer/#create_type_2","title":"create_type_2","text":"<pre><code> create_type_2 (inputs:Union[tensorflow.python.types.core.Tensor,tensorflo\n                w.python.types.core.TensorProtocol,int,float,bool,str,byte\n                s,complex,tuple,list,numpy.ndarray,numpy.generic,Dict[str,\n                Union[tensorflow.python.types.core.Tensor,tensorflow.pytho\n                n.types.core.TensorProtocol,int,float,bool,str,bytes,compl\n                ex,tuple,list,numpy.ndarray,numpy.generic]],List[Union[ten\n                sorflow.python.types.core.Tensor,tensorflow.python.types.c\n                ore.TensorProtocol,int,float,bool,str,bytes,complex,tuple,\n                list,numpy.ndarray,numpy.generic]]],\n                input_units:Optional[int]=None, units:int,\n                final_units:int, activation:Union[str,Callable[[Union[tens\n                orflow.python.types.core.Tensor,tensorflow.python.types.co\n                re.TensorProtocol,int,float,bool,str,bytes,complex,tuple,l\n                ist,numpy.ndarray,numpy.generic]],Union[tensorflow.python.\n                types.core.Tensor,tensorflow.python.types.core.TensorProto\n                col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndar\n                ray,numpy.generic]]], n_layers:int, final_activation:Union\n                [str,Callable[[Union[tensorflow.python.types.core.Tensor,t\n                ensorflow.python.types.core.TensorProtocol,int,float,bool,\n                str,bytes,complex,tuple,list,numpy.ndarray,numpy.generic]]\n                ,Union[tensorflow.python.types.core.Tensor,tensorflow.pyth\n                on.types.core.TensorProtocol,int,float,bool,str,bytes,comp\n                lex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=Non\n                e, monotonicity_indicator:Union[int,Dict[str,int]]=1,\n                is_convex:Union[bool,Dict[str,bool],List[bool]]=False,\n                is_concave:Union[bool,Dict[str,bool],List[bool]]=False,\n                dropout:Optional[float]=None)\n</code></pre> <p>Builds Type-2 monotonic network</p> <p>Args: inputs: input tensor or a dictionary of tensors input_units: used to preprocess features before entering the common mono block units: number of units in hidden layers final_units: number of units in the output layer activation: the base activation function n_layers: total number of layers (hidden layers plus the output layer) final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used. monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator. is_convex: set to True if a particular input feature is convex is_concave: set to True if a particular inputs feature is concave dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <p>Returns: Output tensor</p> <pre><code>for dropout in [False, True]:\n    print(\"*\" * 120)\n    print()\n    print(f\"{dropout=}\")\n    print()\n    inputs = {name: Input(name=name, shape=(1, )) for name in list(\"abcd\")}\n    outputs = create_type_2(\n        inputs,\n        units=32,\n        final_units=10,\n        activation=\"elu\",\n        final_activation=\"softmax\",\n        n_layers=3,\n        dropout=dropout,\n        monotonicity_indicator=dict(a=1, b=0, c=-1, d=0),\n        is_convex=dict(a=True, b=False, c=False, d=False),\n        is_concave=False,\n    )\n    model = Model(inputs=inputs, outputs=outputs)\n    model.summary()\n</code></pre> <pre><code>************************************************************************************************************************\n\ndropout=False\n\nModel: \"model_7\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n a (InputLayer)                 [(None, 1)]          0           []\n\n b (InputLayer)                 [(None, 1)]          0           []\n\n c (InputLayer)                 [(None, 1)]          0           []\n\n d (InputLayer)                 [(None, 1)]          0           []\n\n mono_dense_a_increasing_convex  (None, 8)           16          ['a[0][0]']                      \n  (MonoDense)\n\n dense_b (Dense)                (None, 8)            16          ['b[0][0]']\n\n mono_dense_c_decreasing (MonoD  (None, 8)           16          ['c[0][0]']                      \n ense)\n\n dense_d (Dense)                (None, 8)            16          ['d[0][0]']\n\n preprocessed_features (Concate  (None, 32)          0           ['mono_dense_a_increasing_convex[\n nate)                                                           0][0]',                          \n                                                                  'dense_b[0][0]',                \n                                                                  'mono_dense_c_decreasing[0][0]',\n                                                                  'dense_d[0][0]']\n\n mono_dense_0_convex (MonoDense  (None, 32)          1056        ['preprocessed_features[0][0]']  \n )\n\n mono_dense_1_increasing_convex  (None, 32)          1056        ['mono_dense_0_convex[0][0]']    \n  (MonoDense)\n\n mono_dense_2_increasing_convex  (None, 10)          330         ['mono_dense_1_increasing_convex[\n  (MonoDense)                                                    0][0]']\n\n tf.nn.softmax_1 (TFOpLambda)   (None, 10)           0           ['mono_dense_2_increasing_convex[\n                                                                 0][0]']\n\n==================================================================================================\nTotal params: 2,506\nTrainable params: 2,506\nNon-trainable params: 0\n__________________________________________________________________________________________________\n************************************************************************************************************************\n\ndropout=True\n\nModel: \"model_8\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n a (InputLayer)                 [(None, 1)]          0           []\n\n b (InputLayer)                 [(None, 1)]          0           []\n\n c (InputLayer)                 [(None, 1)]          0           []\n\n d (InputLayer)                 [(None, 1)]          0           []\n\n mono_dense_a_increasing_convex  (None, 8)           16          ['a[0][0]']                      \n  (MonoDense)\n\n dense_b (Dense)                (None, 8)            16          ['b[0][0]']\n\n mono_dense_c_decreasing (MonoD  (None, 8)           16          ['c[0][0]']                      \n ense)\n\n dense_d (Dense)                (None, 8)            16          ['d[0][0]']\n\n preprocessed_features (Concate  (None, 32)          0           ['mono_dense_a_increasing_convex[\n nate)                                                           0][0]',                          \n                                                                  'dense_b[0][0]',                \n                                                                  'mono_dense_c_decreasing[0][0]',\n                                                                  'dense_d[0][0]']\n\n mono_dense_0_convex (MonoDense  (None, 32)          1056        ['preprocessed_features[0][0]']  \n )\n\n dropout_11 (Dropout)           (None, 32)           0           ['mono_dense_0_convex[0][0]']\n\n mono_dense_1_increasing_convex  (None, 32)          1056        ['dropout_11[0][0]']             \n  (MonoDense)\n\n dropout_12 (Dropout)           (None, 32)           0           ['mono_dense_1_increasing_convex[\n                                                                 0][0]']\n\n mono_dense_2_increasing_convex  (None, 10)          330         ['dropout_12[0][0]']             \n  (MonoDense)\n\n tf.nn.softmax_2 (TFOpLambda)   (None, 10)           0           ['mono_dense_2_increasing_convex[\n                                                                 0][0]']\n\n==================================================================================================\nTotal params: 2,506\nTrainable params: 2,506\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Monotonic Dense Layer</li> <li>Experiments<ul> <li>Experiments</li> <li>Auto MPG</li> <li>Heart disease</li> <li>COMPAS</li> <li>Blog feedback</li> <li>Loan</li> </ul> </li> <li>API<ul> <li>mono_dense_keras<ul> <li>MonoDense</li> <li>create_type_1</li> <li>create_type_2</li> <li>experiments<ul> <li>DownloadProgressBar</li> <li>TestHyperModel</li> <li>build_mono_model_f</li> <li>count_model_params</li> <li>create_model_stats</li> <li>create_tuner_stats</li> <li>df2ds</li> <li>download_data</li> <li>download_url</li> <li>find_hyperparameters</li> <li>get_build_model_with_hp_f</li> <li>get_data_path</li> <li>get_train_n_test_data</li> <li>peek</li> <li>sanitize_col_names</li> </ul> </li> <li>helpers<ul> <li>export</li> </ul> </li> <li>replace_kernel_using_monotonicity_indicator</li> </ul> </li> </ul> </li> <li>Releases</li> </ul>"},{"location":"TopLevel/","title":"TopLevel","text":""},{"location":"TopLevel/#dummy","title":"dummy","text":"<pre><code> dummy ()\n</code></pre>"},{"location":"changelog_not_found/","title":"Releases","text":""},{"location":"changelog_not_found/#changelogmd-file-not-found","title":"CHANGELOG.md file not found.","text":"<p>To generate the changelog file, please run the following command from the project root directory. </p> <pre><code>nbdev_changelog\n</code></pre> <p>If you do not want this page to be rendered as part of the documentation, please remove the following line from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- [Releases]{changelog}\n</code></pre>"},{"location":"cli_commands_not_found/","title":"No CLI commands found in console_scripts in settings.ini file.","text":"<p>For documenting CLI commands, please add command line executables in <code>console_scripts</code> in <code>settings.ini</code> file. </p> <p>If you do not want this page to be rendered as part of the documentation, please remove the following lines from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- CLI\n{cli}\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/","title":"MonoDense","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense","title":"<code>mono_dense_keras.MonoDense</code>","text":"<p>         Bases: <code>Dense</code></p> <p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> <p>This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.</p> <p>In the code, the variable <code>monotonicity_indicator</code> corresponds to t in the figure and the variable <code>activation_selector</code> corresponds to s.</p> <p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate <code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is  -1 or 1, then <code>activation_selector</code> will have all elements 0 or 1, respecively. - if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value between 0 and 1 and corresponds to the percentage of elements of <code>activation_selector</code> set to 1.</p> <p></p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>@export\nclass MonoDense(Dense):\n\"\"\"Monotonic counterpart of the regular Dense Layer of tf.keras\n\n    This is an implementation of our Monotonic Dense Unit or Constrained Monotone Fully Connected Layer. The below is the figure from the paper for reference.\n\n    In the code, the variable `monotonicity_indicator` corresponds to **t** in the figure and the variable `activation_selector` corresponds to **s**.\n\n    Parameters `convexity_indicator` and `epsilon` are used to calculate `activation_selector` as follows:\n    - if `convexity_indicator` is  -1 or 1, then `activation_selector` will have all elements 0 or 1, respecively.\n    - if `convexity_indicator` is `None`, then `epsilon` must have a value between 0 and 1 and corresponds to the percentage of elements of `activation_selector` set to 1.\n\n    ![mono-dense-layer-diagram.png](images/mono-dense-layer-diagram.png)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        units: int,\n        *,\n        activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n        monotonicity_indicator: ArrayLike = 1,\n        is_convex: bool = False,\n        is_concave: bool = False,\n        activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0),\n        **kwargs: Dict[str, Any],\n    ):\n\"\"\"Constructs a new MonoDense instance.\n\n        Params:\n            units: Positive integer, dimensionality of the output space.\n            activation: Activation function to use, it is assumed to be convex monotonically\n                increasing function such as \"relu\" or \"elu\"\n            monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n                monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n                -1 for monotonically decreasing and 0 for non-monotonic.\n            is_convex: convex if set to True\n            is_concave: concave if set to True\n            activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n                Ignored if is_convex or is_concave is set to True\n            **kwargs: passed as kwargs to the constructor of `Dense`\n\n        Raise:\n            ValueError:\n                - if both **is_concave** and **is_convex** are set to **True**, or\n                - if any component of activation_weights is negative or there is not exactly three components\n        \"\"\"\n        if is_convex and is_concave:\n            raise ValueError(\n                \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n            )\n\n        if len(activation_weights) != 3:\n            raise ValueError(\n                f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n            )\n\n        if (np.array(activation_weights) &lt; 0).any():\n            raise ValueError(\n                f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n            )\n\n        super(MonoDense, self).__init__(units=units, activation=None, **kwargs)\n\n        self.units = units\n        self.org_activation = activation\n        self.activation_weights = activation_weights\n        self.monotonicity_indicator = monotonicity_indicator\n        self.is_convex = is_convex\n        self.is_concave = is_concave\n\n        (\n            self.convex_activation,\n            self.concave_activation,\n            self.saturated_activation,\n        ) = get_activation_functions(self.org_activation)\n\n    def build(\n        self, input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]\n    ) -&gt; None:\n\"\"\"Build\n\n        Args:\n            input_shape: input tensor\n            args: positional arguments passed to Dense.build()\n            kwargs: keyword arguments passed to Dense.build()\n        \"\"\"\n        super(MonoDense, self).build(input_shape, *args, **kwargs)\n        self.monotonicity_indicator = get_monotonicity_indicator(\n            monotonicity_indicator=self.monotonicity_indicator,\n            input_shape=input_shape,\n            units=self.units,\n        )\n\n    def call(self, inputs: TensorLike) -&gt; TensorLike:\n\"\"\"Call\n\n        Args:\n            inputs: input tensor of shape (batch_size, ..., x_length)\n\n        Returns:\n            N-D tensor with shape: `(batch_size, ..., units)`.\n\n        \"\"\"\n        # calculate W'*x+y after we replace the kernal according to monotonicity vector\n        with replace_kernel_using_monotonicity_indicator(\n            self, monotonicity_indicator=self.monotonicity_indicator\n        ):\n            h = super(MonoDense, self).call(inputs)\n\n        y = apply_activations(\n            h,\n            units=self.units,\n            convex_activation=self.convex_activation,\n            concave_activation=self.concave_activation,\n            saturated_activation=self.saturated_activation,\n            is_convex=self.is_convex,\n            is_concave=self.is_concave,\n            activation_weights=self.activation_weights,\n        )\n\n        return y\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense-attributes","title":"Attributes","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.activation_weights","title":"<code>activation_weights = activation_weights</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.is_concave","title":"<code>is_concave = is_concave</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.is_convex","title":"<code>is_convex = is_convex</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.monotonicity_indicator","title":"<code>monotonicity_indicator = monotonicity_indicator</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.org_activation","title":"<code>org_activation = activation</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.units","title":"<code>units = units</code>  <code>instance-attribute</code>","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras.MonoDense-functions","title":"Functions","text":""},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.__init__","title":"<code>__init__(units: int, *, activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None, monotonicity_indicator: ArrayLike = 1, is_convex: bool = False, is_concave: bool = False, activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0), **kwargs: Dict[str, Any])</code>","text":"<p>Constructs a new MonoDense instance.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Positive integer, dimensionality of the output space.</p> required <code>activation</code> <code>Optional[Union[str, Callable[[TensorLike], TensorLike]]]</code> <p>Activation function to use, it is assumed to be convex monotonically increasing function such as \"relu\" or \"elu\"</p> <code>None</code> <code>monotonicity_indicator</code> <code>ArrayLike</code> <p>Vector to indicate which of the inputs are monotonically increasing or monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing, -1 for monotonically decreasing and 0 for non-monotonic.</p> <code>1</code> <code>is_convex</code> <code>bool</code> <p>convex if set to True</p> <code>False</code> <code>is_concave</code> <code>bool</code> <p>concave if set to True</p> <code>False</code> <code>activation_weights</code> <code>Tuple[float, float, float]</code> <p>relative weights for each type of activation, the default is (1.0, 1.0, 1.0). Ignored if is_convex or is_concave is set to True</p> <code>(7.0, 7.0, 2.0)</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>passed as kwargs to the constructor of <code>Dense</code></p> <code>{}</code> Raise <p>ValueError:     - if both is_concave and is_convex are set to True, or     - if any component of activation_weights is negative or there is not exactly three components</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def __init__(\n    self,\n    units: int,\n    *,\n    activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    monotonicity_indicator: ArrayLike = 1,\n    is_convex: bool = False,\n    is_concave: bool = False,\n    activation_weights: Tuple[float, float, float] = (7.0, 7.0, 2.0),\n    **kwargs: Dict[str, Any],\n):\n\"\"\"Constructs a new MonoDense instance.\n\n    Params:\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use, it is assumed to be convex monotonically\n            increasing function such as \"relu\" or \"elu\"\n        monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n            monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n            -1 for monotonically decreasing and 0 for non-monotonic.\n        is_convex: convex if set to True\n        is_concave: concave if set to True\n        activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n            Ignored if is_convex or is_concave is set to True\n        **kwargs: passed as kwargs to the constructor of `Dense`\n\n    Raise:\n        ValueError:\n            - if both **is_concave** and **is_convex** are set to **True**, or\n            - if any component of activation_weights is negative or there is not exactly three components\n    \"\"\"\n    if is_convex and is_concave:\n        raise ValueError(\n            \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n        )\n\n    if len(activation_weights) != 3:\n        raise ValueError(\n            f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n        )\n\n    if (np.array(activation_weights) &lt; 0).any():\n        raise ValueError(\n            f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n        )\n\n    super(MonoDense, self).__init__(units=units, activation=None, **kwargs)\n\n    self.units = units\n    self.org_activation = activation\n    self.activation_weights = activation_weights\n    self.monotonicity_indicator = monotonicity_indicator\n    self.is_convex = is_convex\n    self.is_concave = is_concave\n\n    (\n        self.convex_activation,\n        self.concave_activation,\n        self.saturated_activation,\n    ) = get_activation_functions(self.org_activation)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_loss","title":"<code>keras.engine.base_layer.Layer.add_loss(losses, **kwargs)</code>","text":"<p>Add loss tensor(s), potentially dependent on layer inputs.</p> <p>Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p> <p>This method can be used inside a subclassed layer or model's <code>call</code> function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p> <p>Example:</p> <pre><code>class MyLayer(tf.keras.layers.Layer):\n  def call(self, inputs):\n    self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n    return inputs\n</code></pre> <p>The same code works in distributed training: the input to <code>add_loss()</code> is treated like a regularization loss and averaged across replicas by the training loop (both built-in <code>Model.fit()</code> and compliant custom training loops).</p> <p>The <code>add_loss</code> method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's <code>Input</code>s. These losses become part of the model's topology and are tracked in <code>get_config</code>.</p> <p>Example:</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\n# Activity regularization.\nmodel.add_loss(tf.abs(tf.reduce_mean(x)))\n</code></pre> <p>If this is not the case for your loss (if, for example, your loss references a <code>Variable</code> of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized.</p> <p>Example:</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nd = tf.keras.layers.Dense(10)\nx = d(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\n# Weight regularization.\nmodel.add_loss(lambda: tf.reduce_mean(d.kernel))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>losses</code> <p>Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor.</p> required <code>**kwargs</code> <p>Used for backwards compatibility only.</p> <code>{}</code> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def add_loss(self, losses, **kwargs):\n\"\"\"Add loss tensor(s), potentially dependent on layer inputs.\n\n    Some losses (for instance, activity regularization losses) may be\n    dependent on the inputs passed when calling a layer. Hence, when reusing\n    the same layer on different inputs `a` and `b`, some entries in\n    `layer.losses` may be dependent on `a` and some on `b`. This method\n    automatically keeps track of dependencies.\n\n    This method can be used inside a subclassed layer or model's `call`\n    function, in which case `losses` should be a Tensor or list of Tensors.\n\n    Example:\n\n    ```python\n    class MyLayer(tf.keras.layers.Layer):\n      def call(self, inputs):\n        self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n        return inputs\n    ```\n\n    The same code works in distributed training: the input to `add_loss()`\n    is treated like a regularization loss and averaged across replicas\n    by the training loop (both built-in `Model.fit()` and compliant custom\n    training loops).\n\n    The `add_loss` method can also be called directly on a Functional Model\n    during construction. In this case, any loss Tensors passed to this Model\n    must be symbolic and be able to be traced back to the model's `Input`s.\n    These losses become part of the model's topology and are tracked in\n    `get_config`.\n\n    Example:\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    # Activity regularization.\n    model.add_loss(tf.abs(tf.reduce_mean(x)))\n    ```\n\n    If this is not the case for your loss (if, for example, your loss\n    references a `Variable` of one of the model's layers), you can wrap your\n    loss in a zero-argument lambda. These losses are not tracked as part of\n    the model's topology since they can't be serialized.\n\n    Example:\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    d = tf.keras.layers.Dense(10)\n    x = d(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    # Weight regularization.\n    model.add_loss(lambda: tf.reduce_mean(d.kernel))\n    ```\n\n    Args:\n      losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n        losses may also be zero-argument callables which create a loss\n        tensor.\n      **kwargs: Used for backwards compatibility only.\n    \"\"\"\n    kwargs.pop(\"inputs\", None)\n    if kwargs:\n        raise TypeError(f\"Unknown keyword arguments: {kwargs.keys()}\")\n\n    def _tag_callable(loss):\n\"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\"\n        if callable(loss):\n            # We run the loss without autocasting, as regularizers are often\n            # numerically unstable in float16.\n            with autocast_variable.enable_auto_cast_variables(None):\n                loss = loss()\n        if loss is None:\n            # Will be filtered out when computing the .losses property\n            return None\n        if not tf.is_tensor(loss):\n            loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n        loss._unconditional_loss = True\n        return loss\n\n    losses = tf.nest.flatten(losses)\n\n    callable_losses = []\n    eager_losses = []\n    symbolic_losses = []\n    for loss in losses:\n        if callable(loss):\n            callable_losses.append(functools.partial(_tag_callable, loss))\n            continue\n        if loss is None:\n            continue\n        if not tf.is_tensor(loss) and not isinstance(\n            loss, keras_tensor.KerasTensor\n        ):\n            loss = tf.convert_to_tensor(loss, dtype=backend.floatx())\n        # TF Functions should take the eager path.\n        if (\n            tf_utils.is_symbolic_tensor(loss)\n            or isinstance(loss, keras_tensor.KerasTensor)\n        ) and not base_layer_utils.is_in_tf_function():\n            symbolic_losses.append(loss)\n        elif tf.is_tensor(loss):\n            eager_losses.append(loss)\n\n    self._callable_losses.extend(callable_losses)\n\n    in_call_context = base_layer_utils.call_context().in_call\n    if eager_losses and not in_call_context:\n        raise ValueError(\n            \"Expected a symbolic Tensors or a callable for the loss value. \"\n            \"Please wrap your loss computation in a zero argument `lambda`.\"\n        )\n\n    self._eager_losses.extend(eager_losses)\n\n    for symbolic_loss in symbolic_losses:\n        if getattr(self, \"_is_graph_network\", False):\n            self._graph_network_add_loss(symbolic_loss)\n        else:\n            # Possible a loss was added in a Layer's `build`.\n            self._losses.append(symbolic_loss)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_metric","title":"<code>keras.engine.base_layer.Layer.add_metric(value, name = None, **kwargs)</code>","text":"<p>Adds metric tensor to the layer.</p> <p>This method can be used inside the <code>call()</code> method of a subclassed layer or model.</p> <pre><code>class MyMetricLayer(tf.keras.layers.Layer):\n  def __init__(self):\n    super(MyMetricLayer, self).__init__(name='my_metric_layer')\n    self.mean = tf.keras.metrics.Mean(name='metric_1')\n\n  def call(self, inputs):\n    self.add_metric(self.mean(inputs))\n    self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n    return inputs\n</code></pre> <p>This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's <code>Input</code>s. These metrics become part of the model's topology and are tracked when you save the model via <code>save()</code>.</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.add_metric(math_ops.reduce_sum(x), name='metric_1')\n</code></pre> <p>Note: Calling <code>add_metric()</code> with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs.</p> <pre><code>inputs = tf.keras.Input(shape=(10,))\nx = tf.keras.layers.Dense(10)(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)\nmodel.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>Metric tensor.</p> required <code>name</code> <p>String metric name.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for backward compatibility. Accepted values: <code>aggregation</code> - When the <code>value</code> tensor provided is not the result of calling a <code>keras.Metric</code> instance, it will be aggregated by default using a <code>keras.Metric.Mean</code>.</p> <code>{}</code> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def add_metric(self, value, name=None, **kwargs):\n\"\"\"Adds metric tensor to the layer.\n\n    This method can be used inside the `call()` method of a subclassed layer\n    or model.\n\n    ```python\n    class MyMetricLayer(tf.keras.layers.Layer):\n      def __init__(self):\n        super(MyMetricLayer, self).__init__(name='my_metric_layer')\n        self.mean = tf.keras.metrics.Mean(name='metric_1')\n\n      def call(self, inputs):\n        self.add_metric(self.mean(inputs))\n        self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n        return inputs\n    ```\n\n    This method can also be called directly on a Functional Model during\n    construction. In this case, any tensor passed to this Model must\n    be symbolic and be able to be traced back to the model's `Input`s. These\n    metrics become part of the model's topology and are tracked when you\n    save the model via `save()`.\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n    ```\n\n    Note: Calling `add_metric()` with the result of a metric object on a\n    Functional Model, as shown in the example below, is not supported. This\n    is because we cannot trace the metric result tensor back to the model's\n    inputs.\n\n    ```python\n    inputs = tf.keras.Input(shape=(10,))\n    x = tf.keras.layers.Dense(10)(inputs)\n    outputs = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs, outputs)\n    model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n    ```\n\n    Args:\n      value: Metric tensor.\n      name: String metric name.\n      **kwargs: Additional keyword arguments for backward compatibility.\n        Accepted values:\n        `aggregation` - When the `value` tensor provided is not the result\n        of calling a `keras.Metric` instance, it will be aggregated by\n        default using a `keras.Metric.Mean`.\n    \"\"\"\n    kwargs_keys = list(kwargs.keys())\n    if len(kwargs_keys) &gt; 1 or (\n        len(kwargs_keys) == 1 and kwargs_keys[0] != \"aggregation\"\n    ):\n        raise TypeError(\n            f\"Unknown keyword arguments: {kwargs.keys()}. \"\n            \"Expected `aggregation`.\"\n        )\n\n    from_metric_obj = hasattr(value, \"_metric_obj\")\n    is_symbolic = isinstance(value, keras_tensor.KerasTensor)\n    in_call_context = base_layer_utils.call_context().in_call\n\n    if name is None and not from_metric_obj:\n        # Eg. `self.add_metric(math_ops.reduce_sum(x))` In eager mode, we\n        # use metric name to lookup a metric. Without a name, a new Mean\n        # metric wrapper will be created on every model/layer call. So, we\n        # raise an error when no name is provided. We will do the same for\n        # symbolic mode for consistency although a name will be generated if\n        # no name is provided.\n\n        # We will not raise this error in the foll use case for the sake of\n        # consistency as name in provided in the metric constructor.\n        # mean = metrics.Mean(name='my_metric')\n        # model.add_metric(mean(outputs))\n        raise ValueError(\n            \"Please provide a name for your metric like \"\n            \"`self.add_metric(tf.reduce_sum(inputs), \"\n            \"name='mean_activation')`\"\n        )\n    elif from_metric_obj:\n        name = value._metric_obj.name\n\n    if not in_call_context and not is_symbolic:\n        raise ValueError(\n            \"Expected a symbolic Tensor for the metric value, received: \"\n            + str(value)\n        )\n\n    # If a metric was added in a Layer's `call` or `build`.\n    if in_call_context or not getattr(self, \"_is_graph_network\", False):\n        # TF Function path should take the eager path.\n\n        # If the given metric is available in `metrics` list we just update\n        # state on it, otherwise we create a new metric instance and\n        # add it to the `metrics` list.\n        metric_obj = getattr(value, \"_metric_obj\", None)\n        # Tensors that come from a Metric object already updated the Metric\n        # state.\n        should_update_state = not metric_obj\n        name = metric_obj.name if metric_obj else name\n\n        with self._metrics_lock:\n            match = self._get_existing_metric(name)\n            if match:\n                metric_obj = match\n            elif metric_obj:\n                self._metrics.append(metric_obj)\n            else:\n                # Build the metric object with the value's dtype if it\n                # defines one\n                metric_obj = metrics_mod.Mean(\n                    name=name, dtype=getattr(value, \"dtype\", None)\n                )\n                self._metrics.append(metric_obj)\n\n        if should_update_state:\n            metric_obj(value)\n    else:\n        if from_metric_obj:\n            raise ValueError(\n                \"Using the result of calling a `Metric` object \"\n                \"when calling `add_metric` on a Functional \"\n                \"Model is not supported. Please pass the \"\n                \"Tensor to monitor directly.\"\n            )\n\n        # Insert layers into the Keras Graph Network.\n        aggregation = None if from_metric_obj else \"mean\"\n        self._graph_network_add_metric(value, aggregation, name)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_update","title":"<code>keras.engine.base_layer.Layer.add_update(updates)</code>","text":"<p>Add update op(s), potentially dependent on layer inputs.</p> <p>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track of dependencies.</p> <p>This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution).</p> <p>Parameters:</p> Name Type Description Default <code>updates</code> <p>Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting <code>trainable=False</code> on this Layer, when executing in Eager mode.</p> required Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef add_update(self, updates):\n\"\"\"Add update op(s), potentially dependent on layer inputs.\n\n    Weight updates (for instance, the updates of the moving mean and\n    variance in a BatchNormalization layer) may be dependent on the inputs\n    passed when calling a layer. Hence, when reusing the same layer on\n    different inputs `a` and `b`, some entries in `layer.updates` may be\n    dependent on `a` and some on `b`. This method automatically keeps track\n    of dependencies.\n\n    This call is ignored when eager execution is enabled (in that case,\n    variable updates are run on the fly and thus do not need to be tracked\n    for later execution).\n\n    Args:\n      updates: Update op, or list/tuple of update ops, or zero-arg callable\n        that returns an update op. A zero-arg callable should be passed in\n        order to disable running the updates by setting `trainable=False`\n        on this Layer, when executing in Eager mode.\n    \"\"\"\n    call_context = base_layer_utils.call_context()\n    # No need to run updates during Functional API construction.\n    if call_context.in_keras_graph:\n        return\n\n    # Callable updates are disabled by setting `trainable=False`.\n    if not call_context.frozen:\n        for update in tf.nest.flatten(updates):\n            if callable(update):\n                update()\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_variable","title":"<code>keras.engine.base_layer.Layer.add_variable(*args, **kwargs)</code>","text":"<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef add_variable(self, *args, **kwargs):\n\"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\n    warnings.warn(\n        \"`layer.add_variable` is deprecated and \"\n        \"will be removed in a future version. \"\n        \"Please use the `layer.add_weight()` method instead.\",\n        stacklevel=2,\n    )\n    return self.add_weight(*args, **kwargs)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.add_weight","title":"<code>keras.engine.base_layer.Layer.add_weight(name = None, shape = None, dtype = None, initializer = None, regularizer = None, trainable = None, constraint = None, use_resource = None, synchronization = tf.VariableSynchronization.AUTO, aggregation = tf.VariableAggregation.NONE, **kwargs)</code>","text":"<p>Adds a new variable to the layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Variable name.</p> <code>None</code> <code>shape</code> <p>Variable shape. Defaults to scalar if unspecified.</p> <code>None</code> <code>dtype</code> <p>The type of the variable. Defaults to <code>self.dtype</code>.</p> <code>None</code> <code>initializer</code> <p>Initializer instance (callable).</p> <code>None</code> <code>regularizer</code> <p>Regularizer instance (callable).</p> <code>None</code> <code>trainable</code> <p>Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code> is set to <code>ON_READ</code>.</p> <code>None</code> <code>constraint</code> <p>Constraint instance (callable).</p> <code>None</code> <code>use_resource</code> <p>Whether to use a <code>ResourceVariable</code> or not. See this guide  for more information.</p> <code>None</code> <code>synchronization</code> <p>Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class <code>tf.VariableSynchronization</code>. By default the synchronization is set to <code>AUTO</code> and the current <code>DistributionStrategy</code> chooses when to synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>, <code>trainable</code> must not be set to <code>True</code>.</p> <code>tf.VariableSynchronization.AUTO</code> <code>aggregation</code> <p>Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class <code>tf.VariableAggregation</code>.</p> <code>tf.VariableAggregation.NONE</code> <code>**kwargs</code> <p>Additional keyword arguments. Accepted values are <code>getter</code>, <code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The variable created.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as <code>ON_READ</code>.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.for_subclass_implementers\ndef add_weight(\n    self,\n    name=None,\n    shape=None,\n    dtype=None,\n    initializer=None,\n    regularizer=None,\n    trainable=None,\n    constraint=None,\n    use_resource=None,\n    synchronization=tf.VariableSynchronization.AUTO,\n    aggregation=tf.VariableAggregation.NONE,\n    **kwargs,\n):\n\"\"\"Adds a new variable to the layer.\n\n    Args:\n      name: Variable name.\n      shape: Variable shape. Defaults to scalar if unspecified.\n      dtype: The type of the variable. Defaults to `self.dtype`.\n      initializer: Initializer instance (callable).\n      regularizer: Regularizer instance (callable).\n      trainable: Boolean, whether the variable should be part of the layer's\n        \"trainable_variables\" (e.g. variables, biases)\n        or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n        Note that `trainable` cannot be `True` if `synchronization`\n        is set to `ON_READ`.\n      constraint: Constraint instance (callable).\n      use_resource: Whether to use a `ResourceVariable` or not.\n        See [this guide](\n        https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n         for more information.\n      synchronization: Indicates when a distributed a variable will be\n        aggregated. Accepted values are constants defined in the class\n        `tf.VariableSynchronization`. By default the synchronization is set\n        to `AUTO` and the current `DistributionStrategy` chooses when to\n        synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n        must not be set to `True`.\n      aggregation: Indicates how a distributed variable will be aggregated.\n        Accepted values are constants defined in the class\n        `tf.VariableAggregation`.\n      **kwargs: Additional keyword arguments. Accepted values are `getter`,\n        `collections`, `experimental_autocast` and `caching_device`.\n\n    Returns:\n      The variable created.\n\n    Raises:\n      ValueError: When giving unsupported dtype and no initializer or when\n        trainable has been set to True with synchronization set as\n        `ON_READ`.\n    \"\"\"\n    if shape is None:\n        shape = ()\n    kwargs.pop(\"partitioner\", None)  # Ignored.\n    # Validate optional keyword arguments.\n    for kwarg in kwargs:\n        if kwarg not in [\n            \"collections\",\n            \"experimental_autocast\",\n            \"caching_device\",\n            \"getter\",\n            \"layout\",\n        ]:\n            raise TypeError(\"Unknown keyword argument:\", kwarg)\n    collections_arg = kwargs.pop(\"collections\", None)\n    # 'experimental_autocast' can be set to False by the caller to indicate\n    # an AutoCastVariable should never be created.\n    autocast = kwargs.pop(\"experimental_autocast\", True)\n    # See the docstring for tf.Variable about the details for\n    # caching_device.\n    caching_device = kwargs.pop(\"caching_device\", None)\n\n    layout = kwargs.pop(\"layout\", None)\n    # Specially handling of auto layout fetch, based on the variable name\n    # and attribute name. For built-in keras layers, usually the variable\n    # name, eg 'kernel', will match with a 'kernel_layout' attribute name on\n    # the instance. We will try to do this auto fetch if layout is not\n    # explicitly specified. This is mainly a quick workaround for not\n    # applying too many interface change to built-in layers, until DTensor\n    # is a public API.  Also see dtensor.utils.allow_initializer_layout for\n    # more details.\n    # TODO(scottzhu): Remove this once dtensor is public to end user.\n    if not layout and name:\n        layout = getattr(self, name + \"_layout\", None)\n\n    if dtype is None:\n        dtype = self.dtype or backend.floatx()\n    dtype = tf.as_dtype(dtype)\n    if self._dtype_policy.variable_dtype is None:\n        # The policy is \"_infer\", so we infer the policy from the variable\n        # dtype.\n        self._set_dtype_policy(policy.Policy(dtype.base_dtype.name))\n    initializer = initializers.get(initializer)\n    regularizer = regularizers.get(regularizer)\n    constraint = constraints.get(constraint)\n\n    if synchronization == tf.VariableSynchronization.ON_READ:\n        if trainable:\n            raise ValueError(\n                \"Synchronization value can be set to \"\n                \"VariableSynchronization.ON_READ only for non-trainable \"\n                \"variables. You have specified trainable=True and \"\n                \"synchronization=VariableSynchronization.ON_READ.\"\n            )\n        else:\n            # Set trainable to be false when variable is to be synced on\n            # read.\n            trainable = False\n    elif trainable is None:\n        trainable = True\n\n    # Initialize variable when no initializer provided\n    if initializer is None:\n        # If dtype is DT_FLOAT, provide a uniform unit scaling initializer\n        if dtype.is_floating:\n            initializer = initializers.get(\"glorot_uniform\")\n        # If dtype is DT_INT/DT_UINT, provide a default value `zero`\n        # If dtype is DT_BOOL, provide a default value `FALSE`\n        elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n            initializer = initializers.get(\"zeros\")\n        # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX\n        # here?\n        elif \"getter\" not in kwargs:\n            # When `getter` is specified, it's possibly fine for\n            # `initializer` to be None since it's up to the custom `getter`\n            # to raise error in case it indeed needs `initializer`.\n            raise ValueError(\n                f\"An initializer for variable {name} of type \"\n                f\"{dtype.base_dtype} is required for layer \"\n                f\"{self.name}. Received: {initializer}.\"\n            )\n\n    getter = kwargs.pop(\"getter\", base_layer_utils.make_variable)\n    if (\n        autocast\n        and self._dtype_policy.compute_dtype\n        != self._dtype_policy.variable_dtype\n        and dtype.is_floating\n    ):\n        old_getter = getter\n\n        # Wrap variable constructor to return an AutoCastVariable.\n        def getter(*args, **kwargs):\n            variable = old_getter(*args, **kwargs)\n            return autocast_variable.create_autocast_variable(variable)\n\n        # Also the caching_device does not work with the mixed precision\n        # API, disable it if it is specified.\n        # TODO(b/142020079): Re-enable it once the bug is fixed.\n        if caching_device is not None:\n            tf_logging.warning(\n                \"`caching_device` does not work with mixed precision API. \"\n                \"Ignoring user specified `caching_device`.\"\n            )\n            caching_device = None\n    if layout:\n        getter = functools.partial(getter, layout=layout)\n\n    variable = self._add_variable_with_custom_getter(\n        name=name,\n        shape=shape,\n        # TODO(allenl): a `make_variable` equivalent should be added as a\n        # `Trackable` method.\n        getter=getter,\n        # Manage errors in Layer rather than Trackable.\n        overwrite=True,\n        initializer=initializer,\n        dtype=dtype,\n        constraint=constraint,\n        trainable=trainable,\n        use_resource=use_resource,\n        collections=collections_arg,\n        synchronization=synchronization,\n        aggregation=aggregation,\n        caching_device=caching_device,\n    )\n    if regularizer is not None:\n        # TODO(fchollet): in the future, this should be handled at the\n        # level of variable creation, and weight regularization losses\n        # should be variable attributes.\n        name_in_scope = variable.name[: variable.name.find(\":\")]\n        self._handle_weight_regularization(\n            name_in_scope, variable, regularizer\n        )\n    if base_layer_utils.is_split_variable(variable):\n        for v in variable:\n            backend.track_variable(v)\n            if trainable:\n                self._trainable_weights.append(v)\n            else:\n                self._non_trainable_weights.append(v)\n    else:\n        backend.track_variable(variable)\n        if trainable:\n            self._trainable_weights.append(variable)\n        else:\n            self._non_trainable_weights.append(variable)\n    return variable\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.build","title":"<code>mono_dense_keras._components.mono_dense_layer.MonoDense.build(input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]) -&gt; None</code>","text":"<p>Build</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple</code> <p>input tensor</p> required <code>args</code> <code>List[Any]</code> <p>positional arguments passed to Dense.build()</p> <code>()</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>keyword arguments passed to Dense.build()</p> <code>{}</code> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def build(\n    self, input_shape: Tuple, *args: List[Any], **kwargs: Dict[str, Any]\n) -&gt; None:\n\"\"\"Build\n\n    Args:\n        input_shape: input tensor\n        args: positional arguments passed to Dense.build()\n        kwargs: keyword arguments passed to Dense.build()\n    \"\"\"\n    super(MonoDense, self).build(input_shape, *args, **kwargs)\n    self.monotonicity_indicator = get_monotonicity_indicator(\n        monotonicity_indicator=self.monotonicity_indicator,\n        input_shape=input_shape,\n        units=self.units,\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#mono_dense_keras._components.mono_dense_layer.MonoDense.call","title":"<code>mono_dense_keras._components.mono_dense_layer.MonoDense.call(inputs: TensorLike) -&gt; TensorLike</code>","text":"<p>Call</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>TensorLike</code> <p>input tensor of shape (batch_size, ..., x_length)</p> required <p>Returns:</p> Type Description <code>TensorLike</code> <p>N-D tensor with shape: <code>(batch_size, ..., units)</code>.</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>def call(self, inputs: TensorLike) -&gt; TensorLike:\n\"\"\"Call\n\n    Args:\n        inputs: input tensor of shape (batch_size, ..., x_length)\n\n    Returns:\n        N-D tensor with shape: `(batch_size, ..., units)`.\n\n    \"\"\"\n    # calculate W'*x+y after we replace the kernal according to monotonicity vector\n    with replace_kernel_using_monotonicity_indicator(\n        self, monotonicity_indicator=self.monotonicity_indicator\n    ):\n        h = super(MonoDense, self).call(inputs)\n\n    y = apply_activations(\n        h,\n        units=self.units,\n        convex_activation=self.convex_activation,\n        concave_activation=self.concave_activation,\n        saturated_activation=self.saturated_activation,\n        is_convex=self.is_convex,\n        is_concave=self.is_concave,\n        activation_weights=self.activation_weights,\n    )\n\n    return y\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.compute_mask","title":"<code>keras.engine.base_layer.Layer.compute_mask(inputs, mask = None)</code>","text":"<p>Computes an output mask tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Tensor or list of tensors.</p> required <code>mask</code> <p>Tensor or list of tensors.</p> <code>None</code> <p>Returns:</p> Type Description <p>None or a tensor (or list of tensors, one per output tensor of the layer).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@generic_utils.default\ndef compute_mask(self, inputs, mask=None):\n\"\"\"Computes an output mask tensor.\n\n    Args:\n        inputs: Tensor or list of tensors.\n        mask: Tensor or list of tensors.\n\n    Returns:\n        None or a tensor (or list of tensors,\n            one per output tensor of the layer).\n    \"\"\"\n    if not self._supports_masking:\n        if any(m is not None for m in tf.nest.flatten(mask)):\n            raise TypeError(\n                \"Layer \" + self.name + \" does not support masking, \"\n                \"but was passed an input_mask: \" + str(mask)\n            )\n        # masking not explicitly supported: return None as mask.\n        return None\n    # if masking is explicitly supported, by default\n    # carry over the input mask\n    return mask\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.compute_output_signature","title":"<code>keras.engine.base_layer.Layer.compute_output_signature(input_signature)</code>","text":"<p>Compute the output tensor signature of the layer based on the inputs.</p> <p>Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use <code>compute_output_shape</code>, and will assume that the output dtype matches the input dtype.</p> <p>Parameters:</p> Name Type Description Default <code>input_signature</code> <p>Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer.</p> required <p>Returns:</p> Type Description <p>Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_signature contains a non-TensorSpec object.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.for_subclass_implementers\ndef compute_output_signature(self, input_signature):\n\"\"\"Compute the output tensor signature of the layer based on the inputs.\n\n    Unlike a TensorShape object, a TensorSpec object contains both shape\n    and dtype information for a tensor. This method allows layers to provide\n    output dtype information if it is different from the input dtype.\n    For any layer that doesn't implement this function,\n    the framework will fall back to use `compute_output_shape`, and will\n    assume that the output dtype matches the input dtype.\n\n    Args:\n      input_signature: Single TensorSpec or nested structure of TensorSpec\n        objects, describing a candidate input for the layer.\n\n    Returns:\n      Single TensorSpec or nested structure of TensorSpec objects,\n        describing how the layer would transform the provided input.\n\n    Raises:\n      TypeError: If input_signature contains a non-TensorSpec object.\n    \"\"\"\n\n    def check_type_return_shape(s):\n        if not isinstance(s, tf.TensorSpec):\n            raise TypeError(\n                \"Only TensorSpec signature types are supported. \"\n                f\"Received: {s}.\"\n            )\n        return s.shape\n\n    input_shape = tf.nest.map_structure(\n        check_type_return_shape, input_signature\n    )\n    output_shape = self.compute_output_shape(input_shape)\n    dtype = self._compute_dtype\n    if dtype is None:\n        input_dtypes = [s.dtype for s in tf.nest.flatten(input_signature)]\n        # Default behavior when self.dtype is None, is to use the first\n        # input's dtype.\n        dtype = input_dtypes[0]\n    return tf.nest.map_structure(\n        lambda s: tf.TensorSpec(dtype=dtype, shape=s), output_shape\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.count_params","title":"<code>keras.engine.base_layer.Layer.count_params()</code>","text":"<p>Count the total number of scalars composing the weights.</p> <p>Returns:</p> Type Description <p>An integer count.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the layer isn't yet built (in which case its weights aren't yet defined).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def count_params(self):\n\"\"\"Count the total number of scalars composing the weights.\n\n    Returns:\n        An integer count.\n\n    Raises:\n        ValueError: if the layer isn't yet built\n          (in which case its weights aren't yet defined).\n    \"\"\"\n    if not self.built:\n        if getattr(self, \"_is_graph_network\", False):\n            with tf_utils.maybe_init_scope(self):\n                self._maybe_build(self.inputs)\n        else:\n            raise ValueError(\n                \"You tried to call `count_params` \"\n                f\"on layer {self.name}\"\n                \", but the layer isn't built. \"\n                \"You can build it manually via: \"\n                f\"`{self.name}.build(batch_input_shape)`.\"\n            )\n    return layer_utils.count_params(self.weights)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.finalize_state","title":"<code>keras.engine.base_layer.Layer.finalize_state()</code>","text":"<p>Finalizes the layers state after updating layer weights.</p> <p>This function can be subclassed in a layer and will be called after updating a layer weights. It can be overridden to finalize any additional layer state after a weight update.</p> <p>This function will be called after weights of a layer have been restored from a loaded model.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_generate_docs\ndef finalize_state(self):\n\"\"\"Finalizes the layers state after updating layer weights.\n\n    This function can be subclassed in a layer and will be called after\n    updating a layer weights. It can be overridden to finalize any\n    additional layer state after a weight update.\n\n    This function will be called after weights of a layer have been restored\n    from a loaded model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_at","title":"<code>keras.engine.base_layer.Layer.get_input_at(node_index)</code>","text":"<p>Retrieves the input tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first input node of the layer.</p> required <p>Returns:</p> Type Description <p>A tensor (or list of tensors if the layer has multiple inputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_at(self, node_index):\n\"\"\"Retrieves the input tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first input node of the layer.\n\n    Returns:\n        A tensor (or list of tensors if the layer has multiple inputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"input_tensors\", \"input\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_mask_at","title":"<code>keras.engine.base_layer.Layer.get_input_mask_at(node_index)</code>","text":"<p>Retrieves the input mask tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A mask tensor</p> <p>(or list of tensors if the layer has multiple inputs).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_mask_at(self, node_index):\n\"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A mask tensor\n        (or list of tensors if the layer has multiple inputs).\n    \"\"\"\n    inputs = self.get_input_at(node_index)\n    if isinstance(inputs, list):\n        return [getattr(x, \"_keras_mask\", None) for x in inputs]\n    else:\n        return getattr(inputs, \"_keras_mask\", None)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_input_shape_at","title":"<code>keras.engine.base_layer.Layer.get_input_shape_at(node_index)</code>","text":"<p>Retrieves the input shape(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A shape tuple</p> <p>(or list of shape tuples if the layer has multiple inputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_input_shape_at(self, node_index):\n\"\"\"Retrieves the input shape(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A shape tuple\n        (or list of shape tuples if the layer has multiple inputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"input_shapes\", \"input shape\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_at","title":"<code>keras.engine.base_layer.Layer.get_output_at(node_index)</code>","text":"<p>Retrieves the output tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first output node of the layer.</p> required <p>Returns:</p> Type Description <p>A tensor (or list of tensors if the layer has multiple outputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_at(self, node_index):\n\"\"\"Retrieves the output tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first output node of the layer.\n\n    Returns:\n        A tensor (or list of tensors if the layer has multiple outputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"output_tensors\", \"output\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_mask_at","title":"<code>keras.engine.base_layer.Layer.get_output_mask_at(node_index)</code>","text":"<p>Retrieves the output mask tensor(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A mask tensor</p> <p>(or list of tensors if the layer has multiple outputs).</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_mask_at(self, node_index):\n\"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A mask tensor\n        (or list of tensors if the layer has multiple outputs).\n    \"\"\"\n    output = self.get_output_at(node_index)\n    if isinstance(output, list):\n        return [getattr(x, \"_keras_mask\", None) for x in output]\n    else:\n        return getattr(output, \"_keras_mask\", None)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_output_shape_at","title":"<code>keras.engine.base_layer.Layer.get_output_shape_at(node_index)</code>","text":"<p>Retrieves the output shape(s) of a layer at a given node.</p> <p>Parameters:</p> Name Type Description Default <code>node_index</code> <p>Integer, index of the node from which to retrieve the attribute. E.g. <code>node_index=0</code> will correspond to the first time the layer was called.</p> required <p>Returns:</p> Type Description <p>A shape tuple</p> <p>(or list of shape tuples if the layer has multiple outputs).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If called in Eager mode.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>@doc_controls.do_not_doc_inheritable\ndef get_output_shape_at(self, node_index):\n\"\"\"Retrieves the output shape(s) of a layer at a given node.\n\n    Args:\n        node_index: Integer, index of the node\n            from which to retrieve the attribute.\n            E.g. `node_index=0` will correspond to the\n            first time the layer was called.\n\n    Returns:\n        A shape tuple\n        (or list of shape tuples if the layer has multiple outputs).\n\n    Raises:\n      RuntimeError: If called in Eager mode.\n    \"\"\"\n    return self._get_node_attribute_at_index(\n        node_index, \"output_shapes\", \"output shape\"\n    )\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.get_weights","title":"<code>keras.engine.base_layer.Layer.get_weights()</code>","text":"<p>Returns the current weights of the layer, as NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of NumPy arrays, which can in turn be used to load state into similarly parameterized layers.</p> <p>For example, a <code>Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code>Dense</code> layer:</p> <p>layer_a = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.],        [2.],        [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)]</p> <p>Returns:</p> Type Description <p>Weights values as a list of NumPy arrays.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def get_weights(self):\n\"\"\"Returns the current weights of the layer, as NumPy arrays.\n\n    The weights of a layer represent the state of the layer. This function\n    returns both trainable and non-trainable weight values associated with\n    this layer as a list of NumPy arrays, which can in turn be used to load\n    state into similarly parameterized layers.\n\n    For example, a `Dense` layer returns a list of two values: the kernel\n    matrix and the bias vector. These can be used to set the weights of\n    another `Dense` layer:\n\n    &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(1.))\n    &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n    &gt;&gt;&gt; layer_a.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(2.))\n    &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[2.],\n           [2.],\n           [2.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n\n    Returns:\n        Weights values as a list of NumPy arrays.\n    \"\"\"\n    weights = self.weights\n    output_weights = []\n    for weight in weights:\n        if isinstance(weight, base_layer_utils.TrackableWeightHandler):\n            output_weights.extend(weight.get_tensors())\n        else:\n            output_weights.append(weight)\n    return backend.batch_get_value(output_weights)\n</code></pre>"},{"location":"api/mono_dense_keras/MonoDense/#keras.engine.base_layer.Layer.set_weights","title":"<code>keras.engine.base_layer.Layer.set_weights(weights)</code>","text":"<p>Sets the weights of the layer, from NumPy arrays.</p> <p>The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function, by calling the layer.</p> <p>For example, a <code>Dense</code> layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another <code>Dense</code> layer:</p> <p>layer_a = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(1.)) a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) layer_a.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)] layer_b = tf.keras.layers.Dense(1, ...   kernel_initializer=tf.constant_initializer(2.)) b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) layer_b.get_weights() [array([[2.],        [2.],        [2.]], dtype=float32), array([0.], dtype=float32)] layer_b.set_weights(layer_a.get_weights()) layer_b.get_weights() [array([[1.],        [1.],        [1.]], dtype=float32), array([0.], dtype=float32)]</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <p>a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of <code>get_weights</code>).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided weights list does not match the layer's specifications.</p> Source code in <code>/opt/hostedtoolcache/Python/3.9.16/x64/lib/python3.9/site-packages/keras/engine/base_layer.py</code> <pre><code>def set_weights(self, weights):\n\"\"\"Sets the weights of the layer, from NumPy arrays.\n\n    The weights of a layer represent the state of the layer. This function\n    sets the weight values from numpy arrays. The weight values should be\n    passed in the order they are created by the layer. Note that the layer's\n    weights must be instantiated before calling this function, by calling\n    the layer.\n\n    For example, a `Dense` layer returns a list of two values: the kernel\n    matrix and the bias vector. These can be used to set the weights of\n    another `Dense` layer:\n\n    &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(1.))\n    &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n    &gt;&gt;&gt; layer_a.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n    ...   kernel_initializer=tf.constant_initializer(2.))\n    &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[2.],\n           [2.],\n           [2.]], dtype=float32), array([0.], dtype=float32)]\n    &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n    &gt;&gt;&gt; layer_b.get_weights()\n    [array([[1.],\n           [1.],\n           [1.]], dtype=float32), array([0.], dtype=float32)]\n\n    Args:\n      weights: a list of NumPy arrays. The number\n        of arrays and their shape must match\n        number of the dimensions of the weights\n        of the layer (i.e. it should match the\n        output of `get_weights`).\n\n    Raises:\n      ValueError: If the provided weights list does not match the\n        layer's specifications.\n    \"\"\"\n    params = self.weights\n\n    expected_num_weights = 0\n    for param in params:\n        if isinstance(param, base_layer_utils.TrackableWeightHandler):\n            expected_num_weights += param.num_tensors\n        else:\n            expected_num_weights += 1\n\n    if expected_num_weights != len(weights):\n        raise ValueError(\n            'You called `set_weights(weights)` on layer \"%s\" '\n            \"with a weight list of length %s, but the layer was \"\n            \"expecting %s weights. Provided weights: %s...\"\n            % (\n                self.name,\n                len(weights),\n                expected_num_weights,\n                str(weights)[:50],\n            )\n        )\n\n    weight_index = 0\n    weight_value_tuples = []\n    for param in params:\n        if isinstance(param, base_layer_utils.TrackableWeightHandler):\n            num_tensors = param.num_tensors\n            tensors = weights[weight_index : weight_index + num_tensors]\n            param.set_weights(tensors)\n            weight_index += num_tensors\n        else:\n            weight = weights[weight_index]\n            weight_shape = weight.shape if hasattr(weight, \"shape\") else ()\n            ref_shape = param.shape\n            if not ref_shape.is_compatible_with(weight_shape):\n                raise ValueError(\n                    f\"Layer {self.name} weight shape {ref_shape} \"\n                    \"is not compatible with provided weight \"\n                    f\"shape {weight_shape}.\"\n                )\n            weight_value_tuples.append((param, weight))\n            weight_index += 1\n\n    backend.batch_set_value(weight_value_tuples)\n\n    # Perform any layer defined finalization of the layer state.\n    for layer in self._flatten_layers():\n        layer.finalize_state()\n</code></pre>"},{"location":"api/mono_dense_keras/create_type_1/","title":"create_type_1","text":""},{"location":"api/mono_dense_keras/create_type_1/#mono_dense_keras.create_type_1","title":"<code>mono_dense_keras.create_type_1(inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]], *, units: int, final_units: int, activation: Union[str, Callable[[TensorLike], TensorLike]], n_layers: int, final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None, monotonicity_indicator: Union[int, Dict[str, int], List[int]] = 1, is_convex: Union[bool, Dict[str, bool], List[bool]] = False, is_concave: Union[bool, Dict[str, bool], List[bool]] = False, dropout: Optional[float] = None) -&gt; TensorLike</code>","text":"<p>Builds Type-1 monotonic network</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[TensorLike, Dict[str, TensorLike], List[TensorLike]]</code> <p>input tensor or a dictionary of tensors</p> required <code>units</code> <code>int</code> <p>number of units in hidden layers</p> required <code>final_units</code> <code>int</code> <p>number of units in the output layer</p> required <code>activation</code> <code>Union[str, Callable[[TensorLike], TensorLike]]</code> <p>the base activation function</p> required <code>n_layers</code> <code>int</code> <p>total number of layers (hidden layers plus the output layer)</p> required <code>final_activation</code> <code>Optional[Union[str, Callable[[TensorLike], TensorLike]]]</code> <p>the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used.</p> <code>None</code> <code>monotonicity_indicator</code> <code>Union[int, Dict[str, int], List[int]]</code> <p>if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator.</p> <code>1</code> <code>is_convex</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular input feature is convex</p> <code>False</code> <code>is_concave</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular inputs feature is concave</p> <code>False</code> <code>dropout</code> <code>Optional[float]</code> <p>dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorLike</code> <p>Output tensor</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>@export\ndef create_type_1(\n    inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]],\n    *,\n    units: int,\n    final_units: int,\n    activation: Union[str, Callable[[TensorLike], TensorLike]],\n    n_layers: int,\n    final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    monotonicity_indicator: Union[int, Dict[str, int], List[int]] = 1,\n    is_convex: Union[bool, Dict[str, bool], List[bool]] = False,\n    is_concave: Union[bool, Dict[str, bool], List[bool]] = False,\n    dropout: Optional[float] = None,\n) -&gt; TensorLike:\n\"\"\"Builds Type-1 monotonic network\n\n    Args:\n        inputs: input tensor or a dictionary of tensors\n        units: number of units in hidden layers\n        final_units: number of units in the output layer\n        activation: the base activation function\n        n_layers: total number of layers (hidden layers plus the output layer)\n        final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear).\n            If set to None (default value), then the linear activation is used.\n        monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity\n            indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int,\n            then all input features are set to the same monotinicity indicator.\n        is_convex: set to True if a particular input feature is convex\n        is_concave: set to True if a particular inputs feature is concave\n        dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.\n\n    Returns:\n        Output tensor\n\n    \"\"\"\n    _, is_convex, _ = _prepare_mono_input_n_param(inputs, is_convex)\n    _, is_concave, _ = _prepare_mono_input_n_param(inputs, is_concave)\n    x, monotonicity_indicator, names = _prepare_mono_input_n_param(\n        inputs, monotonicity_indicator\n    )\n    has_convex, has_concave = _check_convexity_params(\n        monotonicity_indicator, is_convex, is_concave, names\n    )\n\n    y = tf.keras.layers.Concatenate()(x)\n\n    y = _create_mono_block(\n        units=[units] * (n_layers - 1) + [final_units],\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator,\n        is_convex=has_convex,\n        is_concave=has_concave and not has_convex,\n        dropout=dropout,\n    )(y)\n\n    if final_activation is not None:\n        final_activation = tf.keras.activations.get(final_activation)\n        y = final_activation(y)\n\n    return y\n</code></pre>"},{"location":"api/mono_dense_keras/create_type_2/","title":"create_type_2","text":""},{"location":"api/mono_dense_keras/create_type_2/#mono_dense_keras.create_type_2","title":"<code>mono_dense_keras.create_type_2(inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]], *, input_units: Optional[int] = None, units: int, final_units: int, activation: Union[str, Callable[[TensorLike], TensorLike]], n_layers: int, final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None, monotonicity_indicator: Union[int, Dict[str, int]] = 1, is_convex: Union[bool, Dict[str, bool], List[bool]] = False, is_concave: Union[bool, Dict[str, bool], List[bool]] = False, dropout: Optional[float] = None) -&gt; TensorLike</code>","text":"<p>Builds Type-2 monotonic network</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[TensorLike, Dict[str, TensorLike], List[TensorLike]]</code> <p>input tensor or a dictionary of tensors</p> required <code>input_units</code> <code>Optional[int]</code> <p>used to preprocess features before entering the common mono block</p> <code>None</code> <code>units</code> <code>int</code> <p>number of units in hidden layers</p> required <code>final_units</code> <code>int</code> <p>number of units in the output layer</p> required <code>activation</code> <code>Union[str, Callable[[TensorLike], TensorLike]]</code> <p>the base activation function</p> required <code>n_layers</code> <code>int</code> <p>total number of layers (hidden layers plus the output layer)</p> required <code>final_activation</code> <code>Optional[Union[str, Callable[[TensorLike], TensorLike]]]</code> <p>the activation function of the final layer (typicall softmax, sigmoid or linear). If set to None (default value), then the linear activation is used.</p> <code>None</code> <code>monotonicity_indicator</code> <code>Union[int, Dict[str, int]]</code> <p>if an instance of dictionary, then maps names of input feature to their monotonicity indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int, then all input features are set to the same monotinicity indicator.</p> <code>1</code> <code>is_convex</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular input feature is convex</p> <code>False</code> <code>is_concave</code> <code>Union[bool, Dict[str, bool], List[bool]]</code> <p>set to True if a particular inputs feature is concave</p> <code>False</code> <code>dropout</code> <code>Optional[float]</code> <p>dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.</p> <code>None</code> <p>Returns:</p> Type Description <code>TensorLike</code> <p>Output tensor</p> Source code in <code>mono_dense_keras/_components/mono_dense_layer.py</code> <pre><code>@export\ndef create_type_2(\n    inputs: Union[TensorLike, Dict[str, TensorLike], List[TensorLike]],\n    *,\n    input_units: Optional[int] = None,\n    units: int,\n    final_units: int,\n    activation: Union[str, Callable[[TensorLike], TensorLike]],\n    n_layers: int,\n    final_activation: Optional[Union[str, Callable[[TensorLike], TensorLike]]] = None,\n    monotonicity_indicator: Union[int, Dict[str, int]] = 1,\n    is_convex: Union[bool, Dict[str, bool], List[bool]] = False,\n    is_concave: Union[bool, Dict[str, bool], List[bool]] = False,\n    dropout: Optional[float] = None,\n) -&gt; TensorLike:\n\"\"\"Builds Type-2 monotonic network\n\n    Args:\n        inputs: input tensor or a dictionary of tensors\n        input_units: used to preprocess features before entering the common mono block\n        units: number of units in hidden layers\n        final_units: number of units in the output layer\n        activation: the base activation function\n        n_layers: total number of layers (hidden layers plus the output layer)\n        final_activation: the activation function of the final layer (typicall softmax, sigmoid or linear).\n            If set to None (default value), then the linear activation is used.\n        monotonicity_indicator: if an instance of dictionary, then maps names of input feature to their monotonicity\n            indicator (-1 for monotonically decreasing, 1 for monotonically increasing and 0 otherwise). If int,\n            then all input features are set to the same monotinicity indicator.\n        is_convex: set to True if a particular input feature is convex\n        is_concave: set to True if a particular inputs feature is concave\n        dropout: dropout rate. If set to float greater than 0, Dropout layers are inserted after hidden layers.\n\n    Returns:\n        Output tensor\n\n    \"\"\"\n    _, is_convex, _ = _prepare_mono_input_n_param(inputs, is_convex)\n    _, is_concave, _ = _prepare_mono_input_n_param(inputs, is_concave)\n    x, monotonicity_indicator, names = _prepare_mono_input_n_param(\n        inputs, monotonicity_indicator\n    )\n    has_convex, has_concave = _check_convexity_params(\n        monotonicity_indicator, is_convex, is_concave, names\n    )\n\n    if input_units is None:\n        input_units = max(units // 4, 1)\n\n    y = [\n        (\n            MonoDense(\n                units=input_units,\n                activation=activation,\n                monotonicity_indicator=monotonicity_indicator[i],\n                is_convex=is_convex[i],\n                is_concave=is_concave[i],\n                name=f\"mono_dense_{names[i]}\"\n                + (\"_increasing\" if monotonicity_indicator[i] == 1 else \"_decreasing\")\n                + (\"_convex\" if is_convex[i] else \"\")\n                + (\"_concave\" if is_concave[i] else \"\"),\n            )\n            if monotonicity_indicator[i] != 0\n            else (\n                Dense(\n                    units=input_units, activation=activation, name=f\"dense_{names[i]}\"\n                )\n            )\n        )(x[i])\n        for i in range(len(inputs))\n    ]\n\n    y = Concatenate(name=\"preprocessed_features\")(y)\n    monotonicity_indicator_block = sum(\n        [[abs(x)] * input_units for x in monotonicity_indicator], []\n    )\n\n    y = _create_mono_block(\n        units=[units] * (n_layers - 1) + [final_units],\n        activation=activation,\n        monotonicity_indicator=monotonicity_indicator_block,\n        is_convex=has_convex,\n        is_concave=has_concave and not has_convex,\n        dropout=dropout,\n    )(y)\n\n    if final_activation is not None:\n        final_activation = tf.keras.activations.get(final_activation)\n        y = final_activation(y)\n\n    return y\n</code></pre>"},{"location":"experiments/AutoMPG/","title":"Auto MPG","text":"<p>The Auto MPG Dataset is a regression dataset [1] with 7 features - Cylinders, Displacement, Horsepower, Weight, Acceleration, Model Year, Origin and the dependant variable is monotonically decreasing with respect to features weigh, displacement, and horsepower. The <code>monotonicity_indicator</code> corrsponding to these features are set to -1, since the relationship is a monotonically decreasing one with respect to the dependant variable.</p> <p>This is a part of comparison with methods and datasets from COMET [2] (Reference #20 in our paper).</p> <p>References:</p> <ol> <li> <p>Ross Quinlan. Combining Instance-Based and Model-Based Learning. In     Proceedings on the Tenth International Conference of Machine     Learning, 236-243, University of Massachusetts, Amherst. Morgan     Kaufmann, 1993.</p> <p>https://archive.ics.uci.edu/ml/datasets/auto+mpg</p> </li> <li> <p>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020.</p> </li> </ol> <pre><code>Github repo: https://github.com/AishwaryaSivaraman/COMET\n</code></pre> <p>These are a few examples of the dataset:</p> 0 1 2 3 4 Cylinders 1.482807 1.482807 1.482807 1.482807 1.482807 Displacement 1.073028 1.482902 1.044432 1.025368 2.235927 Horsepower 0.650564 1.548993 1.163952 0.907258 2.396084 Weight 0.606625 0.828131 0.523413 0.542165 1.587581 Acceleration -1.275546 -1.452517 -1.275546 -1.806460 -1.983431 Model_Year -1.631803 -1.631803 -1.631803 -1.631803 -1.631803 Origin -0.701669 -0.701669 -0.701669 -0.701669 -0.701669 ground_truth 18.000000 15.000000 16.000000 17.000000 15.000000 <pre><code>batch_size = 16\nmax_epochs = 50\n\ntuner = find_hyperparameters(\n    \"auto\",\n    monotonicity_indicator={\n        \"Cylinders\": 0,\n        \"Displacement\": -1,\n        \"Horsepower\": -1,\n        \"Weight\": -1,\n        \"Acceleration\": 0,\n        \"Model_Year\": 0,\n        \"Origin\": 0,\n    },\n    max_trials=200,\n    final_activation=None,\n    loss=\"mse\",\n    metrics=\"mse\",\n    objective=\"val_mse\",\n    direction=\"min\",\n    batch_size=batch_size,\n    max_epochs=max_epochs,    \n    executions_per_trial=3,\n)\n</code></pre> <pre><code>Trial 200 Complete [00h 00m 30s]\nval_mse: 9.106983184814453\n\nBest val_mse So Far: 8.562163988749186\nTotal elapsed time: 01h 30m 07s\nINFO:tensorflow:Oracle triggered exit\n</code></pre> 0 1 2 3 4 units 26 32 19 22 32 n_layers 2 1 1 1 4 activation elu elu elu elu elu learning_rate 0.086301 0.300000 0.169810 0.067666 0.013175 weight_decay 0.147297 0.100000 0.145653 0.138456 0.100000 dropout 0.162063 0.201769 0.175619 0.500000 0.156609 decay_rate 0.927282 1.000000 0.921521 1.000000 1.000000 val_mse_mean 8.368483 8.621466 8.629537 8.635558 8.635864 val_mse_std 0.078852 0.187314 0.077030 0.070520 0.220399 val_mse_min 8.276224 8.382471 8.510103 8.543325 8.301335 val_mse_max 8.490236 8.862036 8.698004 8.741608 8.895535 params 1229 169 85 106 4081"},{"location":"experiments/Blog/","title":"Blog feedback","text":"<p>Blog Feedback [1] is a dataset containing 54,270 data points from blog posts. The raw HTML-documents of the blog posts were crawled and processed. The prediction task associated with the data is the prediction of the number of comments in the upcoming 24 hours. The feature of the dataset has 276 dimensions, and 8 attributes among them should be monotonically non-decreasing with the prediction. They are A51, A52, A53, A54, A56, A57, A58, A59. Thus the <code>monotonicity_indicator</code> corrsponding to these features are set to 1. As done in [2], we only use the data points with targets smaller than the 90th percentile.</p> <p>References:</p> <ol> <li>Krisztian Buza. Feedback prediction for blogs. In Data analysis,     machine learning and knowledge discovery, pages 145\u2013152. Springer,     2014</li> <li>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</li> </ol> <p>These are a few examples of the dataset:</p> 0 1 2 3 4 feature_0 0.001920 0.001920 0.000640 0.001920 0.001920 feature_1 0.001825 0.001825 0.001825 0.000000 0.000000 feature_2 0.002920 0.002920 0.000000 0.001460 0.001460 feature_3 0.001627 0.001627 0.000651 0.001627 0.001627 feature_4 0.000000 0.000000 0.000000 0.000000 0.000000 feature_5 0.000000 0.000000 0.000000 0.000000 0.000000 feature_6 0.000000 0.000000 0.000000 0.000000 0.000000 feature_7 0.000000 0.000000 0.000000 0.000000 0.000000 feature_8 0.035901 0.035901 0.035901 0.035901 0.035901 feature_9 0.096250 0.096250 0.096250 0.096250 0.096250 feature_10 0.000000 0.000000 0.000000 0.000000 0.000000 feature_11 0.196184 0.196184 0.196184 0.196184 0.196184 feature_12 0.011416 0.011416 0.011416 0.011416 0.011416 feature_13 0.035070 0.035070 0.035070 0.035070 0.035070 feature_14 0.090234 0.090234 0.090234 0.090234 0.090234 feature_15 0.000000 0.000000 0.000000 0.000000 0.000000 feature_16 0.264747 0.264747 0.264747 0.264747 0.264747 feature_17 0.005102 0.005102 0.005102 0.005102 0.005102 feature_18 0.032064 0.032064 0.032064 0.032064 0.032064 feature_19 0.089666 0.089666 0.089666 0.089666 0.089666 feature_20 0.264747 0.264747 0.264747 0.264747 0.264747 feature_21 0.003401 0.003401 0.003401 0.003401 0.003401 feature_22 0.031368 0.031368 0.031368 0.031368 0.031368 feature_23 0.083403 0.083403 0.083403 0.083403 0.083403 feature_24 0.000000 0.000000 0.000000 0.000000 0.000000 feature_25 0.195652 0.195652 0.195652 0.195652 0.195652 feature_26 0.009302 0.009302 0.009302 0.009302 0.009302 feature_27 0.068459 0.068459 0.068459 0.068459 0.068459 feature_28 0.085496 0.085496 0.085496 0.085496 0.085496 feature_29 0.716561 0.716561 0.716561 0.716561 0.716561 feature_30 0.265120 0.265120 0.265120 0.265120 0.265120 feature_31 0.419453 0.419453 0.419453 0.419453 0.419453 feature_32 0.120206 0.120206 0.120206 0.120206 0.120206 feature_33 0.345656 0.345656 0.345656 0.345656 0.345656 feature_34 0.000000 0.000000 0.000000 0.000000 0.000000 feature_35 0.366667 0.366667 0.366667 0.366667 0.366667 feature_36 0.000000 0.000000 0.000000 0.000000 0.000000 feature_37 0.126985 0.126985 0.126985 0.126985 0.126985 feature_38 0.226342 0.226342 0.226342 0.226342 0.226342 feature_39 0.375000 0.375000 0.375000 0.375000 0.375000 feature_40 0.000000 0.000000 0.000000 0.000000 0.000000 feature_41 0.125853 0.125853 0.125853 0.125853 0.125853 feature_42 0.224422 0.224422 0.224422 0.224422 0.224422 feature_43 0.375000 0.375000 0.375000 0.375000 0.375000 feature_44 0.000000 0.000000 0.000000 0.000000 0.000000 feature_45 0.114587 0.114587 0.114587 0.114587 0.114587 feature_46 0.343826 0.343826 0.343826 0.343826 0.343826 feature_47 0.000000 0.000000 0.000000 0.000000 0.000000 feature_48 0.384615 0.384615 0.384615 0.384615 0.384615 feature_49 0.000000 0.000000 0.000000 0.000000 0.000000 feature_50 0.108675 0.108675 0.108675 0.108675 0.108675 feature_51 0.195570 0.195570 0.195570 0.195570 0.195570 feature_52 0.600000 0.600000 0.600000 0.600000 0.600000 feature_53 0.391304 0.391304 0.391304 0.391304 0.391304 feature_54 0.333333 0.333333 0.333333 0.333333 0.333333 feature_55 0.516725 0.516725 0.518486 0.516725 0.516725 feature_56 0.550000 0.550000 0.550000 0.550000 0.550000 feature_57 0.486111 0.486111 0.138889 0.819444 0.819444 feature_58 0.000000 0.000000 0.000000 0.000000 0.000000 feature_59 0.000000 0.000000 0.000000 0.000000 0.000000 feature_60 0.000000 0.000000 0.000000 0.000000 0.000000 feature_61 0.000000 0.000000 0.000000 0.000000 0.000000 feature_62 0.000000 0.000000 0.000000 0.000000 0.000000 feature_63 0.000000 0.000000 0.000000 0.000000 0.000000 feature_64 0.000000 0.000000 0.000000 0.000000 0.000000 feature_65 0.000000 0.000000 0.000000 0.000000 0.000000 feature_66 0.000000 0.000000 0.000000 0.000000 0.000000 feature_67 0.000000 0.000000 0.000000 0.000000 0.000000 feature_68 0.000000 0.000000 0.000000 0.000000 0.000000 feature_69 0.000000 0.000000 0.000000 0.000000 0.000000 feature_70 0.000000 0.000000 0.000000 0.000000 0.000000 feature_71 0.000000 0.000000 0.000000 0.000000 0.000000 feature_72 0.000000 0.000000 0.000000 0.000000 0.000000 feature_73 0.000000 0.000000 0.000000 0.000000 0.000000 feature_74 0.000000 0.000000 0.000000 0.000000 0.000000 feature_75 0.000000 0.000000 0.000000 0.000000 0.000000 feature_76 0.000000 0.000000 0.000000 0.000000 0.000000 feature_77 0.000000 0.000000 0.000000 0.000000 0.000000 feature_78 0.000000 0.000000 0.000000 0.000000 0.000000 feature_79 0.000000 0.000000 0.000000 0.000000 0.000000 feature_80 0.000000 0.000000 0.000000 0.000000 0.000000 feature_81 0.000000 0.000000 0.000000 0.000000 0.000000 feature_82 0.000000 0.000000 0.000000 0.000000 0.000000 feature_83 0.000000 0.000000 0.000000 0.000000 0.000000 feature_84 0.000000 0.000000 0.000000 0.000000 0.000000 feature_85 0.000000 0.000000 0.000000 0.000000 0.000000 feature_86 0.000000 0.000000 0.000000 0.000000 0.000000 feature_87 0.000000 0.000000 0.000000 0.000000 0.000000 feature_88 0.000000 0.000000 0.000000 0.000000 0.000000 feature_89 0.000000 0.000000 0.000000 0.000000 0.000000 feature_90 0.000000 0.000000 0.000000 0.000000 0.000000 feature_91 0.000000 0.000000 0.000000 0.000000 0.000000 feature_92 0.000000 0.000000 0.000000 0.000000 0.000000 feature_93 0.000000 0.000000 0.000000 0.000000 0.000000 feature_94 0.000000 0.000000 0.000000 0.000000 0.000000 feature_95 0.000000 0.000000 0.000000 0.000000 0.000000 feature_96 0.000000 0.000000 0.000000 0.000000 0.000000 feature_97 0.000000 0.000000 0.000000 0.000000 0.000000 feature_98 0.000000 0.000000 0.000000 0.000000 0.000000 feature_99 0.000000 0.000000 0.000000 0.000000 0.000000 feature_100 0.000000 0.000000 0.000000 0.000000 0.000000 feature_101 0.000000 0.000000 0.000000 0.000000 0.000000 feature_102 0.000000 0.000000 0.000000 0.000000 0.000000 feature_103 0.000000 0.000000 0.000000 0.000000 0.000000 feature_104 0.000000 0.000000 0.000000 0.000000 0.000000 feature_105 0.000000 0.000000 0.000000 0.000000 0.000000 feature_106 0.000000 0.000000 0.000000 0.000000 0.000000 feature_107 0.000000 0.000000 0.000000 0.000000 0.000000 feature_108 0.000000 0.000000 0.000000 0.000000 0.000000 feature_109 0.000000 0.000000 0.000000 0.000000 0.000000 feature_110 0.000000 0.000000 0.000000 0.000000 0.000000 feature_111 0.000000 0.000000 0.000000 0.000000 0.000000 feature_112 0.000000 0.000000 0.000000 0.000000 0.000000 feature_113 0.000000 0.000000 0.000000 0.000000 0.000000 feature_114 0.000000 0.000000 0.000000 0.000000 0.000000 feature_115 0.000000 0.000000 0.000000 0.000000 0.000000 feature_116 0.000000 0.000000 0.000000 0.000000 0.000000 feature_117 0.000000 0.000000 0.000000 0.000000 0.000000 feature_118 0.000000 0.000000 0.000000 0.000000 0.000000 feature_119 0.000000 0.000000 0.000000 0.000000 0.000000 feature_120 0.000000 0.000000 0.000000 0.000000 0.000000 feature_121 0.000000 0.000000 0.000000 0.000000 0.000000 feature_122 0.000000 0.000000 0.000000 0.000000 0.000000 feature_123 0.000000 0.000000 0.000000 0.000000 0.000000 feature_124 0.000000 0.000000 0.000000 0.000000 0.000000 feature_125 0.000000 0.000000 0.000000 0.000000 0.000000 feature_126 0.000000 0.000000 0.000000 0.000000 0.000000 feature_127 0.000000 0.000000 0.000000 0.000000 0.000000 feature_128 0.000000 0.000000 0.000000 0.000000 0.000000 feature_129 0.000000 0.000000 0.000000 0.000000 0.000000 feature_130 0.000000 0.000000 0.000000 0.000000 0.000000 feature_131 0.000000 0.000000 0.000000 0.000000 0.000000 feature_132 0.000000 0.000000 0.000000 0.000000 0.000000 feature_133 0.000000 0.000000 0.000000 0.000000 0.000000 feature_134 0.000000 0.000000 0.000000 0.000000 0.000000 feature_135 0.000000 0.000000 0.000000 0.000000 0.000000 feature_136 0.000000 0.000000 0.000000 0.000000 0.000000 feature_137 0.000000 0.000000 0.000000 0.000000 0.000000 feature_138 0.000000 0.000000 0.000000 0.000000 0.000000 feature_139 0.000000 0.000000 0.000000 0.000000 0.000000 feature_140 0.000000 0.000000 0.000000 0.000000 0.000000 feature_141 0.000000 0.000000 0.000000 0.000000 0.000000 feature_142 0.000000 0.000000 0.000000 0.000000 0.000000 feature_143 0.000000 0.000000 0.000000 0.000000 0.000000 feature_144 0.000000 0.000000 0.000000 0.000000 0.000000 feature_145 0.000000 0.000000 0.000000 0.000000 0.000000 feature_146 0.000000 0.000000 0.000000 0.000000 0.000000 feature_147 0.000000 0.000000 0.000000 0.000000 0.000000 feature_148 0.000000 0.000000 0.000000 0.000000 0.000000 feature_149 0.000000 0.000000 0.000000 0.000000 0.000000 feature_150 0.000000 0.000000 0.000000 0.000000 0.000000 feature_151 0.000000 0.000000 0.000000 0.000000 0.000000 feature_152 0.000000 0.000000 0.000000 0.000000 0.000000 feature_153 0.000000 0.000000 0.000000 0.000000 0.000000 feature_154 0.000000 0.000000 0.000000 0.000000 0.000000 feature_155 0.000000 0.000000 0.000000 0.000000 0.000000 feature_156 0.000000 0.000000 0.000000 0.000000 0.000000 feature_157 0.000000 0.000000 0.000000 0.000000 0.000000 feature_158 0.000000 0.000000 0.000000 0.000000 0.000000 feature_159 0.000000 0.000000 0.000000 0.000000 0.000000 feature_160 0.000000 0.000000 0.000000 0.000000 0.000000 feature_161 0.000000 0.000000 0.000000 0.000000 0.000000 feature_162 0.000000 0.000000 0.000000 0.000000 0.000000 feature_163 0.000000 0.000000 0.000000 0.000000 0.000000 feature_164 0.000000 0.000000 0.000000 0.000000 0.000000 feature_165 0.000000 0.000000 0.000000 0.000000 0.000000 feature_166 0.000000 0.000000 0.000000 0.000000 0.000000 feature_167 0.000000 0.000000 0.000000 0.000000 0.000000 feature_168 0.000000 0.000000 0.000000 0.000000 0.000000 feature_169 0.000000 0.000000 0.000000 0.000000 0.000000 feature_170 0.000000 0.000000 0.000000 0.000000 0.000000 feature_171 0.000000 0.000000 0.000000 0.000000 0.000000 feature_172 0.000000 0.000000 0.000000 0.000000 0.000000 feature_173 0.000000 0.000000 0.000000 0.000000 0.000000 feature_174 0.000000 0.000000 0.000000 0.000000 0.000000 feature_175 0.000000 0.000000 0.000000 0.000000 0.000000 feature_176 0.000000 0.000000 0.000000 0.000000 0.000000 feature_177 0.000000 0.000000 0.000000 0.000000 0.000000 feature_178 0.000000 0.000000 0.000000 0.000000 0.000000 feature_179 0.000000 0.000000 0.000000 0.000000 0.000000 feature_180 0.000000 0.000000 0.000000 0.000000 0.000000 feature_181 0.000000 0.000000 0.000000 0.000000 0.000000 feature_182 0.000000 0.000000 0.000000 0.000000 0.000000 feature_183 0.000000 0.000000 0.000000 0.000000 0.000000 feature_184 0.000000 0.000000 0.000000 0.000000 0.000000 feature_185 0.000000 0.000000 0.000000 0.000000 0.000000 feature_186 0.000000 0.000000 0.000000 0.000000 0.000000 feature_187 0.000000 0.000000 0.000000 0.000000 0.000000 feature_188 0.000000 0.000000 0.000000 0.000000 0.000000 feature_189 0.000000 0.000000 0.000000 0.000000 0.000000 feature_190 0.000000 0.000000 0.000000 0.000000 0.000000 feature_191 0.000000 0.000000 0.000000 0.000000 0.000000 feature_192 0.000000 0.000000 0.000000 0.000000 0.000000 feature_193 0.000000 0.000000 0.000000 0.000000 0.000000 feature_194 0.000000 0.000000 0.000000 0.000000 0.000000 feature_195 0.000000 0.000000 0.000000 0.000000 0.000000 feature_196 0.000000 0.000000 0.000000 0.000000 0.000000 feature_197 0.000000 0.000000 0.000000 0.000000 0.000000 feature_198 0.000000 0.000000 0.000000 0.000000 0.000000 feature_199 0.000000 0.000000 0.000000 0.000000 0.000000 feature_200 0.000000 0.000000 0.000000 0.000000 0.000000 feature_201 0.000000 0.000000 0.000000 0.000000 0.000000 feature_202 0.000000 0.000000 0.000000 0.000000 0.000000 feature_203 0.000000 0.000000 0.000000 0.000000 0.000000 feature_204 0.000000 0.000000 0.000000 0.000000 0.000000 feature_205 0.000000 0.000000 0.000000 0.000000 0.000000 feature_206 0.000000 0.000000 0.000000 0.000000 0.000000 feature_207 0.000000 0.000000 0.000000 0.000000 0.000000 feature_208 0.000000 0.000000 0.000000 0.000000 0.000000 feature_209 0.000000 0.000000 0.000000 0.000000 0.000000 feature_210 0.000000 0.000000 0.000000 0.000000 0.000000 feature_211 0.000000 0.000000 0.000000 0.000000 0.000000 feature_212 0.000000 0.000000 0.000000 0.000000 0.000000 feature_213 0.000000 0.000000 0.000000 0.000000 0.000000 feature_214 0.000000 0.000000 0.000000 0.000000 0.000000 feature_215 0.000000 0.000000 0.000000 0.000000 0.000000 feature_216 0.000000 0.000000 0.000000 0.000000 0.000000 feature_217 0.000000 0.000000 0.000000 0.000000 0.000000 feature_218 0.000000 0.000000 0.000000 0.000000 0.000000 feature_219 0.000000 0.000000 0.000000 0.000000 0.000000 feature_220 0.000000 0.000000 0.000000 0.000000 0.000000 feature_221 0.000000 0.000000 0.000000 0.000000 0.000000 feature_222 0.000000 0.000000 0.000000 0.000000 0.000000 feature_223 0.000000 0.000000 0.000000 0.000000 0.000000 feature_224 0.000000 0.000000 0.000000 0.000000 0.000000 feature_225 0.000000 0.000000 0.000000 0.000000 0.000000 feature_226 0.000000 0.000000 0.000000 0.000000 0.000000 feature_227 0.000000 0.000000 0.000000 0.000000 0.000000 feature_228 0.000000 0.000000 0.000000 0.000000 0.000000 feature_229 0.000000 0.000000 0.000000 0.000000 0.000000 feature_230 0.000000 0.000000 0.000000 0.000000 0.000000 feature_231 0.000000 0.000000 0.000000 0.000000 0.000000 feature_232 0.000000 0.000000 0.000000 0.000000 0.000000 feature_233 0.000000 0.000000 0.000000 0.000000 0.000000 feature_234 0.000000 0.000000 0.000000 0.000000 0.000000 feature_235 0.000000 0.000000 0.000000 0.000000 0.000000 feature_236 0.000000 0.000000 0.000000 0.000000 0.000000 feature_237 0.000000 0.000000 0.000000 0.000000 0.000000 feature_238 0.000000 0.000000 0.000000 0.000000 0.000000 feature_239 0.000000 0.000000 0.000000 0.000000 0.000000 feature_240 0.000000 0.000000 0.000000 0.000000 0.000000 feature_241 0.000000 0.000000 0.000000 0.000000 0.000000 feature_242 0.000000 0.000000 0.000000 0.000000 0.000000 feature_243 0.000000 0.000000 0.000000 0.000000 0.000000 feature_244 0.000000 0.000000 0.000000 0.000000 0.000000 feature_245 0.000000 0.000000 0.000000 0.000000 0.000000 feature_246 0.000000 0.000000 0.000000 0.000000 0.000000 feature_247 0.000000 0.000000 0.000000 0.000000 0.000000 feature_248 0.000000 0.000000 0.000000 0.000000 0.000000 feature_249 0.000000 0.000000 0.000000 0.000000 0.000000 feature_250 0.000000 0.000000 0.000000 0.000000 0.000000 feature_251 0.000000 0.000000 0.000000 0.000000 0.000000 feature_252 0.000000 0.000000 0.000000 0.000000 0.000000 feature_253 0.000000 0.000000 0.000000 0.000000 0.000000 feature_254 0.000000 0.000000 0.000000 0.000000 0.000000 feature_255 0.000000 0.000000 0.000000 0.000000 0.000000 feature_256 0.000000 0.000000 0.000000 0.000000 0.000000 feature_257 0.000000 0.000000 0.000000 0.000000 0.000000 feature_258 0.000000 0.000000 0.000000 0.000000 0.000000 feature_259 0.000000 0.000000 0.000000 0.000000 0.000000 feature_260 0.000000 0.000000 0.000000 0.000000 0.000000 feature_261 0.000000 0.000000 0.000000 0.000000 0.000000 feature_262 0.000000 0.000000 0.000000 0.000000 0.000000 feature_263 1.000000 1.000000 1.000000 0.000000 0.000000 feature_264 0.000000 0.000000 0.000000 1.000000 1.000000 feature_265 0.000000 0.000000 0.000000 0.000000 0.000000 feature_266 0.000000 0.000000 0.000000 0.000000 0.000000 feature_267 0.000000 0.000000 0.000000 0.000000 0.000000 feature_268 1.000000 1.000000 0.000000 1.000000 1.000000 feature_269 0.000000 0.000000 1.000000 0.000000 0.000000 feature_270 0.000000 0.000000 0.000000 0.000000 0.000000 feature_271 0.000000 0.000000 0.000000 0.000000 0.000000 feature_272 0.000000 0.000000 0.000000 0.000000 0.000000 feature_273 0.000000 0.000000 0.000000 0.000000 0.000000 feature_274 0.000000 0.000000 0.000000 0.000000 0.000000 feature_275 0.000000 0.000000 0.000000 0.000000 0.000000 ground_truth 0.000000 0.000000 0.125000 0.000000 0.000000 <pre><code>monotonicity_indicator = {\n    f\"feature_{i}\": 1 if i in range(50, 54) or i in range(55, 59) else 0\n    for i in range(276)\n}\n\nbatch_size = 256\nmax_epochs = 100\n\ntuner = find_hyperparameters(\n    \"blog\",\n    monotonicity_indicator=monotonicity_indicator,\n    max_trials=50,\n    final_activation=None,\n    loss=\"mse\",\n    metrics=tf.keras.metrics.RootMeanSquaredError(),\n    objective=\"val_root_mean_squared_error\",\n    direction=\"min\",\n    batch_size=batch_size,\n    max_epochs=max_epochs,\n    executions_per_trial=1\n)\n</code></pre> <pre><code>Trial 37 Complete [00h 12m 02s]\nval_root_mean_squared_error: 0.16311830282211304\n\nBest val_root_mean_squared_error So Far: 0.15839169919490814\nTotal elapsed time: 07h 01m 53s\n\nSearch: Running Trial #38\n\nValue             |Best Value So Far |Hyperparameter\n8                 |12                |units\n2                 |2                 |n_layers\nelu               |elu               |activation\n0.0049663         |0.010091          |learning_rate\n0.11304           |0.11447           |weight_decay\n0.5               |0.49596           |dropout\n1                 |1                 |decay_rate\n\nEpoch 1/100\n185/185 [==============================] - 84s 99ms/step - loss: 0.0499 - root_mean_squared_error: 0.2167 - val_loss: 0.0307 - val_root_mean_squared_error: 0.1753\nEpoch 2/100\n185/185 [==============================] - 21s 90ms/step - loss: 0.0346 - root_mean_squared_error: 0.1860 - val_loss: 0.0300 - val_root_mean_squared_error: 0.1732\nEpoch 3/100\n185/185 [==============================] - 20s 90ms/step - loss: 0.0334 - root_mean_squared_error: 0.1828 - val_loss: 0.0293 - val_root_mean_squared_error: 0.1713\nEpoch 4/100\n185/185 [==============================] - 22s 98ms/step - loss: 0.0327 - root_mean_squared_error: 0.1810 - val_loss: 0.0288 - val_root_mean_squared_error: 0.1697\nEpoch 5/100\n185/185 [==============================] - 20s 88ms/step - loss: 0.0320 - root_mean_squared_error: 0.1789 - val_loss: 0.0283 - val_root_mean_squared_error: 0.1683\nEpoch 6/100\n185/185 [==============================] - 18s 82ms/step - loss: 0.0321 - root_mean_squared_error: 0.1792 - val_loss: 0.0284 - val_root_mean_squared_error: 0.1686\nEpoch 7/100\n185/185 [==============================] - 20s 90ms/step - loss: 0.0315 - root_mean_squared_error: 0.1775 - val_loss: 0.0279 - val_root_mean_squared_error: 0.1670\nEpoch 8/100\n185/185 [==============================] - 21s 90ms/step - loss: 0.0316 - root_mean_squared_error: 0.1778 - val_loss: 0.0287 - val_root_mean_squared_error: 0.1695\nEpoch 9/100\n185/185 [==============================] - 18s 82ms/step - loss: 0.0316 - root_mean_squared_error: 0.1778 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1673\nEpoch 10/100\n185/185 [==============================] - 20s 89ms/step - loss: 0.0312 - root_mean_squared_error: 0.1765 - val_loss: 0.0274 - val_root_mean_squared_error: 0.1656\nEpoch 11/100\n185/185 [==============================] - 19s 85ms/step - loss: 0.0314 - root_mean_squared_error: 0.1772 - val_loss: 0.0286 - val_root_mean_squared_error: 0.1692\nEpoch 12/100\n185/185 [==============================] - 20s 88ms/step - loss: 0.0313 - root_mean_squared_error: 0.1769 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1674\nEpoch 13/100\n185/185 [==============================] - 19s 82ms/step - loss: 0.0312 - root_mean_squared_error: 0.1765 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1658\nEpoch 14/100\n185/185 [==============================] - 21s 91ms/step - loss: 0.0312 - root_mean_squared_error: 0.1766 - val_loss: 0.0280 - val_root_mean_squared_error: 0.1672\nEpoch 15/100\n185/185 [==============================] - 19s 85ms/step - loss: 0.0313 - root_mean_squared_error: 0.1769 - val_loss: 0.0279 - val_root_mean_squared_error: 0.1670\nEpoch 16/100\n185/185 [==============================] - 21s 91ms/step - loss: 0.0314 - root_mean_squared_error: 0.1771 - val_loss: 0.0281 - val_root_mean_squared_error: 0.1677\nEpoch 17/100\n185/185 [==============================] - 20s 92ms/step - loss: 0.0313 - root_mean_squared_error: 0.1769 - val_loss: 0.0274 - val_root_mean_squared_error: 0.1655\nEpoch 18/100\n185/185 [==============================] - 18s 81ms/step - loss: 0.0310 - root_mean_squared_error: 0.1761 - val_loss: 0.0288 - val_root_mean_squared_error: 0.1698\nEpoch 19/100\n185/185 [==============================] - 20s 92ms/step - loss: 0.0311 - root_mean_squared_error: 0.1763 - val_loss: 0.0273 - val_root_mean_squared_error: 0.1653\nEpoch 20/100\n185/185 [==============================] - 19s 86ms/step - loss: 0.0311 - root_mean_squared_error: 0.1763 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1659\nEpoch 21/100\n185/185 [==============================] - 20s 92ms/step - loss: 0.0312 - root_mean_squared_error: 0.1767 - val_loss: 0.0273 - val_root_mean_squared_error: 0.1651\nEpoch 22/100\n185/185 [==============================] - 20s 90ms/step - loss: 0.0311 - root_mean_squared_error: 0.1764 - val_loss: 0.0271 - val_root_mean_squared_error: 0.1646\nEpoch 23/100\n185/185 [==============================] - 19s 83ms/step - loss: 0.0311 - root_mean_squared_error: 0.1763 - val_loss: 0.0278 - val_root_mean_squared_error: 0.1668\nEpoch 24/100\n185/185 [==============================] - 19s 83ms/step - loss: 0.0310 - root_mean_squared_error: 0.1761 - val_loss: 0.0275 - val_root_mean_squared_error: 0.1658\nEpoch 25/100\n</code></pre>"},{"location":"experiments/Compas/","title":"COMPAS","text":"<p>COMPAS [1] is a dataset containing the criminal records of 6,172 individuals arrested in Florida. The task is to predict whether the individual will commit a crime again in 2 years. The probability predicted by the system will be used as a risk score. As mentioned in [2] 13 attributes for prediction. The risk score should be monotonically increasing w.r.t. four attributes, number of prior adult convictions, number of juvenile felony, number of juvenile misdemeanor, and number of other convictions. The <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li> <p>S. Mattu J. Angwin, J. Larson and L. Kirchner. Machine bias: There\u2019s     software used across the country to predict future criminals. and     it\u2019s biased against blacks. ProPublica, 2016.</p> </li> <li> <p>Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic     neural networks. Advances in Neural Information Processing Systems,     33:15427\u201315438, 2020</p> </li> </ol> <p>These are a few examples of the dataset:</p> 0 1 2 3 4 priors_count 0.368421 0.000000 0.026316 0.394737 0.052632 juv_fel_count 0.000000 0.000000 0.000000 0.000000 0.000000 juv_misd_count 0.000000 0.000000 0.000000 0.000000 0.000000 juv_other_count 0.000000 0.000000 0.000000 0.000000 0.000000 age 0.230769 0.051282 0.179487 0.230769 0.102564 race_0 1.000000 1.000000 0.000000 1.000000 1.000000 race_1 0.000000 0.000000 1.000000 0.000000 0.000000 race_2 0.000000 0.000000 0.000000 0.000000 0.000000 race_3 0.000000 0.000000 0.000000 0.000000 0.000000 race_4 0.000000 0.000000 0.000000 0.000000 0.000000 race_5 0.000000 0.000000 0.000000 0.000000 0.000000 sex_0 1.000000 1.000000 1.000000 1.000000 1.000000 sex_1 0.000000 0.000000 0.000000 0.000000 0.000000 ground_truth 1.000000 0.000000 0.000000 0.000000 1.000000 <pre><code>batch_size = 8\nmax_epochs = 50\n\ntuner = find_hyperparameters(\n    \"compas\",\n    monotonicity_indicator = {\n        \"priors_count\": 1,\n        \"juv_fel_count\": 1,\n        \"juv_misd_count\": 1,\n        \"juv_other_count\": 1,\n        \"age\": 0,\n        \"race_0\": 0,\n        \"race_1\": 0,\n        \"race_2\": 0,\n        \"race_3\": 0,\n        \"race_4\": 0,\n        \"race_5\": 0,\n        \"sex_0\": 0,\n        \"sex_1\": 0,\n    },\n    max_trials=50,\n    final_activation=\"sigmoid\",\n    loss = \"binary_crossentropy\",\n    metrics = \"accuracy\",\n    objective=\"val_accuracy\",\n    direction=\"max\",\n    batch_size=batch_size,\n    max_epochs=max_epochs,    \n    executions_per_trial=1,\n)\n</code></pre> <pre><code>Trial 50 Complete [00h 01m 15s]\nval_accuracy: 0.6793522238731384\n\nBest val_accuracy So Far: 0.6995951533317566\nTotal elapsed time: 01h 43m 54s\nINFO:tensorflow:Oracle triggered exit\n</code></pre> 0 1 2 3 4 units 27 28 25 31 23 n_layers 2 3 2 3 2 activation elu elu elu elu elu learning_rate 0.084685 0.105227 0.069011 0.018339 0.089831 weight_decay 0.137518 0.120702 0.153525 0.105921 0.140927 dropout 0.175917 0.160270 0.180772 0.480390 0.106579 decay_rate 0.899399 0.872222 0.874505 0.964135 0.824555 val_accuracy_mean 0.693441 0.693117 0.692955 0.691498 0.691498 val_accuracy_std 0.003648 0.000992 0.002016 0.001280 0.001899 val_accuracy_min 0.689069 0.692308 0.691498 0.689879 0.689069 val_accuracy_max 0.698785 0.694737 0.696356 0.693117 0.693117 params 2317 3599 2157 4058 1672"},{"location":"experiments/Heart/","title":"Heart disease","text":"<p>Heart Disease [1] is a classification dataset used for predicting the presence of heart disease with 13 features (age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal) and monotonically increasing with respect to features trestbps and cholestrol (chol). The <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li>John H. Gennari, Pat Langley, and Douglas H. Fisher. Models of     incremental concept formation. Artif. Intell., 40(1-3):11\u201361, 1989.</li> </ol> <p>https://archive.ics.uci.edu/ml/datasets/heart+disease</p> <ol> <li>Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van     den Broeck. Counterexample-guided learning of monotonic neural     networks. Advances in Neural Information Processing Systems,     33:11936\u201311948, 2020</li> </ol> <p>These are a few examples of the dataset:</p> 0 1 2 3 4 age 0.972778 1.415074 1.415074 -1.902148 -1.459852 sex 0.649445 0.649445 0.649445 0.649445 -1.533413 cp -2.020077 0.884034 0.884034 -0.084003 -1.052040 trestbps 0.721008 1.543527 -0.649858 -0.101512 -0.101512 chol -0.251855 0.740555 -0.326754 0.066465 -0.794872 fbs 2.426901 -0.410346 -0.410346 -0.410346 -0.410346 restecg 1.070838 1.070838 1.070838 -0.953715 1.070838 thalach -0.025055 -1.831151 -0.928103 1.566030 0.920995 exang -0.721010 1.381212 1.381212 -0.721010 -0.721010 oldpeak 0.986440 0.330395 1.232457 1.970508 0.248389 slope 2.334348 0.687374 0.687374 2.334348 -0.959601 ca -0.770198 2.425024 1.359950 -0.770198 -0.770198 thal -2.070238 -0.514345 1.041548 -0.514345 -0.514345 ground_truth 0.000000 1.000000 0.000000 0.000000 0.000000 <pre><code>batch_size = 16\nmax_epochs = 50\n\ntuner = find_hyperparameters(\n    \"heart\",\n    monotonicity_indicator = {\n        \"age\": 0,\n        \"sex\": 0,\n        \"cp\": 0,\n        \"trestbps\": 1,\n        \"chol\": 1,\n        \"fbs\": 0,\n        \"restecg\": 0,\n        \"thalach\": 0,\n        \"exang\": 0,\n        \"oldpeak\": 0,\n        \"slope\": 0,\n        \"ca\": 0,\n        \"thal\": 0,\n    },\n    max_trials=200,\n    final_activation=\"sigmoid\",\n    loss = \"binary_crossentropy\",\n    metrics = \"accuracy\",\n    objective=\"val_accuracy\",\n    direction=\"max\",\n    batch_size=batch_size,\n    max_epochs=max_epochs,\n    executions_per_trial=3,\n)\n</code></pre> <pre><code>Trial 200 Complete [00h 00m 24s]\nval_accuracy: 0.8633879621823629\n\nBest val_accuracy So Far: 0.8852459192276001\nTotal elapsed time: 01h 44m 27s\nINFO:tensorflow:Oracle triggered exit\n</code></pre> 0 1 2 3 4 units 29 22 16 29 23 n_layers 2 2 2 2 3 activation elu elu elu elu elu learning_rate 0.001000 0.001099 0.002199 0.001000 0.001180 weight_decay 0.165278 0.127898 0.189895 0.189451 0.229994 dropout 0.357323 0.272848 0.334458 0.457679 0.365390 decay_rate 0.934428 0.925691 0.887252 0.902783 0.930666 val_accuracy_mean 0.885246 0.885246 0.885246 0.881967 0.881967 val_accuracy_std 0.000000 0.000000 0.000000 0.007331 0.007331 val_accuracy_min 0.885246 0.885246 0.885246 0.868852 0.868852 val_accuracy_max 0.885246 0.885246 0.885246 0.885246 0.885246 params 2880 1605 969 2880 2224"},{"location":"experiments/Loan/","title":"Loan","text":"<p>Lending club loan data contains complete loan data for all loans issued through 2007-2015 of several banks. Each data point is a 28-dimensional feature including the current loan status, latest payment information, and other additional features. The task is to predict loan defaulters given the feature vector. The possibility of loan default should be nondecreasing w.r.t. number of public record bankruptcies, Debt-to-Income ratio, and non-increasing w.r.t. credit score, length of employment, annual income. Thus the <code>monotonicity_indicator</code> corrsponding to these features are set to 1.</p> <p>References:</p> <ol> <li>https://www.kaggle.com/wendykan/lending-club-loan-data (Note:     Currently, the dataset seems to be withdrawn from kaggle)</li> </ol> <p>These are a few examples of the dataset:</p> 0 1 2 3 4 feature_0 0.833333 1.000000 0.666667 0.333333 0.666667 feature_1 0.000000 0.000000 0.000000 0.000000 0.000000 feature_2 0.400000 1.000000 0.800000 0.500000 0.700000 feature_3 0.005263 0.003474 0.005263 0.007158 0.006842 feature_4 0.005185 0.023804 0.029700 0.024434 0.021962 feature_5 0.185751 0.134860 0.236641 0.745547 0.440204 feature_6 0.240654 0.036215 0.271807 0.778037 0.260125 feature_7 0.000000 0.000000 0.000000 1.000000 0.000000 feature_8 0.000000 0.000000 0.000000 0.000000 0.000000 feature_9 0.000000 0.000000 1.000000 0.000000 1.000000 feature_10 0.000000 0.000000 0.000000 0.000000 0.000000 feature_11 0.000000 0.000000 0.000000 0.000000 0.000000 feature_12 0.000000 1.000000 0.000000 0.000000 0.000000 feature_13 1.000000 0.000000 0.000000 1.000000 0.000000 feature_14 0.000000 0.000000 0.000000 0.000000 0.000000 feature_15 1.000000 1.000000 1.000000 0.000000 1.000000 feature_16 0.000000 0.000000 0.000000 1.000000 0.000000 feature_17 0.000000 0.000000 0.000000 0.000000 0.000000 feature_18 0.000000 0.000000 0.000000 0.000000 0.000000 feature_19 0.000000 0.000000 0.000000 0.000000 0.000000 feature_20 0.000000 0.000000 0.000000 0.000000 0.000000 feature_21 0.000000 0.000000 0.000000 0.000000 0.000000 feature_22 0.000000 0.000000 0.000000 0.000000 0.000000 feature_23 0.000000 0.000000 0.000000 0.000000 0.000000 feature_24 0.000000 0.000000 0.000000 0.000000 0.000000 feature_25 0.000000 0.000000 0.000000 0.000000 0.000000 feature_26 0.000000 0.000000 0.000000 0.000000 0.000000 feature_27 0.000000 0.000000 0.000000 0.000000 0.000000 ground_truth 0.000000 0.000000 0.000000 0.000000 0.000000 <pre><code>monotonicity_indicator = {\n    f\"feature_{i}\": mi for i, mi in enumerate([-1, 1, -1, -1, 1] + [0] * 23)\n}\n\nbatch_size = 256\nmax_epochs = 20\n\ntuner = find_hyperparameters(\n    \"loan\",\n    monotonicity_indicator=monotonicity_indicator,\n    max_trials=50,\n    final_activation=\"sigmoid\",\n    loss=\"binary_crossentropy\",\n    metrics=\"accuracy\",\n    objective=\"val_accuracy\",\n    direction=\"max\",\n    batch_size=batch_size,\n    max_epochs=max_epochs,\n    executions_per_trial=1,\n)\n</code></pre> <pre><code>Trial 42 Complete [00h 10m 22s]\nval_accuracy: 0.6502591967582703\n\nBest val_accuracy So Far: 0.6515268087387085\nTotal elapsed time: 07h 02m 34s\n\nSearch: Running Trial #43\n\nValue             |Best Value So Far |Hyperparameter\n8                 |32                |units\n4                 |4                 |n_layers\nelu               |elu               |activation\n0.028901          |0.001             |learning_rate\n0.11556           |0.1               |weight_decay\n0                 |0.5               |dropout\n0.77927           |0.85232           |decay_rate\n\nEpoch 1/20\n1636/1636 [==============================] - 39s 17ms/step - loss: 0.6350 - accuracy: 0.6395 - val_loss: 0.6323 - val_accuracy: 0.6467\nEpoch 2/20\n1636/1636 [==============================] - 31s 17ms/step - loss: 0.6320 - accuracy: 0.6418 - val_loss: 0.6286 - val_accuracy: 0.6467\nEpoch 3/20\n1636/1636 [==============================] - 30s 16ms/step - loss: 0.6321 - accuracy: 0.6408 - val_loss: 0.6281 - val_accuracy: 0.6465\nEpoch 4/20\n1636/1636 [==============================] - 32s 17ms/step - loss: 0.6307 - accuracy: 0.6421 - val_loss: 0.6280 - val_accuracy: 0.6467\nEpoch 5/20\n1636/1636 [==============================] - 31s 16ms/step - loss: 0.6304 - accuracy: 0.6426 - val_loss: 0.6299 - val_accuracy: 0.6453\nEpoch 6/20\n1636/1636 [==============================] - 31s 16ms/step - loss: 0.6296 - accuracy: 0.6441 - val_loss: 0.6260 - val_accuracy: 0.6493\nEpoch 7/20\n1636/1636 [==============================] - 30s 16ms/step - loss: 0.6293 - accuracy: 0.6440 - val_loss: 0.6413 - val_accuracy: 0.6293\nEpoch 8/20\n1636/1636 [==============================] - 31s 16ms/step - loss: 0.6287 - accuracy: 0.6443 - val_loss: 0.6266 - val_accuracy: 0.6479\nEpoch 9/20\n1636/1636 [==============================] - 31s 16ms/step - loss: 0.6280 - accuracy: 0.6448 - val_loss: 0.6247 - val_accuracy: 0.6499\nEpoch 10/20\n1497/1636 [==========================&gt;...] - ETA: 2s - loss: 0.6279 - accuracy: 0.6456\n</code></pre>"}]}