[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Constrained Monotonic Neural Networks",
    "section": "",
    "text": "This Python library implements Monotonic Dense Layer as described in Davor Runje, Sharath M. Shankaranarayana, “Constrained Monotonic Neural Networks”, https://https://arxiv.org/abs/2205.11775.\nIf you use this library, please cite:"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Constrained Monotonic Neural Networks",
    "section": "Install",
    "text": "Install\npip install mono_dense_keras"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Constrained Monotonic Neural Networks",
    "section": "License",
    "text": "License\nThe full text of the license is available at:\nhttps://github.com/airtai/mono-dense-keras/blob/main/LICENSE\nYou are free to: - Share — copy and redistribute the material in any medium or format\n\nAdapt — remix, transform, and build upon the material\n\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms: - Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nNonCommercial — You may not use the material for commercial purposes.\nShareAlike — If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\nNo additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Constrained Monotonic Neural Networks",
    "section": "How to use",
    "text": "How to use\nFirst, we’ll create a simple dataset for testing using numpy. Inputs values \\(x_1\\), \\(x_2\\) and \\(x_3\\) will be sampled from the normal distribution, while the output value \\(y\\) will be calculated according to the following formula before adding noise to it:\n\\(y = x_1^3 + \\sin\\left(\\frac{x_2}{2 \\pi}\\right) + e^{-x_3}\\)\n\nimport numpy as np\n\nrng = np.random.default_rng(42)\n\ndef generate_data(no_samples: int, noise: float):\n    x = rng.normal(size=(no_samples, 3))\n    y = x[:, 0] ** 3\n    y += np.sin(x[:, 1] / (2*np.pi))\n    y += np.exp(-x[:, 2])\n    y += noise * rng.normal(size=no_samples)\n    return x, y\n\nx_train, y_train = generate_data(10_000, noise=0.1)\nx_val, y_val = generate_data(10_000, noise=0.)\n\nFirst, we’ll build a simple feedforward neural network using Dense layer from Keras library.\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n# build a simple model with 3 hidden layer\nmodel = Sequential()\n\nmodel.add(Input(shape=(3,)))\nmodel.add(Dense(128, activation=\"elu\"))\nmodel.add(Dense(128, activation=\"elu\"))\nmodel.add(Dense(1))\n\nWe’ll train the network using the Adam optimizer and the ExponentialDecay learning rate schedule:\n\ndef train_model(model, initial_learning_rate):\n    # train the model\n    lr_schedule = ExponentialDecay(\n        initial_learning_rate=initial_learning_rate,\n        decay_steps=10_000,\n        decay_rate=0.9,\n    )\n    optimizer = Adam(learning_rate=lr_schedule)\n    model.compile(optimizer=\"adam\", loss=\"mse\")\n\n    model.fit(x=x_train, y=y_train, batch_size=32, validation_data=(x_val, y_val), epochs=10)\n    \ntrain_model(model, initial_learning_rate=.1)\n\nEpoch 1/10\n313/313 [==============================] - 1s 2ms/step - loss: 9.1098 - val_loss: 9.2552\nEpoch 2/10\n313/313 [==============================] - 1s 2ms/step - loss: 7.7995 - val_loss: 8.3143\nEpoch 3/10\n313/313 [==============================] - 1s 2ms/step - loss: 7.5270 - val_loss: 8.0499\nEpoch 4/10\n313/313 [==============================] - 1s 2ms/step - loss: 7.2095 - val_loss: 7.5935\nEpoch 5/10\n313/313 [==============================] - 1s 2ms/step - loss: 6.0665 - val_loss: 6.7911\nEpoch 6/10\n313/313 [==============================] - 1s 2ms/step - loss: 3.1178 - val_loss: 1.5964\nEpoch 7/10\n313/313 [==============================] - 1s 2ms/step - loss: 1.1686 - val_loss: 0.8541\nEpoch 8/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.7370 - val_loss: 1.5969\nEpoch 9/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.6011 - val_loss: 0.3739\nEpoch 10/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.4458 - val_loss: 0.3114\n\n\nNow, we’ll use the MonotonicDense layer instead of Dense layer. By default, the MonotonicDense layer assumes the output of the layer is monotonically increasing with all inputs. This assumtion is always true for all layers except possibly the first one. For the first layer, we use indicator_vector to specify which input parameters are monotonic and to specify are they increasingly or decreasingly monotonic: - set 1 for increasingly monotonic parameter, - set -1 for decreasingly monotonic parameter, and - set 0 otherwise.\nIn our case, the indicator_vector is [1, 0, -1] because \\(y\\) is: - monotonically increasing w.r.t. \\(x_1\\) \\(\\left(\\frac{\\partial y}{x_1} = 3 {x_1}^2 \\geq 0\\right)\\), and - monotonically decreasing w.r.t. \\(x_3\\) \\(\\left(\\frac{\\partial y}{x_3} = - e^{-x_2} \\leq 0\\right)\\).\n\nfrom airt.keras.layers import MonotonicDense\n\n\n# build a simple model with 3 hidden layer, but this using MonotonicDense layer\nmono_model = Sequential()\n\nmono_model.add(Input(shape=(3,)))\nindicator_vector = [1, 0, -1]\nmono_model.add(MonotonicDense(128, activation=\"elu\", indicator_vector=indicator_vector))\nmono_model.add(MonotonicDense(128, activation=\"elu\"))\n\nmono_model.add(Dense(1))\n\n\ntrain_model(model, initial_learning_rate=.001)\n\nEpoch 1/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.3646 - val_loss: 0.2042\nEpoch 2/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.2895 - val_loss: 0.1387\nEpoch 3/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.2756 - val_loss: 0.1027\nEpoch 4/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.2281 - val_loss: 0.0814\nEpoch 5/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.1816 - val_loss: 0.0634\nEpoch 6/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.1631 - val_loss: 0.1443\nEpoch 7/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.1455 - val_loss: 0.1299\nEpoch 8/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.1701 - val_loss: 0.0709\nEpoch 9/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.1250 - val_loss: 0.0644\nEpoch 10/10\n313/313 [==============================] - 1s 2ms/step - loss: 0.1426 - val_loss: 0.0405"
  },
  {
    "objectID": "keras.layers.html",
    "href": "keras.layers.html",
    "title": "Monotonic Dense Layer",
    "section": "",
    "text": "source\n\nMonotonicDense\n\n MonotonicDense (*args, **kwargs)\n\nMonotonic counterpart of the regular Dense Layer of tf.Keras\nArgs: units: Positive integer, dimensionality of the output space. activation: Activation function to use. indicator_vector: Vector to indicate which of the inputs are monotonically increasing or monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing and -1 for monotonically decreasing and 0 for non-monotonic variables. convexity_indicator: If the value is 0 or 1, then all elements of the activation selector will be 0 or 1, respectevely. If None, epsilon will be used to determine the number of 0 and 1 in the activation selector. epsilon: Percentage of elements with value 1 in the activation vector if convexity_indicator is None, ignored otherwise.\n\nrng = np.random.default_rng(42)\n\nx = rng.uniform(size=(3, 4))\n\nlayer = MonotonicDense(units=5)\ny = layer(x)\ny\n\n<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\narray([[0.8502902 , 2.1193693 , 1.1444832 , 0.9975339 , 0.9893499 ],\n       [0.7763262 , 2.0163271 , 1.1297605 , 0.94500107, 1.0620797 ],\n       [0.5978056 , 1.4017718 , 0.98523664, 0.79769945, 0.766499  ]],\n      dtype=float32)>"
  }
]