{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mono-dense-keras\n",
    "\n",
    "> Monotonic Dense Layer implementation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python library implements Monotonic Dense Layer as described in Davor Runje, Sharath M. Shankaranarayana, \"Constrained Monotonic Neural Networks\", https://https://arxiv.org/abs/2205.11775.\n",
    "\n",
    "If you use this library, please cite:\n",
    "\n",
    "        @misc{https://doi.org/10.48550/arxiv.2205.11775,\n",
    "          doi = {10.48550/ARXIV.2205.11775},\n",
    "          url = {https://arxiv.org/abs/2205.11775},\n",
    "          author = {Runje, Davor and Shankaranarayana, Sharath M.},\n",
    "          title = {Constrained Monotonic Neural Networks},\n",
    "          publisher = {arXiv},\n",
    "          year = {2022},\n",
    "          copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install mono_dense_keras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full text of the license is available at:\n",
    "\n",
    "https://github.com/airtai/mono-dense-keras/blob/main/LICENSE\n",
    "\n",
    "You are free to:\n",
    "- Share — copy and redistribute the material in any medium or format\n",
    "- Adapt — remix, transform, and build upon the material\n",
    "\n",
    "The licensor cannot revoke these freedoms as long as you follow the license terms.\n",
    "\n",
    "Under the following terms:\n",
    "- Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n",
    "\n",
    "- NonCommercial — You may not use the material for commercial purposes.\n",
    "\n",
    "- ShareAlike — If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n",
    "\n",
    "- No additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll create a simple dataset for testing using numpy. Inputs values $x_1$, $x_2$ and $x_3$  will be sampled from the normal distribution, while the output value $y$ will be calculated according to the following formula before adding noise to it:\n",
    "\n",
    "$y = x_1^3 + \\sin\\left(\\frac{x_2}{2 \\pi}\\right) + e^{-x_3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def generate_data(no_samples: int, noise: float):\n",
    "    x = rng.normal(size=(no_samples, 3))\n",
    "    y = x[:, 0] ** 3\n",
    "    y += np.sin(x[:, 1] / (2*np.pi))\n",
    "    y += np.exp(-x[:, 2])\n",
    "    y += 0.1 * rng.normal(size=no_samples)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = generate_data(10_000, noise=0.1)\n",
    "x_val, y_val = generate_data(10_000, noise=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll build a simple feedforward neural network using `Dense` layer from Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# build a simple model with 3 hidden layer\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(3,)))\n",
    "model.add(Dense(128, activation=\"elu\"))\n",
    "model.add(Dense(128, activation=\"elu\"))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the network using the `Adam` optimizer and the `ExponentialDecay` learning rate schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 9.1129 - val_loss: 9.2426\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 7.8147 - val_loss: 8.4241\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 7.5386 - val_loss: 8.0456\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 7.0556 - val_loss: 7.3737\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 5.8239 - val_loss: 4.4619\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3169 - val_loss: 1.5477\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.0104 - val_loss: 0.7407\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6479 - val_loss: 0.9755\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5144 - val_loss: 0.3477\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3837 - val_loss: 0.2306\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, initial_learning_rate):\n",
    "    # train the model\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        decay_steps=10_000,\n",
    "        decay_rate=0.9,\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    model.fit(x=x_train, y=y_train, batch_size=32, validation_data=(x_val, y_val), epochs=10)\n",
    "    \n",
    "train_model(model, initial_learning_rate=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use the `MonotonicDense` layer instead of `Dense` layer. By default, the `MonotonicDense` layer assumes the output of the layer is monotonically increasing with all inputs. This assumtion is always true for all layers except possibly the first one. For the first layer, we use `indicator_vector` to specify which input parameters are monotonic and to specify are they increasingly or decreasingly monotonic:\n",
    "- set 1 for increasingly monotonic parameter,\n",
    "- set -1 for decreasingly monotonic parameter, and\n",
    "- set 0 otherwise.\n",
    "\n",
    "In our case, the `indicator_vector` is `[1, 0, -1]` because $y$ is:\n",
    "- monotonically increasing w.r.t. $x_1$ $\\left(\\frac{\\partial y}{x_1} = 3 {x_1}^2 \\geq 0\\right)$, and\n",
    "- monotonically decreasing w.r.t. $x_3$ $\\left(\\frac{\\partial y}{x_3} = - e^{-x_2} \\leq 0\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airt.keras.layers import MonotonicDense\n",
    "\n",
    "\n",
    "# build a simple model with 3 hidden layer, but this using MonotonicDense layer\n",
    "mono_model = Sequential()\n",
    "\n",
    "mono_model.add(Input(shape=(3,)))\n",
    "indicator_vector = [1, 0, -1]\n",
    "mono_model.add(MonotonicDense(128, activation=\"elu\", indicator_vector=indicator_vector))\n",
    "mono_model.add(MonotonicDense(128, activation=\"elu\"))\n",
    "\n",
    "mono_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3578 - val_loss: 0.1557\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2650 - val_loss: 0.2754\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2238 - val_loss: 0.0917\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1867 - val_loss: 0.0798\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1724 - val_loss: 0.7418\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1356 - val_loss: 0.4525\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1533 - val_loss: 0.0503\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1274 - val_loss: 0.0395\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1171 - val_loss: 0.0381\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1005 - val_loss: 0.0500\n"
     ]
    }
   ],
   "source": [
    "train_model(model, initial_learning_rate=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
