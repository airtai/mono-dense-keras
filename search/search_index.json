{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Constrained Monotonic Neural Networks","text":"<p>This Python library implements Monotonic Dense Layer as described in Davor Runje, Sharath M. Shankaranarayana, \u201cConstrained Monotonic Neural Networks\u201d, https://https://arxiv.org/abs/2205.11775.</p> <p>If you use this library, please cite:</p> <pre><code>@misc{https://doi.org/10.48550/arxiv.2205.11775,\n  doi = {10.48550/ARXIV.2205.11775},\n  url = {https://arxiv.org/abs/2205.11775},\n  author = {Runje, Davor and Shankaranarayana, Sharath M.},\n  title = {Constrained Monotonic Neural Networks},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}\n</code></pre>"},{"location":"#install","title":"Install","text":"<pre><code>pip install mono-dense-keras\n</code></pre>"},{"location":"#how-to-use","title":"How to use","text":"<p>First, we\u2019ll create a simple dataset for testing using numpy. Inputs values \\(x_1\\), \\(x_2\\) and \\(x_3\\) will be sampled from the normal distribution, while the output value \\(y\\) will be calculated according to the following formula before adding noise to it:</p> <p>\\(y = x_1^3 + \\sin\\left(\\frac{x_2}{2 \\pi}\\right) + e^{-x_3}\\)</p> <pre><code>import numpy as np\n\nrng = np.random.default_rng(42)\n\ndef generate_data(no_samples: int, noise: float):\n    x = rng.normal(size=(no_samples, 3))\n    y = x[:, 0] ** 3\n    y += np.sin(x[:, 1] / (2*np.pi))\n    y += np.exp(-x[:, 2])\n    y += noise * rng.normal(size=no_samples)\n    return x, y\n\nx_train, y_train = generate_data(10_000, noise=0.1)\nx_val, y_val = generate_data(10_000, noise=0.)\n</code></pre> <p>First, we\u2019ll build a simple feedforward neural network using <code>Dense</code> layer from Keras library.</p> <pre><code>import tensorflow as tf\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n# build a simple model with 3 hidden layer\nmodel = Sequential()\n\nmodel.add(Input(shape=(3,)))\nmodel.add(Dense(128, activation=\"elu\"))\nmodel.add(Dense(128, activation=\"elu\"))\nmodel.add(Dense(1))\n</code></pre> <p>We\u2019ll train the network using the <code>Adam</code> optimizer and the <code>ExponentialDecay</code> learning rate schedule:</p> <pre><code>def train_model(model, initial_learning_rate):\n    # train the model\n    lr_schedule = ExponentialDecay(\n        initial_learning_rate=initial_learning_rate,\n        decay_steps=10_000,\n        decay_rate=0.9,\n    )\n    optimizer = Adam(learning_rate=lr_schedule)\n    model.compile(optimizer=\"adam\", loss=\"mse\")\n\n    model.fit(x=x_train, y=y_train, batch_size=32, validation_data=(x_val, y_val), epochs=10)\n\ntrain_model(model, initial_learning_rate=.1)\n</code></pre> <pre><code>Epoch 1/10\n313/313 [==============================] - 2s 3ms/step - loss: 9.2746 - val_loss: 9.4512\nEpoch 2/10\n313/313 [==============================] - 1s 3ms/step - loss: 7.9443 - val_loss: 8.4021\nEpoch 3/10\n313/313 [==============================] - 1s 3ms/step - loss: 7.5441 - val_loss: 8.0130\nEpoch 4/10\n313/313 [==============================] - 1s 3ms/step - loss: 7.0930 - val_loss: 7.1562\nEpoch 5/10\n313/313 [==============================] - 1s 3ms/step - loss: 5.1077 - val_loss: 3.1863\nEpoch 6/10\n313/313 [==============================] - 1s 3ms/step - loss: 1.6965 - val_loss: 1.0904\nEpoch 7/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.9010 - val_loss: 0.6402\nEpoch 8/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.6239 - val_loss: 0.3862\nEpoch 9/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.4810 - val_loss: 0.3410\nEpoch 10/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.3945 - val_loss: 0.2314\n</code></pre> <p>Now, we\u2019ll use the <code>MonotonicDense</code> layer instead of <code>Dense</code> layer. By default, the <code>MonotonicDense</code> layer assumes the output of the layer is monotonically increasing with all inputs. This assumtion is always true for all layers except possibly the first one. For the first layer, we use <code>indicator_vector</code> to specify which input parameters are monotonic and to specify are they increasingly or decreasingly monotonic: - set 1 for increasingly monotonic parameter,</p> <ul> <li> <p>set -1 for decreasingly monotonic parameter, and</p> </li> <li> <p>set 0 otherwise.</p> </li> </ul> <p>In our case, the <code>indicator_vector</code> is <code>[1, 0, -1]</code> because \\(y\\) is: - monotonically increasing w.r.t. \\(x_1\\) \\(\\left(\\frac{\\partial y}{x_1} = 3 {x_1}^2 \\geq 0\\right)\\), and</p> <ul> <li>monotonically decreasing w.r.t. \\(x_3\\) \\(\\left(\\frac{\\partial y}{x_3} = - e^{-x_2} \\leq 0\\right)\\).</li> </ul> <pre><code>from airt.keras.layers import MonotonicDense\n\n\n# build a simple model with 3 hidden layer, but this using MonotonicDense layer\nmono_model = Sequential()\n\nmono_model.add(Input(shape=(3,)))\nindicator_vector = [1, 0, -1]\nmono_model.add(MonotonicDense(128, activation=\"elu\", indicator_vector=indicator_vector))\nmono_model.add(MonotonicDense(128, activation=\"elu\"))\n\nmono_model.add(Dense(1))\n</code></pre> <pre><code>train_model(model, initial_learning_rate=.001)\n</code></pre> <pre><code>Epoch 1/10\n313/313 [==============================] - 2s 3ms/step - loss: 0.3102 - val_loss: 0.1625\nEpoch 2/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.3265 - val_loss: 0.1656\nEpoch 3/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.2603 - val_loss: 0.0976\nEpoch 4/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.2140 - val_loss: 0.1340\nEpoch 5/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.1731 - val_loss: 0.4058\nEpoch 6/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.1726 - val_loss: 0.0517\nEpoch 7/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.1550 - val_loss: 0.0655\nEpoch 8/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.1324 - val_loss: 0.1012\nEpoch 9/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.1219 - val_loss: 0.0337\nEpoch 10/10\n313/313 [==============================] - 1s 3ms/step - loss: 0.1148 - val_loss: 0.1091\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The full text of the license is available at:</p> <p>https://github.com/airtai/mono-dense-keras/blob/main/LICENSE</p> <p>You are free to: - Share \u2014 copy and redistribute the material in any medium or format</p> <ul> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p> <p>Under the following terms: - Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> <ul> <li> <p>NonCommercial \u2014 You may not use the material for commercial purposes.</p> </li> <li> <p>ShareAlike \u2014 If you remix, transform, or build upon the material, you   must distribute your contributions under the same license as the   original.</p> </li> <li> <p>No additional restrictions \u2014 You may not apply legal terms or   technological measures that legally restrict others from doing   anything the license permits.</p> </li> </ul>"},{"location":"MonoDenseLayer/","title":"Monotonic dense layer","text":"<pre><code>import pytest\n\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\n\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input\n</code></pre> <pre><code>def print_header(header: str, length: int=120):\n    n = (length-2-len(header)) // 2\n    print(\"*\"*length)\n    print(\"*\" + \" \"*(length-2) + \"*\")\n    print(\"*\" + \" \"*n + header + \" \"*(length-n-2-len(header)) + \"*\")\n    print(\"*\" + \" \"*(length-2) + \"*\")\n    print(\"*\"*length)\n    print()\nprint_header(\"hello\")\n</code></pre> <pre><code>************************************************************************************************************************\n*                                                                                                                      *\n*                                                        hello                                                         *\n*                                                                                                                      *\n************************************************************************************************************************\n</code></pre>"},{"location":"MonoDenseLayer/#monotonic-dense-unit-class","title":"Monotonic Dense Unit Class","text":""},{"location":"MonoDenseLayer/#activation-functions","title":"Activation functions","text":"<p>source</p>"},{"location":"MonoDenseLayer/#get_saturated_activation","title":"get_saturated_activation","text":"<pre><code> get_saturated_activation (convex_activation:Callable[[tensorflow.python.f\n                           ramework.ops.Tensor],tensorflow.python.framewor\n                           k.ops.Tensor], concave_activation:Callable[[ten\n                           sorflow.python.framework.ops.Tensor],tensorflow\n                           .python.framework.ops.Tensor], a:float=1.0,\n                           c:float=1.0)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#get_activation_functions","title":"get_activation_functions","text":"<pre><code> get_activation_functions (activation:Union[str,Callable[[tensorflow.pytho\n                           n.framework.ops.Tensor],tensorflow.python.frame\n                           work.ops.Tensor],NoneType]=None)\n</code></pre> <pre><code>for activation in [\"ReLU\", \"ELU\", \"SELU\"]:\n    (\n        convex_activation,\n        concave_activation,\n        saturated_activation,\n    ) = get_activation_functions(activation)\n\n    x = np.arange(-3.5, 3.5, 0.1)\n    plt.plot(\n        x, convex_activation(x), label=r\"$\\breve{\\rho}(x)$\", linestyle=\"--\", alpha=0.7\n    )\n    plt.plot(\n        x, concave_activation(x), label=r\"$\\hat{\\rho}(x)$\", linestyle=\"--\", alpha=0.7\n    )\n    plt.plot(\n        x,\n        saturated_activation(x),\n        label=r\"$\\tilde{\\rho}(x)$\",\n        linestyle=\"--\",\n        alpha=0.7,\n    )\n    plt.legend()\n    plt.title(\n        f\"{activation.__name__ if hasattr(activation, '__name__') else activation} based activations\"\n    )\n    plt.show()\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_activations","title":"apply_activations","text":"<pre><code> apply_activations (x:tensorflow.python.framework.ops.Tensor, units, activ\n                    ation:Union[str,Callable[[tensorflow.python.framework.\n                    ops.Tensor],tensorflow.python.framework.ops.Tensor],No\n                    neType]=None, is_convex:bool=False,\n                    is_concave:bool=False,\n                    activation_weights:Tuple[float,float,float]=(1.0, 1.0,\n                    1.0))\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\nx = np.random.normal(0, 5, size=(5, 7, 12))\n\ny = apply_activations(\n    x,\n    activation=\"relu\",\n    units=12,\n)\n\nassert y.shape == (5, 7, 12)\n\nnp.testing.assert_array_almost_equal(y[:, :, :4].numpy(), np.maximum(x[:, :, :4], 0))\nnp.testing.assert_array_almost_equal(-y[:, :, 4:8].numpy(), -np.maximum(x[:, :, 4:8], 0))\nassert (y[:, :, 8:].numpy() != 0).all()\nassert (y[:, :, 8:].numpy() &gt;= -1).all()\nassert (y[:, :, 8:].numpy() &lt;= 1).all()\nassert (y[:, :, 8:].numpy() == -1).any()\nassert (y[:, :, 8:].numpy() == 1).any()\n</code></pre>"},{"location":"MonoDenseLayer/#monotonicity-indicator","title":"Monotonicity indicator","text":"<p>source</p>"},{"location":"MonoDenseLayer/#check_monotonicity_indicator_values","title":"check_monotonicity_indicator_values","text":"<pre><code> check_monotonicity_indicator_values (monotonicity_indicator:Union[int,num\n                                      py.ndarray[Any,numpy.dtype[numpy.int\n                                      64]]])\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\n\nwith pytest.raises(ValueError) as e:\n    check_monotonicity_indicator_values(2)\nprint(e.value)\n\nmonotonicity_indicator = np.random.randint(3, size=(4, 5))-1\ncheck_monotonicity_indicator_values(monotonicity_indicator)\n\nmonotonicity_indicator = monotonicity_indicator-1\nwith pytest.raises(ValueError) as e:\n    check_monotonicity_indicator_values(monotonicity_indicator)\nprint(e.value)\n</code></pre> <p>source</p>"},{"location":"MonoDenseLayer/#apply_monotonicity_indicator_to_kernel","title":"apply_monotonicity_indicator_to_kernel","text":"<pre><code> apply_monotonicity_indicator_to_kernel\n                                         (kernel:tensorflow.python.ops.var\n                                         iables.Variable, monotonicity_ind\n                                         icator:Union[int,numpy.ndarray[An\n                                         y,numpy.dtype[numpy.int64]]])\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\nlayer = Dense(7)\nlayer.build(\n    input_shape=(\n        None,\n        9,\n    )\n)\n\npos_kernel = apply_monotonicity_indicator_to_kernel(layer.kernel, 1)\nassert pos_kernel.shape == layer.kernel.shape\nassert (pos_kernel &gt;= 0).numpy().all()\n\nneg_kernel = apply_monotonicity_indicator_to_kernel(layer.kernel, -1)\nassert neg_kernel.shape == layer.kernel.shape\nassert (neg_kernel &lt;= 0).numpy().all()\n\norg_kernel = apply_monotonicity_indicator_to_kernel(layer.kernel, 0)\nassert (org_kernel == layer.kernel).numpy().all()\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\nlayer = Dense(7)\nlayer.build(\n    input_shape=(\n        None,\n        3,\n        5,\n        9,\n    )\n)\n\nmonotonicity_indicator = np.random.randint(3, size=(9, 7))-1\ncheck_monotonicity_indicator_values(monotonicity_indicator)\n\nmono_kernel = apply_monotonicity_indicator_to_kernel(\n    layer.kernel, monotonicity_indicator\n)\nassert mono_kernel.shape == layer.kernel.shape\n\nassert ((mono_kernel &gt;= 0).numpy() | (monotonicity_indicator != 1)).all()\nassert ((mono_kernel &lt;= 0).numpy() | (monotonicity_indicator != -1)).all()\n</code></pre>"},{"location":"MonoDenseLayer/#monotonic-dense-layer_1","title":"Monotonic Dense Layer","text":"<p>source</p>"},{"location":"MonoDenseLayer/#monotonicdense","title":"MonotonicDense","text":"<pre><code> MonotonicDense (units:int, activation:Union[str,Callable[[tensorflow.pyth\n                 on.framework.ops.Tensor],tensorflow.python.framework.ops.\n                 Tensor],NoneType]=None, monotonicity_indicator:Union[int,\n                 numpy.ndarray[Any,numpy.dtype[numpy.int64]]]=1,\n                 is_convex:bool=False, is_concave:bool=False,\n                 activation_weights:Tuple[float,float,float]=(1.0, 1.0,\n                 1.0), **kwargs)\n</code></pre> <p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> <pre><code>tf.keras.utils.set_random_seed(42)\n\ninput_shape=(None, 3, 5)\nunits = 7\n\nlayer = MonotonicDense(units, monotonicity_indicator=1, activation=\"relu\")\nlayer.build(input_shape=input_shape)\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\n\nmonotonicity_indicator = np.random.randint(3, size=(11, 13))-1\n\nlayer = MonotonicDense(13, monotonicity_indicator=monotonicity_indicator, activation=\"relu\")\nlayer.build(input_shape=(None, 7, 9, 11))\n\nlayer = MonotonicDense(13, monotonicity_indicator=monotonicity_indicator, activation=\"relu\")\nwith pytest.raises(ValueError) as e:\n    layer.build(input_shape=(None, 7, 9, 17))\nprint(e.value)\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\n\nmonotonicity_indicator = np.random.randint(3, size=(11, 13))-1\n\nlayer = MonotonicDense(13, monotonicity_indicator=monotonicity_indicator, activation=\"relu\")\n\nlayer.build(input_shape=(None, 7, 9, 11))\n\nx = np.random.normal(size=(2, 7, 9, 11))\nwith layer.replace_kernel():\n    pass\n\ny = layer(x)\nassert y.shape[:-1] == x.shape[:-1]\n</code></pre> <pre><code>tf.keras.utils.set_random_seed(42)\n\nunits=17\nfor x in [np.ones((5, 7)), np.zeros((2, 3))]:\n    for activation in [None, \"elu\", \"relu\"]:\n        for is_convex in [True, False]:\n            for is_concave in [True, False]:\n                for monotonicity_indicator in [1, np.random.randint(3, size=(x.shape[-1], units))-1]:\n                    if is_concave and is_convex:\n                        continue\n                    if isinstance(monotonicity_indicator, int):\n                        print_header(f\"{activation=}, {is_convex=}, {is_concave=}, {monotonicity_indicator=}\")\n                    else:\n                        print_header(f\"{activation=}, {is_convex=}, {is_concave=}, {monotonicity_indicator.shape=}\")\n\n                    layer = MonotonicDense(\n                        units=units,\n                        monotonicity_indicator=monotonicity_indicator,\n                        is_convex=is_convex,\n                        is_concave=is_concave,\n                        activation=activation,\n                    )\n                    y = layer(x)\n                    print(f\"{x.shape=}\")\n                    print(f\"{y.shape=}\")\n                    print()\n</code></pre>"},{"location":"MonoDenseLayer/#universal-approximation","title":"Universal approximation","text":"<pre><code># this function is problematic for unsaturated activations only\n\nx = np.arange(-10, 10, 0.1)\ny = np.sin(x) + x\ndy = np.cos(x) + 1\nplt.plot(x, y)\nplt.plot(x, dy)\n# plt.legend()\nplt.show()\n</code></pre> <pre><code>def generate_heavyside_dataset(n_points=100_000):\n    tf.keras.utils.set_random_seed(42)\n\n    rng = np.random.default_rng(42)\n    x_train = rng.normal(size=n_points)\n    y_train = (x_train &gt;= 0).astype(\"float\")\n    df_train = pd.DataFrame(dict(x=x_train, y=y_train))\n\n    return df_train\n</code></pre> <p>df_train = generate_heavyside_dataset()</p> <p>train_ds = df_train.to_ds().map(lambda d: (d[\u201cx\u201d], d[\u201cy\u201d])) train_ds.batch(8).peek()</p> <p>def build_wide_model_f( df_train: pd.DataFrame, units: int = 12, is_mono: bool = True, activation=\u201celu\u201d, ): x_mean, x_stddev = df_train.describe().loc[[\u201cmean\u201d, \u201cstd\u201d], \u201cx\u201d] y_mean, y_stddev = df_train.describe().loc[[\u201cmean\u201d, \u201cstd\u201d], \u201cy\u201d]</p> <pre><code>print(f\"{x_mean=:.3f}, {x_stddev=:.3f}, {y_mean=:.3f}, {y_stddev=:.3f}\")\n\nx = Input(shape=(1,))\ny = (x - x_mean) / x_stddev\n\nif is_mono:\n    y = MonotonicDense(\n        units=units,\n        activation=activation,\n    )(y)\n    y = MonotonicDense(units=1, activation=None)(y)\nelse:\n    y = Dense(units=units, activation=activation)(y)\n    y = Dense(units=1, activation=None)(y)\n\n# unnormalize\ny = y * y_stddev + y_mean\n\nreturn Model(\n    inputs=x,\n    outputs=y,\n)\n</code></pre> <p>model = build_wide_model_f(df_train) model.summary()</p> <p>def get_mono_layers(learner): return [ layer for layer in learner.model.layers if \u201cMonotonicDense\u201d == layer.__class__.__name__ ]</p> <p>def show_weights(learner):</p> <pre><code>import seaborn as sns\n\ncm = sns.dark_palette(\"pink\", as_cmap=True)\n\ndef get_kernel_weights(layer):\n    return [w for w in layer.weights if \"kernel\" in w.name][0].numpy().reshape(-1)\n\nmono_layers = get_mono_layers(learner)\n\ndf = pd.DataFrame(\n    dict(\n        w0=get_kernel_weights(mono_layers[0]),\n        w1=get_kernel_weights(mono_layers[1]),\n    )\n)\n\ndisplay(df.abs().style.background_gradient(cmap=cm))\n</code></pre> <p>def train_model( activation, df_train, units=12, is_mono=True, batch_size=128, weight_decay=0.0001, max_lr=None, ): tf.keras.utils.set_random_seed(42)</p> <pre><code>learner = Learner(\n    build_model_f=lambda: build_wide_model_f(\n        df_train=df_train, units=units, is_mono=True, activation=activation\n    ),\n    loss=\"mean_squared_error\",\n    train_ds=train_ds.batch(batch_size),\n    val_ds=train_ds.batch(batch_size),\n    optimizer=lambda: AdamW(weight_decay=weight_decay),\n)\n\nif max_lr is None:\n    #         max_lr = learner.lr_find(steps=3_000, multiple_run_strategy=np.median)\n    max_lr = learner.lr_find(steps=3_000, multiple_run_strategy=\"max\")\n\nlearner.one_cycle(max_lr=max_lr, epochs=5)\n\nplot_f(\n    lambda x: learner.model.predict(x).reshape(-1),\n    start=-2,\n    stop=2,\n    title=\"Learned function\",\n).jtstyle().show()\n\nshow_weights(learner)\n\nplot_activation_functions(get_mono_layers(learner)[0]).show()\n\nreturn learner.model\n</code></pre> <p>train_model(\u201crelu\u201d, df_train)</p> <p>train_model(\u201celu\u201d, df_train)</p>"},{"location":"MonoDenseLayer/#train_modelselu-df_train","title":"train_model(\u201cselu\u201d, df_train)","text":""},{"location":"MonoDenseLayer/#train_modelselu-df_train-max_lr009","title":"train_model(\u201cselu\u201d, df_train, max_lr=0.09)","text":""},{"location":"MonoDenseLayer/#synthetic-tests","title":"Synthetic tests","text":""},{"location":"MonoDenseLayer/#export","title":"export","text":"<p>def _m_1(x, a=1, b=0): return np.sin(a(x-b)) + a(x-b)</p> <p>def _m_2(x, a=1, b=0): return np.floor(a*(x-b))</p> <p>def _m_3(x, d=1.5, a=1, b=0): return np.sign(a(x-b))np.power(np.abs(a*(x-b)), d)</p> <p>def _m_4(x, a=1, b=0): return np.exp(0.5a(x-b))</p> <p>def _m_5(x, a=1, b=0): return 1/(1+np.exp(-a*(x-b)))</p> <p>def softmax(fx: List[Callable]): def _softmax(x, fx=fx): if x.shape[1] != len(fx): raise ValueError(f\u201d{x.shape[1]=} is not equal to {len(fx)=}\u201c) xs = [f(x[:, i]) if i == 0 else -f(x[:, i]) for i, f in enumerate(fx)] exs = [np.exp(x) for x in xs] total = np.concatenate([x.reshape((-1, 1)) for x in exs], axis=-1) total = np.sum(total, axis=1) r = exs[0] / total</p> <pre><code>    return r\nreturn _softmax\n</code></pre> <p>plot_f([_m_1, _m_2, _m_3, _m_4, _m_5], start=-2.2, stop = 2.2, n_points=1000).jtstyle()</p>"},{"location":"MonoDenseLayer/#export_1","title":"export","text":"<p>def sample_mono_function( n_inputs: int, rng=None ) -&gt; Tuple[ List[Callable[[NDArray[np.float_]], NDArray[np.float_]]], Callable[[NDArray[np.float_]], NDArray[np.float_]],]: mx = [_m_1, _m_2, _m_3, _m_4, _m_5]</p> <pre><code>if rng is None:\n    rng = np.random.default_rng(42)\n\nfx = rng.choice(mx, size=n_inputs)\nax = rng.gamma(shape=2.0, scale=0.5, size=n_inputs)\nbx = rng.normal(size=n_inputs)\ncx = rng.gamma(shape=2.0, scale=0.5, size=n_inputs)\ndx = rng.normal(size=n_inputs)\n\nfx = [\n    lambda x, f=fx[i], a=ax[i], b=bx[i], c=cx[i], d=dx[i]: c * f(x, a=a, b=b) + d\n    for i in range(n_inputs)\n]\n\nf = softmax(fx)\n\nreturn fx, f\n</code></pre> <p>fx, f = list(sample_mono_function(10)) plot_f(fx, start=-3.2, stop=3.2, n_points=1000).jtstyle().show() # f</p>"},{"location":"MonoDenseLayer/#export_2","title":"export","text":"<p>def sample_mono_ds( f: Callable[[NDArray[np.float_]], NDArray[np.float_]], n_inputs: int, rng=None, size: int = 1_000_000, noise: float = 0.2, ) -&gt; pd.DataFrame: if rng is None: rng = np.random.default_rng(42)</p> <pre><code>x = rng.normal(size=(size, n_inputs))\ny = f(x)\nprint(f\"{x.shape=}, {y.shape=}\")\n\ndf = pd.DataFrame({f\"x_{i}\": x[:, i] for i in range(n_inputs)})\ndf[\"y\"] = y\n\nreturn df\n</code></pre> <p>df = sample_mono_ds(f, n_inputs=10) df</p> <p>df.describe().T</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Constrained Monotonic Neural Networks</li> <li>API<ul> <li>mono_dense_keras.layers</li> </ul> </li> <li>Releases</li> </ul>"},{"location":"changelog_not_found/","title":"Releases","text":""},{"location":"changelog_not_found/#changelogmd-file-not-found","title":"CHANGELOG.md file not found.","text":"<p>To generate the changelog file, please run the following command from the project root directory. </p> <pre><code>nbdev_changelog\n</code></pre> <p>If you do not want this page to be rendered as part of the documentation, please remove the following line from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- [Releases]{changelog}\n</code></pre>"},{"location":"cli_commands_not_found/","title":"No CLI commands found in console_scripts in settings.ini file.","text":"<p>For documenting CLI commands, please add command line executables in <code>console_scripts</code> in <code>settings.ini</code> file. </p> <p>If you do not want this page to be rendered as part of the documentation, please remove the following lines from the mkdocs/summary_template.txt file and build the docs again.</p> <pre><code>- CLI\n{cli}\n</code></pre>"},{"location":"mono_dense_keras_api_docs/mono_dense_keras/layers/","title":"mono_dense_keras.layers","text":""},{"location":"mono_dense_keras_api_docs/mono_dense_keras/layers/#mono_dense_keras.layers.MonotonicDense","title":"<code> MonotonicDense            (Dense)         </code>","text":"<p>Monotonic counterpart of the regular Dense Layer of tf.keras</p> Source code in <code>mono_dense_keras/layers.py</code> <pre><code>class MonotonicDense(Dense):\n\"\"\"Monotonic counterpart of the regular Dense Layer of tf.keras\"\"\"\n\n    def __init__(\n        self,\n        units: int,\n        *,\n        activation: Optional[Union[str, Callable[[tf.Tensor], tf.Tensor]]] = None,\n        monotonicity_indicator: Union[int, NDArray[np.int_]] = 1,\n        is_convex: bool = False,\n        is_concave: bool = False,\n        activation_weights: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n        **kwargs,\n    ):\n\"\"\"Constructs a new MonotonicDense instance.\n\n        Params:\n            units: Positive integer, dimensionality of the output space.\n            activation: Activation function to use, it is assumed to be convex monotonically\n                increasing function such as \"relu\" or \"elu\"\n            monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n                monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n                -1 for monotonically decreasing and 0 for non-monotonic.\n            is_convex: convex if set to True\n            is_concave: concave if set to True\n            activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n                Ignored if is_convex or is_concave is set to True\n            **kwargs: passed directly to the constructor of `Dense`\n\n        Returns:\n            N-D tensor with shape: `(batch_size, ..., units)`.\n\n        Raise:\n            ValueError:\n                - if both **is_concave** and **is_convex** are set to **True**, or\n                - if any component of activation_weights is negative or there is not exactly three components\n        \"\"\"\n        check_monotonicity_indicator_values(monotonicity_indicator)\n\n        if is_convex and is_concave:\n            raise ValueError(\n                \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n            )\n\n        if len(activation_weights) != 3:\n            raise ValueError(\n                f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n            )\n\n        if (np.array(activation_weights) &lt; 0).any():\n            raise ValueError(\n                f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n            )\n\n        super(MonotonicDense, self).__init__(units, activation=None, **kwargs)\n\n        self.units = units\n        self.org_activation = activation\n        self.activation_weights = activation_weights\n        self.monotonicity_indicator = monotonicity_indicator\n        self.is_convex = is_convex\n        self.is_concave = is_concave\n\n    @contextmanager\n    def replace_kernel(self):\n\"\"\"Replaces kernel with non-negative or non-positive values according\n        to the **monotonicity_indicator**\n        \"\"\"\n        kernel_org = self.kernel\n        self.kernel = apply_monotonicity_indicator_to_kernel(\n            self.kernel, self.monotonicity_indicator\n        )\n        try:\n            yield\n        finally:\n            self.kernel = kernel_org\n\n    def build(self, input_shape, *args, **kwargs):\n\"\"\"Build\n\n        Args:\n            input_shape: input tensor\n        \"\"\"\n        super(MonotonicDense, self).build(input_shape, *args, **kwargs)\n        if not isinstance(self.monotonicity_indicator, int):\n            if self.kernel.shape != self.monotonicity_indicator.shape:\n                raise ValueError(\n                    f\"Input shape and monotonicity vector don't have matching shapes: {self.kernel.shape} != {self.monotonicity_indicator.shape}\"\n                )\n\n    def call(self, inputs):\n\"\"\"Call\n\n        Args:\n            inputs: input tensor\n        \"\"\"\n        # calculate W'*x+y after we replace the kernal according to monotonicity vector\n        with self.replace_kernel():\n            y = super(MonotonicDense, self).call(inputs)\n\n        y = apply_activations(\n            y,\n            units=self.units,\n            activation=self.org_activation,\n            is_convex=self.is_convex,\n            is_concave=self.is_concave,\n            activation_weights=self.activation_weights,\n        )\n\n        return y\n</code></pre>"},{"location":"mono_dense_keras_api_docs/mono_dense_keras/layers/#mono_dense_keras.layers.MonotonicDense.__init__","title":"<code>__init__(self, units, *, activation=None, monotonicity_indicator=1, is_convex=False, is_concave=False, activation_weights=(1.0, 1.0, 1.0), **kwargs)</code>  <code>special</code>","text":"<p>Constructs a new MonotonicDense instance.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Positive integer, dimensionality of the output space.</p> required <code>activation</code> <code>Union[str, Callable[[tensorflow.python.framework.ops.Tensor], tensorflow.python.framework.ops.Tensor]]</code> <p>Activation function to use, it is assumed to be convex monotonically increasing function such as \"relu\" or \"elu\"</p> <code>None</code> <code>monotonicity_indicator</code> <code>Union[int, numpy.ndarray[Any, numpy.dtype[numpy.int64]]]</code> <p>Vector to indicate which of the inputs are monotonically increasing or monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing, -1 for monotonically decreasing and 0 for non-monotonic.</p> <code>1</code> <code>is_convex</code> <code>bool</code> <p>convex if set to True</p> <code>False</code> <code>is_concave</code> <code>bool</code> <p>concave if set to True</p> <code>False</code> <code>activation_weights</code> <code>Tuple[float, float, float]</code> <p>relative weights for each type of activation, the default is (1.0, 1.0, 1.0). Ignored if is_convex or is_concave is set to True</p> <code>(1.0, 1.0, 1.0)</code> <code>**kwargs</code> <p>passed directly to the constructor of <code>Dense</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>N-D tensor with shape</code> <p><code>(batch_size, ..., units)</code>.</p> Source code in <code>mono_dense_keras/layers.py</code> <pre><code>def __init__(\n    self,\n    units: int,\n    *,\n    activation: Optional[Union[str, Callable[[tf.Tensor], tf.Tensor]]] = None,\n    monotonicity_indicator: Union[int, NDArray[np.int_]] = 1,\n    is_convex: bool = False,\n    is_concave: bool = False,\n    activation_weights: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n    **kwargs,\n):\n\"\"\"Constructs a new MonotonicDense instance.\n\n    Params:\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use, it is assumed to be convex monotonically\n            increasing function such as \"relu\" or \"elu\"\n        monotonicity_indicator: Vector to indicate which of the inputs are monotonically increasing or\n            monotonically decreasing or non-monotonic. Has value 1 for monotonically increasing,\n            -1 for monotonically decreasing and 0 for non-monotonic.\n        is_convex: convex if set to True\n        is_concave: concave if set to True\n        activation_weights: relative weights for each type of activation, the default is (1.0, 1.0, 1.0).\n            Ignored if is_convex or is_concave is set to True\n        **kwargs: passed directly to the constructor of `Dense`\n\n    Returns:\n        N-D tensor with shape: `(batch_size, ..., units)`.\n\n    Raise:\n        ValueError:\n            - if both **is_concave** and **is_convex** are set to **True**, or\n            - if any component of activation_weights is negative or there is not exactly three components\n    \"\"\"\n    check_monotonicity_indicator_values(monotonicity_indicator)\n\n    if is_convex and is_concave:\n        raise ValueError(\n            \"The model cannot be set to be both convex and concave (only linear functions are both).\"\n        )\n\n    if len(activation_weights) != 3:\n        raise ValueError(\n            f\"There must be exactly three components of activation_weights, but we have this instead: {activation_weights}.\"\n        )\n\n    if (np.array(activation_weights) &lt; 0).any():\n        raise ValueError(\n            f\"Values of activation_weights must be non-negative, but we have this instead: {activation_weights}.\"\n        )\n\n    super(MonotonicDense, self).__init__(units, activation=None, **kwargs)\n\n    self.units = units\n    self.org_activation = activation\n    self.activation_weights = activation_weights\n    self.monotonicity_indicator = monotonicity_indicator\n    self.is_convex = is_convex\n    self.is_concave = is_concave\n</code></pre>"},{"location":"mono_dense_keras_api_docs/mono_dense_keras/layers/#mono_dense_keras.layers.MonotonicDense.build","title":"<code>build(self, input_shape, *args, **kwargs)</code>","text":"<p>Build</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>input tensor</p> required Source code in <code>mono_dense_keras/layers.py</code> <pre><code>def build(self, input_shape, *args, **kwargs):\n\"\"\"Build\n\n    Args:\n        input_shape: input tensor\n    \"\"\"\n    super(MonotonicDense, self).build(input_shape, *args, **kwargs)\n    if not isinstance(self.monotonicity_indicator, int):\n        if self.kernel.shape != self.monotonicity_indicator.shape:\n            raise ValueError(\n                f\"Input shape and monotonicity vector don't have matching shapes: {self.kernel.shape} != {self.monotonicity_indicator.shape}\"\n            )\n</code></pre>"},{"location":"mono_dense_keras_api_docs/mono_dense_keras/layers/#mono_dense_keras.layers.MonotonicDense.call","title":"<code>call(self, inputs)</code>","text":"<p>Call</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>input tensor</p> required Source code in <code>mono_dense_keras/layers.py</code> <pre><code>def call(self, inputs):\n\"\"\"Call\n\n    Args:\n        inputs: input tensor\n    \"\"\"\n    # calculate W'*x+y after we replace the kernal according to monotonicity vector\n    with self.replace_kernel():\n        y = super(MonotonicDense, self).call(inputs)\n\n    y = apply_activations(\n        y,\n        units=self.units,\n        activation=self.org_activation,\n        is_convex=self.is_convex,\n        is_concave=self.is_concave,\n        activation_weights=self.activation_weights,\n    )\n\n    return y\n</code></pre>"},{"location":"mono_dense_keras_api_docs/mono_dense_keras/layers/#mono_dense_keras.layers.MonotonicDense.replace_kernel","title":"<code>replace_kernel(self)</code>","text":"<p>Replaces kernel with non-negative or non-positive values according to the monotonicity_indicator</p> Source code in <code>mono_dense_keras/layers.py</code> <pre><code>@contextmanager\ndef replace_kernel(self):\n\"\"\"Replaces kernel with non-negative or non-positive values according\n    to the **monotonicity_indicator**\n    \"\"\"\n    kernel_org = self.kernel\n    self.kernel = apply_monotonicity_indicator_to_kernel(\n        self.kernel, self.monotonicity_indicator\n    )\n    try:\n        yield\n    finally:\n        self.kernel = kernel_org\n</code></pre>"}]}